38936
« »
Number of samples: 38936
Number of unique input tokens: 128
Number of unique output tokens: 130
Max sequence length for inputs: 498
Max sequence length for outputs: 500
Epoch: 0/10
Train on 36989 samples, validate on 1947 samples
Epoch 1/1

  256/36989 [..............................] - ETA: 15:11 - loss: 0.9506
  512/36989 [..............................] - ETA: 10:43 - loss: 1.1710
  768/36989 [..............................] - ETA: 9:09 - loss: 1.1941 
 1024/36989 [..............................] - ETA: 8:19 - loss: 1.1968
 1280/36989 [>.............................] - ETA: 7:47 - loss: 1.1686
 1536/36989 [>.............................] - ETA: 7:25 - loss: 1.1545
 1792/36989 [>.............................] - ETA: 7:08 - loss: 1.1451
 2048/36989 [>.............................] - ETA: 6:52 - loss: 1.1332
 2304/36989 [>.............................] - ETA: 6:40 - loss: 1.1258
 2560/36989 [=>............................] - ETA: 6:30 - loss: 1.1155
 2816/36989 [=>............................] - ETA: 6:21 - loss: 1.1074
 3072/36989 [=>............................] - ETA: 6:14 - loss: 1.0979
 3328/36989 [=>............................] - ETA: 6:07 - loss: 1.0879
 3584/36989 [=>............................] - ETA: 6:03 - loss: 1.0793
 3840/36989 [==>...........................] - ETA: 5:59 - loss: 1.0720
 4096/36989 [==>...........................] - ETA: 5:54 - loss: 1.0636
 4352/36989 [==>...........................] - ETA: 5:48 - loss: 1.0598
 4608/36989 [==>...........................] - ETA: 5:43 - loss: 1.0542
 4864/36989 [==>...........................] - ETA: 5:39 - loss: 1.0472
 5120/36989 [===>..........................] - ETA: 5:35 - loss: 1.0428
 5376/36989 [===>..........................] - ETA: 5:32 - loss: 1.0381
 5632/36989 [===>..........................] - ETA: 5:30 - loss: 1.0346
 5888/36989 [===>..........................] - ETA: 5:27 - loss: 1.0317
 6144/36989 [===>..........................] - ETA: 5:23 - loss: 1.0281
 6400/36989 [====>.........................] - ETA: 5:20 - loss: 1.0237
 6656/36989 [====>.........................] - ETA: 5:16 - loss: 1.0192
 6912/36989 [====>.........................] - ETA: 5:12 - loss: 1.0169
 7168/36989 [====>.........................] - ETA: 5:09 - loss: 1.0112
 7424/36989 [=====>........................] - ETA: 5:07 - loss: 1.0078
 7680/36989 [=====>........................] - ETA: 5:03 - loss: 1.0029
 7936/36989 [=====>........................] - ETA: 5:00 - loss: 0.9990
 8192/36989 [=====>........................] - ETA: 4:57 - loss: 0.9949
 8448/36989 [=====>........................] - ETA: 4:54 - loss: 0.9917
 8704/36989 [======>.......................] - ETA: 4:50 - loss: 0.9900
 8960/36989 [======>.......................] - ETA: 4:47 - loss: 0.9883
 9216/36989 [======>.......................] - ETA: 4:44 - loss: 0.9847
 9472/36989 [======>.......................] - ETA: 4:42 - loss: 0.9827
 9728/36989 [======>.......................] - ETA: 4:39 - loss: 0.9792
 9984/36989 [=======>......................] - ETA: 4:36 - loss: 0.9753
10240/36989 [=======>......................] - ETA: 4:32 - loss: 0.9727
10496/36989 [=======>......................] - ETA: 4:29 - loss: 0.9701
10752/36989 [=======>......................] - ETA: 4:26 - loss: 0.9687
11008/36989 [=======>......................] - ETA: 4:23 - loss: 0.9661
11264/36989 [========>.....................] - ETA: 4:20 - loss: 0.9634
11520/36989 [========>.....................] - ETA: 4:17 - loss: 0.9616
11776/36989 [========>.....................] - ETA: 4:14 - loss: 0.9600
12032/36989 [========>.....................] - ETA: 4:11 - loss: 0.9571
12288/36989 [========>.....................] - ETA: 4:08 - loss: 0.9557
12544/36989 [=========>....................] - ETA: 4:04 - loss: 0.9540
12800/36989 [=========>....................] - ETA: 4:01 - loss: 0.9515
13056/36989 [=========>....................] - ETA: 3:58 - loss: 0.9508
13312/36989 [=========>....................] - ETA: 3:56 - loss: 0.9486
13568/36989 [==========>...................] - ETA: 3:53 - loss: 0.9459
13824/36989 [==========>...................] - ETA: 3:50 - loss: 0.9442
14080/36989 [==========>...................] - ETA: 3:47 - loss: 0.9418
14336/36989 [==========>...................] - ETA: 3:45 - loss: 0.9402
14592/36989 [==========>...................] - ETA: 3:42 - loss: 0.9393
14848/36989 [===========>..................] - ETA: 3:39 - loss: 0.9375
15104/36989 [===========>..................] - ETA: 3:36 - loss: 0.9352
15360/36989 [===========>..................] - ETA: 3:33 - loss: 0.9337
15616/36989 [===========>..................] - ETA: 3:31 - loss: 0.9319
15872/36989 [===========>..................] - ETA: 3:28 - loss: 0.9298
16128/36989 [============>.................] - ETA: 3:25 - loss: 0.9285
16384/36989 [============>.................] - ETA: 3:23 - loss: 0.9263
16640/36989 [============>.................] - ETA: 3:20 - loss: 0.9244
16896/36989 [============>.................] - ETA: 3:17 - loss: 0.9229
17152/36989 [============>.................] - ETA: 3:14 - loss: 0.9209
17408/36989 [=============>................] - ETA: 3:12 - loss: 0.9196
17664/36989 [=============>................] - ETA: 3:09 - loss: 0.9180
17920/36989 [=============>................] - ETA: 3:07 - loss: 0.9167
18176/36989 [=============>................] - ETA: 3:04 - loss: 0.9151
18432/36989 [=============>................] - ETA: 3:01 - loss: 0.9130
18688/36989 [==============>...............] - ETA: 2:59 - loss: 0.9116
18944/36989 [==============>...............] - ETA: 2:56 - loss: 0.9096
19200/36989 [==============>...............] - ETA: 2:53 - loss: 0.9083
19456/36989 [==============>...............] - ETA: 2:51 - loss: 0.9068
19712/36989 [==============>...............] - ETA: 2:48 - loss: 0.9050
19968/36989 [===============>..............] - ETA: 2:46 - loss: 0.9043
20224/36989 [===============>..............] - ETA: 2:43 - loss: 0.9028
20480/36989 [===============>..............] - ETA: 2:40 - loss: 0.9018
20736/36989 [===============>..............] - ETA: 2:38 - loss: 0.8997
20992/36989 [================>.............] - ETA: 2:35 - loss: 0.8984
21248/36989 [================>.............] - ETA: 2:33 - loss: 0.8969
21504/36989 [================>.............] - ETA: 2:30 - loss: 0.8954
21760/36989 [================>.............] - ETA: 2:28 - loss: 0.8938
22016/36989 [================>.............] - ETA: 2:25 - loss: 0.8922
22272/36989 [=================>............] - ETA: 2:22 - loss: 0.8903
22528/36989 [=================>............] - ETA: 2:20 - loss: 0.8889
22784/36989 [=================>............] - ETA: 2:17 - loss: 0.8880
23040/36989 [=================>............] - ETA: 2:15 - loss: 0.8866
23296/36989 [=================>............] - ETA: 2:12 - loss: 0.8856
23552/36989 [==================>...........] - ETA: 2:10 - loss: 0.8837
23808/36989 [==================>...........] - ETA: 2:07 - loss: 0.8823
24064/36989 [==================>...........] - ETA: 2:04 - loss: 0.8815
24320/36989 [==================>...........] - ETA: 2:02 - loss: 0.8807
24576/36989 [==================>...........] - ETA: 1:59 - loss: 0.8794
24832/36989 [===================>..........] - ETA: 1:57 - loss: 0.8775
25088/36989 [===================>..........] - ETA: 1:54 - loss: 0.8763
25344/36989 [===================>..........] - ETA: 1:52 - loss: 0.8753
25600/36989 [===================>..........] - ETA: 1:50 - loss: 0.8738
25856/36989 [===================>..........] - ETA: 1:47 - loss: 0.8725
26112/36989 [====================>.........] - ETA: 1:45 - loss: 0.8717
26368/36989 [====================>.........] - ETA: 1:42 - loss: 0.8701
26624/36989 [====================>.........] - ETA: 1:40 - loss: 0.8690
26880/36989 [====================>.........] - ETA: 1:37 - loss: 0.8678
27136/36989 [=====================>........] - ETA: 1:35 - loss: 0.8664
27392/36989 [=====================>........] - ETA: 1:32 - loss: 0.8650
27648/36989 [=====================>........] - ETA: 1:30 - loss: 0.8641
27904/36989 [=====================>........] - ETA: 1:27 - loss: 0.8628
28160/36989 [=====================>........] - ETA: 1:25 - loss: 0.8616
28416/36989 [======================>.......] - ETA: 1:22 - loss: 0.8605
28672/36989 [======================>.......] - ETA: 1:20 - loss: 0.8594
28928/36989 [======================>.......] - ETA: 1:17 - loss: 0.8582
29184/36989 [======================>.......] - ETA: 1:15 - loss: 0.8568
29440/36989 [======================>.......] - ETA: 1:12 - loss: 0.8559
29696/36989 [=======================>......] - ETA: 1:10 - loss: 0.8551
29952/36989 [=======================>......] - ETA: 1:07 - loss: 0.8543
30208/36989 [=======================>......] - ETA: 1:05 - loss: 0.8530
30464/36989 [=======================>......] - ETA: 1:03 - loss: 0.8518
30720/36989 [=======================>......] - ETA: 1:00 - loss: 0.8507
30976/36989 [========================>.....] - ETA: 58s - loss: 0.8498 
31232/36989 [========================>.....] - ETA: 55s - loss: 0.8488
31488/36989 [========================>.....] - ETA: 53s - loss: 0.8476
31744/36989 [========================>.....] - ETA: 50s - loss: 0.8464
32000/36989 [========================>.....] - ETA: 48s - loss: 0.8453
32256/36989 [=========================>....] - ETA: 45s - loss: 0.8442
32512/36989 [=========================>....] - ETA: 43s - loss: 0.8430
32768/36989 [=========================>....] - ETA: 40s - loss: 0.8419
33024/36989 [=========================>....] - ETA: 38s - loss: 0.8410
33280/36989 [=========================>....] - ETA: 35s - loss: 0.8399
33536/36989 [==========================>...] - ETA: 33s - loss: 0.8387
33792/36989 [==========================>...] - ETA: 30s - loss: 0.8377
34048/36989 [==========================>...] - ETA: 28s - loss: 0.8368
34304/36989 [==========================>...] - ETA: 25s - loss: 0.8360
34560/36989 [===========================>..] - ETA: 23s - loss: 0.8345
34816/36989 [===========================>..] - ETA: 20s - loss: 0.8337
35072/36989 [===========================>..] - ETA: 18s - loss: 0.8326
35328/36989 [===========================>..] - ETA: 15s - loss: 0.8310
35584/36989 [===========================>..] - ETA: 13s - loss: 0.8299
35840/36989 [============================>.] - ETA: 11s - loss: 0.8288
36096/36989 [============================>.] - ETA: 8s - loss: 0.8279 
36352/36989 [============================>.] - ETA: 6s - loss: 0.8269
36608/36989 [============================>.] - ETA: 3s - loss: 0.8261
36864/36989 [============================>.] - ETA: 1s - loss: 0.8250
36989/36989 [==============================] - 363s 10ms/step - loss: 0.8245 - val_loss: 0.5637
---
Input python program: 
n = int(input())
nums = list(map(int, input().split()))
sum = 0
for num in nums:
    sum += num
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n;
    cin>>n;
    int a[i];
    int sum=0;
    cin>>n;
    int a[i];
        cin>>> n;
        sum==0;
        cout<<sum;
    return 0;
}»
---
Input python program: 
num = int(input())
sum = 0
for i in [int(x) for x in input().split()]:
    sum += i
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n;
    cin>>n;
    int a[i];
    int sum=0;
    cin>>n;
    int a[i];
        cin>>> n;
        sum==0;
        cout<<sum;
    return 0;
}»
---
Input python program: 
map(int,input())
array = list(map(int,input().split()))
sum = 0
for x in array:
    sum += x
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n;
    cin>>n;
    int a[i];
    int sum=0;
    cin>>n;
    int a[i];
        cin>>> n;
        sum==0;
        cout<<sum;
    return 0;
}»
---
Input python program: 
def arraySum(array):
    total = 0
    for num in array:
        total += num
    return total

n = int(input())
arrayString = input().split()
arrayInt = []

for i in range(0,n):
    arrayInt.append(int(arrayString[i]))

print (arraySum(arrayInt))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n;
    cin>>n;
    int a[i];
    int sum=0;
    cin>>n;
    int a[i];
        cin>>> n;
        sum==0;
        cout<<sum;
    return 0;
}»
---
Input python program: 
# Inputs
n = int(input())
array = input().split()

# Solution
array = list(map(int, array))
print(sum(array))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n;
    cin>>n;
    int a[i];
    int sum=0;
    cin>>n;
    int a[i];
        cin>>> n;
        sum==0;
        cout<<sum;
    return 0;
}»
Epoch: 1/10
Train on 36989 samples, validate on 1947 samples
Epoch 1/1

  256/36989 [..............................] - ETA: 5:28 - loss: 0.6834
  512/36989 [..............................] - ETA: 5:23 - loss: 0.6928
  768/36989 [..............................] - ETA: 5:36 - loss: 0.6957
 1024/36989 [..............................] - ETA: 5:30 - loss: 0.6781
 1280/36989 [>.............................] - ETA: 5:28 - loss: 0.6728
 1536/36989 [>.............................] - ETA: 5:27 - loss: 0.6676
 1792/36989 [>.............................] - ETA: 5:24 - loss: 0.6677
 2048/36989 [>.............................] - ETA: 5:20 - loss: 0.6674
 2304/36989 [>.............................] - ETA: 5:16 - loss: 0.6649
 2560/36989 [=>............................] - ETA: 5:14 - loss: 0.6738
 2816/36989 [=>............................] - ETA: 5:12 - loss: 0.6737
 3072/36989 [=>............................] - ETA: 5:10 - loss: 0.6765
 3328/36989 [=>............................] - ETA: 5:08 - loss: 0.6708
 3584/36989 [=>............................] - ETA: 5:06 - loss: 0.6674
 3840/36989 [==>...........................] - ETA: 5:04 - loss: 0.6656
 4096/36989 [==>...........................] - ETA: 5:02 - loss: 0.6664
 4352/36989 [==>...........................] - ETA: 4:59 - loss: 0.6656
 4608/36989 [==>...........................] - ETA: 4:56 - loss: 0.6667
 4864/36989 [==>...........................] - ETA: 4:54 - loss: 0.6658
 5120/36989 [===>..........................] - ETA: 4:52 - loss: 0.6635
 5376/36989 [===>..........................] - ETA: 4:49 - loss: 0.6627
 5632/36989 [===>..........................] - ETA: 4:47 - loss: 0.6634
 5888/36989 [===>..........................] - ETA: 4:45 - loss: 0.6618
 6144/36989 [===>..........................] - ETA: 4:42 - loss: 0.6620
 6400/36989 [====>.........................] - ETA: 4:40 - loss: 0.6605
 6656/36989 [====>.........................] - ETA: 4:38 - loss: 0.6597
 6912/36989 [====>.........................] - ETA: 4:35 - loss: 0.6570
 7168/36989 [====>.........................] - ETA: 4:33 - loss: 0.6574
 7424/36989 [=====>........................] - ETA: 4:31 - loss: 0.6578
 7680/36989 [=====>........................] - ETA: 4:28 - loss: 0.6578
 7936/36989 [=====>........................] - ETA: 4:26 - loss: 0.6581
 8192/36989 [=====>........................] - ETA: 4:24 - loss: 0.6576
 8448/36989 [=====>........................] - ETA: 4:22 - loss: 0.6583
 8704/36989 [======>.......................] - ETA: 4:19 - loss: 0.6565
 8960/36989 [======>.......................] - ETA: 4:17 - loss: 0.6571
 9216/36989 [======>.......................] - ETA: 4:15 - loss: 0.6565
 9472/36989 [======>.......................] - ETA: 4:12 - loss: 0.6566
 9728/36989 [======>.......................] - ETA: 4:10 - loss: 0.6567
 9984/36989 [=======>......................] - ETA: 4:08 - loss: 0.6573
10240/36989 [=======>......................] - ETA: 4:05 - loss: 0.6579
10496/36989 [=======>......................] - ETA: 4:03 - loss: 0.6577
10752/36989 [=======>......................] - ETA: 4:00 - loss: 0.6575
11008/36989 [=======>......................] - ETA: 3:58 - loss: 0.6570
11264/36989 [========>.....................] - ETA: 3:55 - loss: 0.6573
11520/36989 [========>.....................] - ETA: 3:53 - loss: 0.6569
11776/36989 [========>.....................] - ETA: 3:51 - loss: 0.6566
12032/36989 [========>.....................] - ETA: 3:48 - loss: 0.6558
12288/36989 [========>.....................] - ETA: 3:46 - loss: 0.6558
12544/36989 [=========>....................] - ETA: 3:44 - loss: 0.6551
12800/36989 [=========>....................] - ETA: 3:41 - loss: 0.6550
13056/36989 [=========>....................] - ETA: 3:39 - loss: 0.6546
13312/36989 [=========>....................] - ETA: 3:37 - loss: 0.6544
13568/36989 [==========>...................] - ETA: 3:34 - loss: 0.6544
13824/36989 [==========>...................] - ETA: 3:32 - loss: 0.6538
14080/36989 [==========>...................] - ETA: 3:29 - loss: 0.6536
14336/36989 [==========>...................] - ETA: 3:27 - loss: 0.6530
14592/36989 [==========>...................] - ETA: 3:24 - loss: 0.6527
14848/36989 [===========>..................] - ETA: 3:22 - loss: 0.6521
15104/36989 [===========>..................] - ETA: 3:20 - loss: 0.6528
15360/36989 [===========>..................] - ETA: 3:17 - loss: 0.6515
15616/36989 [===========>..................] - ETA: 3:15 - loss: 0.6510
15872/36989 [===========>..................] - ETA: 3:12 - loss: 0.6505
16128/36989 [============>.................] - ETA: 3:10 - loss: 0.6495
16384/36989 [============>.................] - ETA: 3:08 - loss: 0.6493
16640/36989 [============>.................] - ETA: 3:05 - loss: 0.6491
16896/36989 [============>.................] - ETA: 3:03 - loss: 0.6489
17152/36989 [============>.................] - ETA: 3:01 - loss: 0.6482
17408/36989 [=============>................] - ETA: 2:58 - loss: 0.6473
17664/36989 [=============>................] - ETA: 2:56 - loss: 0.6460
17920/36989 [=============>................] - ETA: 2:53 - loss: 0.6455
18176/36989 [=============>................] - ETA: 2:51 - loss: 0.6451
18432/36989 [=============>................] - ETA: 2:49 - loss: 0.6444
18688/36989 [==============>...............] - ETA: 2:46 - loss: 0.6439
18944/36989 [==============>...............] - ETA: 2:44 - loss: 0.6435
19200/36989 [==============>...............] - ETA: 2:42 - loss: 0.6434
19456/36989 [==============>...............] - ETA: 2:39 - loss: 0.6427
19712/36989 [==============>...............] - ETA: 2:37 - loss: 0.6432
19968/36989 [===============>..............] - ETA: 2:35 - loss: 0.6427
20224/36989 [===============>..............] - ETA: 2:32 - loss: 0.6416
20480/36989 [===============>..............] - ETA: 2:30 - loss: 0.6414
20736/36989 [===============>..............] - ETA: 2:28 - loss: 0.6406
20992/36989 [================>.............] - ETA: 2:25 - loss: 0.6398
21248/36989 [================>.............] - ETA: 2:23 - loss: 0.6391
21504/36989 [================>.............] - ETA: 2:21 - loss: 0.6382
21760/36989 [================>.............] - ETA: 2:18 - loss: 0.6375
22016/36989 [================>.............] - ETA: 2:16 - loss: 0.6368
22272/36989 [=================>............] - ETA: 2:14 - loss: 0.6365
22528/36989 [=================>............] - ETA: 2:11 - loss: 0.6357
22784/36989 [=================>............] - ETA: 2:09 - loss: 0.6350
23040/36989 [=================>............] - ETA: 2:07 - loss: 0.6344
23296/36989 [=================>............] - ETA: 2:04 - loss: 0.6336
23552/36989 [==================>...........] - ETA: 2:02 - loss: 0.6328
23808/36989 [==================>...........] - ETA: 2:00 - loss: 0.6322
24064/36989 [==================>...........] - ETA: 1:57 - loss: 0.6319
24320/36989 [==================>...........] - ETA: 1:55 - loss: 0.6314
24576/36989 [==================>...........] - ETA: 1:53 - loss: 0.6304
24832/36989 [===================>..........] - ETA: 1:50 - loss: 0.6299
25088/36989 [===================>..........] - ETA: 1:48 - loss: 0.6296
25344/36989 [===================>..........] - ETA: 1:46 - loss: 0.6291
25600/36989 [===================>..........] - ETA: 1:43 - loss: 0.6285
25856/36989 [===================>..........] - ETA: 1:41 - loss: 0.6278
26112/36989 [====================>.........] - ETA: 1:39 - loss: 0.6268
26368/36989 [====================>.........] - ETA: 1:36 - loss: 0.6261
26624/36989 [====================>.........] - ETA: 1:34 - loss: 0.6256
26880/36989 [====================>.........] - ETA: 1:32 - loss: 0.6252
27136/36989 [=====================>........] - ETA: 1:29 - loss: 0.6246
27392/36989 [=====================>........] - ETA: 1:27 - loss: 0.6241
27648/36989 [=====================>........] - ETA: 1:25 - loss: 0.6235
27904/36989 [=====================>........] - ETA: 1:22 - loss: 0.6230
28160/36989 [=====================>........] - ETA: 1:20 - loss: 0.6223
28416/36989 [======================>.......] - ETA: 1:18 - loss: 0.6218
28672/36989 [======================>.......] - ETA: 1:15 - loss: 0.6213
28928/36989 [======================>.......] - ETA: 1:13 - loss: 0.6211
29184/36989 [======================>.......] - ETA: 1:11 - loss: 0.6208
29440/36989 [======================>.......] - ETA: 1:08 - loss: 0.6200
29696/36989 [=======================>......] - ETA: 1:06 - loss: 0.6194
29952/36989 [=======================>......] - ETA: 1:04 - loss: 0.6193
30208/36989 [=======================>......] - ETA: 1:01 - loss: 0.6187
30464/36989 [=======================>......] - ETA: 59s - loss: 0.6183 
30720/36989 [=======================>......] - ETA: 57s - loss: 0.6178
30976/36989 [========================>.....] - ETA: 54s - loss: 0.6171
31232/36989 [========================>.....] - ETA: 52s - loss: 0.6166
31488/36989 [========================>.....] - ETA: 50s - loss: 0.6161
31744/36989 [========================>.....] - ETA: 47s - loss: 0.6159
32000/36989 [========================>.....] - ETA: 45s - loss: 0.6153
32256/36989 [=========================>....] - ETA: 43s - loss: 0.6151
32512/36989 [=========================>....] - ETA: 40s - loss: 0.6148
32768/36989 [=========================>....] - ETA: 38s - loss: 0.6142
33024/36989 [=========================>....] - ETA: 36s - loss: 0.6139
33280/36989 [=========================>....] - ETA: 33s - loss: 0.6132
33536/36989 [==========================>...] - ETA: 31s - loss: 0.6127
33792/36989 [==========================>...] - ETA: 29s - loss: 0.6123
34048/36989 [==========================>...] - ETA: 26s - loss: 0.6115
34304/36989 [==========================>...] - ETA: 24s - loss: 0.6111
34560/36989 [===========================>..] - ETA: 22s - loss: 0.6105
34816/36989 [===========================>..] - ETA: 19s - loss: 0.6103
35072/36989 [===========================>..] - ETA: 17s - loss: 0.6096
35328/36989 [===========================>..] - ETA: 15s - loss: 0.6092
35584/36989 [===========================>..] - ETA: 12s - loss: 0.6086
35840/36989 [============================>.] - ETA: 10s - loss: 0.6081
36096/36989 [============================>.] - ETA: 8s - loss: 0.6079 
36352/36989 [============================>.] - ETA: 5s - loss: 0.6075
36608/36989 [============================>.] - ETA: 3s - loss: 0.6073
36864/36989 [============================>.] - ETA: 1s - loss: 0.6068
36989/36989 [==============================] - 344s 9ms/step - loss: 0.6065 - val_loss: 0.4569
---
Input python program: 
n = int(input())
nums = list(map(int, input().split()))
sum = 0
for num in nums:
    sum += num
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n;
    cin>>n;
    int a[n];
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum++;
        cout<<sum<<endl;
    }
    return 0;
}»
---
Input python program: 
num = int(input())
sum = 0
for i in [int(x) for x in input().split()]:
    sum += i
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n;
    cin>>n;
    int a[n];
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum++;
        cout<<sum<<endl;
    }
    return 0;
}»
---
Input python program: 
map(int,input())
array = list(map(int,input().split()))
sum = 0
for x in array:
    sum += x
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n;
    cin>>n;
    int a[n];
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum++;
        cout<<sum<<endl;
    }
    return 0;
}»
---
Input python program: 
def arraySum(array):
    total = 0
    for num in array:
        total += num
    return total

n = int(input())
arrayString = input().split()
arrayInt = []

for i in range(0,n):
    arrayInt.append(int(arrayString[i]))

print (arraySum(arrayInt))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n;
    cin>>n;
    int a[n];
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum++;
        cout<<sum<<endl;
    }
    return 0;
}»
---
Input python program: 
# Inputs
n = int(input())
array = input().split()

# Solution
array = list(map(int, array))
print(sum(array))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n;
    cin>>n;
    int a[n];
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum++;
        cout<<sum<<endl;
    }
    return 0;
}»
Epoch: 2/10
Train on 36989 samples, validate on 1947 samples
Epoch 1/1

  256/36989 [..............................] - ETA: 5:55 - loss: 0.5482
  512/36989 [..............................] - ETA: 5:44 - loss: 0.5632
  768/36989 [..............................] - ETA: 5:39 - loss: 0.5444
 1024/36989 [..............................] - ETA: 5:33 - loss: 0.5338
 1280/36989 [>.............................] - ETA: 5:32 - loss: 0.5371
 1536/36989 [>.............................] - ETA: 5:29 - loss: 0.5352
 1792/36989 [>.............................] - ETA: 5:25 - loss: 0.5414
 2048/36989 [>.............................] - ETA: 5:22 - loss: 0.5417
 2304/36989 [>.............................] - ETA: 5:19 - loss: 0.5425
 2560/36989 [=>............................] - ETA: 5:16 - loss: 0.5403
 2816/36989 [=>............................] - ETA: 5:13 - loss: 0.5389
 3072/36989 [=>............................] - ETA: 5:11 - loss: 0.5413
 3328/36989 [=>............................] - ETA: 5:08 - loss: 0.5420
 3584/36989 [=>............................] - ETA: 5:05 - loss: 0.5421
 3840/36989 [==>...........................] - ETA: 5:03 - loss: 0.5420
 4096/36989 [==>...........................] - ETA: 5:02 - loss: 0.5410
 4352/36989 [==>...........................] - ETA: 5:00 - loss: 0.5419
 4608/36989 [==>...........................] - ETA: 4:58 - loss: 0.5425
 4864/36989 [==>...........................] - ETA: 4:56 - loss: 0.5420
 5120/36989 [===>..........................] - ETA: 4:53 - loss: 0.5426
 5376/36989 [===>..........................] - ETA: 4:51 - loss: 0.5420
 5632/36989 [===>..........................] - ETA: 4:48 - loss: 0.5439
 5888/36989 [===>..........................] - ETA: 4:46 - loss: 0.5433
 6144/36989 [===>..........................] - ETA: 4:44 - loss: 0.5421
 6400/36989 [====>.........................] - ETA: 4:41 - loss: 0.5424
 6656/36989 [====>.........................] - ETA: 4:39 - loss: 0.5417
 6912/36989 [====>.........................] - ETA: 4:38 - loss: 0.5415
 7168/36989 [====>.........................] - ETA: 4:36 - loss: 0.5404
 7424/36989 [=====>........................] - ETA: 4:34 - loss: 0.5385
 7680/36989 [=====>........................] - ETA: 4:32 - loss: 0.5386
 7936/36989 [=====>........................] - ETA: 4:30 - loss: 0.5387
 8192/36989 [=====>........................] - ETA: 4:27 - loss: 0.5381
 8448/36989 [=====>........................] - ETA: 4:25 - loss: 0.5373
 8704/36989 [======>.......................] - ETA: 4:22 - loss: 0.5373
 8960/36989 [======>.......................] - ETA: 4:20 - loss: 0.5374
 9216/36989 [======>.......................] - ETA: 4:18 - loss: 0.5365
 9472/36989 [======>.......................] - ETA: 4:15 - loss: 0.5360
 9728/36989 [======>.......................] - ETA: 4:13 - loss: 0.5357
 9984/36989 [=======>......................] - ETA: 4:10 - loss: 0.5347
10240/36989 [=======>......................] - ETA: 4:08 - loss: 0.5348
10496/36989 [=======>......................] - ETA: 4:06 - loss: 0.5354
10752/36989 [=======>......................] - ETA: 4:03 - loss: 0.5352
11008/36989 [=======>......................] - ETA: 4:01 - loss: 0.5360
11264/36989 [========>.....................] - ETA: 3:58 - loss: 0.5358
11520/36989 [========>.....................] - ETA: 3:56 - loss: 0.5357
11776/36989 [========>.....................] - ETA: 3:54 - loss: 0.5349
12032/36989 [========>.....................] - ETA: 3:51 - loss: 0.5341
12288/36989 [========>.....................] - ETA: 3:49 - loss: 0.5342
12544/36989 [=========>....................] - ETA: 3:46 - loss: 0.533538936
« »
Number of samples: 38936
Number of unique input tokens: 128
Number of unique output tokens: 130
Max sequence length for inputs: 498
Max sequence length for outputs: 500
Epoch: 0/10
Train on 36989 samples, validate on 1947 samples
Epoch 1/1

  256/36989 [..............................] - ETA: 15:08 - loss: 0.5247
  512/36989 [..............................] - ETA: 10:34 - loss: 1.0574
  768/36989 [..............................] - ETA: 8:59 - loss: 1.1487 
 1024/36989 [..............................] - ETA: 8:11 - loss: 1.1127
 1280/36989 [>.............................] - ETA: 7:36 - loss: 1.0785
 1536/36989 [>.............................] - ETA: 7:15 - loss: 1.0355
 1792/36989 [>.............................] - ETA: 6:57 - loss: 0.9955
 2048/36989 [>.............................] - ETA: 6:45 - loss: 0.9593
 2304/36989 [>.............................] - ETA: 6:34 - loss: 0.9298
 2560/36989 [=>............................] - ETA: 6:24 - loss: 0.9043
 2816/36989 [=>............................] - ETA: 6:16 - loss: 0.8889
 3072/36989 [=>............................] - ETA: 6:09 - loss: 0.8742
 3328/36989 [=>............................] - ETA: 6:04 - loss: 0.8625
 3584/36989 [=>............................] - ETA: 5:59 - loss: 0.8498
 3840/36989 [==>...........................] - ETA: 5:55 - loss: 0.8377
 4096/36989 [==>...........................] - ETA: 5:50 - loss: 0.8257
 4352/36989 [==>...........................] - ETA: 5:44 - loss: 0.8167
 4608/36989 [==>...........................] - ETA: 5:40 - loss: 0.8080
 4864/36989 [==>...........................] - ETA: 5:36 - loss: 0.7979
 5120/36989 [===>..........................] - ETA: 5:31 - loss: 0.7899
 5376/36989 [===>..........................] - ETA: 5:29 - loss: 0.7829
 5632/36989 [===>..........................] - ETA: 5:26 - loss: 0.7746
 5888/36989 [===>..........................] - ETA: 5:23 - loss: 0.7687
 6144/36989 [===>..........................] - ETA: 5:19 - loss: 0.7637
 6400/36989 [====>.........................] - ETA: 5:16 - loss: 0.7577
 6656/36989 [====>.........................] - ETA: 5:13 - loss: 0.7512
 6912/36989 [====>.........................] - ETA: 5:09 - loss: 0.7460
 7168/36989 [====>.........................] - ETA: 5:06 - loss: 0.7406
 7424/36989 [=====>........................] - ETA: 5:02 - loss: 0.7348
 7680/36989 [=====>........................] - ETA: 4:59 - loss: 0.7305
 7936/36989 [=====>........................] - ETA: 4:55 - loss: 0.7259
 8192/36989 [=====>........................] - ETA: 4:52 - loss: 0.7230
 8448/36989 [=====>........................] - ETA: 4:48 - loss: 0.7192
 8704/36989 [======>.......................] - ETA: 4:45 - loss: 0.7142
 8960/36989 [======>.......................] - ETA: 4:42 - loss: 0.7109
 9216/36989 [======>.......................] - ETA: 4:39 - loss: 0.7070
 9472/36989 [======>.......................] - ETA: 4:36 - loss: 0.7032
 9728/36989 [======>.......................] - ETA: 4:33 - loss: 0.6993
 9984/36989 [=======>......................] - ETA: 4:30 - loss: 0.6946
10240/36989 [=======>......................] - ETA: 4:27 - loss: 0.6916
10496/36989 [=======>......................] - ETA: 4:24 - loss: 0.6896
10752/36989 [=======>......................] - ETA: 4:21 - loss: 0.6866
11008/36989 [=======>......................] - ETA: 4:18 - loss: 0.6834
11264/36989 [========>.....................] - ETA: 4:15 - loss: 0.6805
11520/36989 [========>.....................] - ETA: 4:12 - loss: 0.6777
11776/36989 [========>.....................] - ETA: 4:09 - loss: 0.6746
12032/36989 [========>.....................] - ETA: 4:07 - loss: 0.6716
12288/36989 [========>.....................] - ETA: 4:04 - loss: 0.6697
12544/36989 [=========>....................] - ETA: 4:01 - loss: 0.6671
12800/36989 [=========>....................] - ETA: 3:58 - loss: 0.6645
13056/36989 [=========>....................] - ETA: 3:55 - loss: 0.6623
13312/36989 [=========>....................] - ETA: 3:53 - loss: 0.6604
13568/36989 [==========>...................] - ETA: 3:50 - loss: 0.6586
13824/36989 [==========>...................] - ETA: 3:47 - loss: 0.6561
14080/36989 [==========>...................] - ETA: 3:45 - loss: 0.6545
14336/36989 [==========>...................] - ETA: 3:42 - loss: 0.6518
14592/36989 [==========>...................] - ETA: 3:39 - loss: 0.6502
14848/36989 [===========>..................] - ETA: 3:37 - loss: 0.6479
15104/36989 [===========>..................] - ETA: 3:34 - loss: 0.6459
15360/36989 [===========>..................] - ETA: 3:32 - loss: 0.6446
15616/36989 [===========>..................] - ETA: 3:29 - loss: 0.6431
15872/36989 [===========>..................] - ETA: 3:26 - loss: 0.6410
16128/36989 [============>.................] - ETA: 3:24 - loss: 0.6394
16384/36989 [============>.................] - ETA: 3:21 - loss: 0.6382
16640/36989 [============>.................] - ETA: 3:18 - loss: 0.6365
16896/36989 [============>.................] - ETA: 3:16 - loss: 0.6352
17152/36989 [============>.................] - ETA: 3:13 - loss: 0.6341
17408/36989 [=============>................] - ETA: 3:10 - loss: 0.6330
17664/36989 [=============>................] - ETA: 3:08 - loss: 0.6321
17920/36989 [=============>................] - ETA: 3:05 - loss: 0.6307
18176/36989 [=============>................] - ETA: 3:03 - loss: 0.6292
18432/36989 [=============>................] - ETA: 3:00 - loss: 0.6278
18688/36989 [==============>...............] - ETA: 2:57 - loss: 0.6269
18944/36989 [==============>...............] - ETA: 2:55 - loss: 0.6257
19200/36989 [==============>...............] - ETA: 2:52 - loss: 0.6245
19456/36989 [==============>...............] - ETA: 2:50 - loss: 0.6233
19712/36989 [==============>...............] - ETA: 2:47 - loss: 0.6222
19968/36989 [===============>..............] - ETA: 2:44 - loss: 0.6212
20224/36989 [===============>..............] - ETA: 2:42 - loss: 0.6203
20480/36989 [===============>..............] - ETA: 2:39 - loss: 0.6193
20736/36989 [===============>..............] - ETA: 2:37 - loss: 0.6187
20992/36989 [================>.............] - ETA: 2:34 - loss: 0.6177
21248/36989 [================>.............] - ETA: 2:32 - loss: 0.6165
21504/36989 [================>.............] - ETA: 2:29 - loss: 0.6152
21760/36989 [================>.............] - ETA: 2:27 - loss: 0.6143
22016/36989 [================>.............] - ETA: 2:24 - loss: 0.6132
22272/36989 [=================>............] - ETA: 2:22 - loss: 0.6122
22528/36989 [=================>............] - ETA: 2:19 - loss: 0.6113
22784/36989 [=================>............] - ETA: 2:17 - loss: 0.6102
23040/36989 [=================>............] - ETA: 2:14 - loss: 0.6095
23296/36989 [=================>............] - ETA: 2:11 - loss: 0.6090
23552/36989 [==================>...........] - ETA: 2:09 - loss: 0.6084
23808/36989 [==================>...........] - ETA: 2:07 - loss: 0.6079
24064/36989 [==================>...........] - ETA: 2:04 - loss: 0.6070
24320/36989 [==================>...........] - ETA: 2:02 - loss: 0.6062
24576/36989 [==================>...........] - ETA: 1:59 - loss: 0.6055
24832/36989 [===================>..........] - ETA: 1:57 - loss: 0.6050
25088/36989 [===================>..........] - ETA: 1:54 - loss: 0.6047
25344/36989 [===================>..........] - ETA: 1:52 - loss: 0.6043
25600/36989 [===================>..........] - ETA: 1:49 - loss: 0.6037
25856/36989 [===================>..........] - ETA: 1:47 - loss: 0.6031
26112/36989 [====================>.........] - ETA: 1:44 - loss: 0.6025
26368/36989 [====================>.........] - ETA: 1:42 - loss: 0.6018
26624/36989 [====================>.........] - ETA: 1:39 - loss: 0.6012
26880/36989 [====================>.........] - ETA: 1:37 - loss: 0.6005
27136/36989 [=====================>........] - ETA: 1:34 - loss: 0.5996
27392/36989 [=====================>........] - ETA: 1:32 - loss: 0.5986
27648/36989 [=====================>........] - ETA: 1:29 - loss: 0.5978
27904/36989 [=====================>........] - ETA: 1:27 - loss: 0.5973
28160/36989 [=====================>........] - ETA: 1:24 - loss: 0.5968
28416/36989 [======================>.......] - ETA: 1:22 - loss: 0.5960
28672/36989 [======================>.......] - ETA: 1:19 - loss: 0.5955
28928/36989 [======================>.......] - ETA: 1:17 - loss: 0.5950
29184/36989 [======================>.......] - ETA: 1:14 - loss: 0.5943
29440/36989 [======================>.......] - ETA: 1:12 - loss: 0.5934
29696/36989 [=======================>......] - ETA: 1:09 - loss: 0.5928
29952/36989 [=======================>......] - ETA: 1:07 - loss: 0.5924
30208/36989 [=======================>......] - ETA: 1:04 - loss: 0.5921
30464/36989 [=======================>......] - ETA: 1:02 - loss: 0.5912
30720/36989 [=======================>......] - ETA: 1:00 - loss: 0.5903
30976/36989 [========================>.....] - ETA: 57s - loss: 0.5895 
31232/36989 [========================>.....] - ETA: 55s - loss: 0.5889
31488/36989 [========================>.....] - ETA: 52s - loss: 0.5884
31744/36989 [========================>.....] - ETA: 50s - loss: 0.5880
32000/36989 [========================>.....] - ETA: 47s - loss: 0.5873
32256/36989 [=========================>....] - ETA: 45s - loss: 0.5869
32512/36989 [=========================>....] - ETA: 42s - loss: 0.5867
32768/36989 [=========================>....] - ETA: 40s - loss: 0.5863
33024/36989 [=========================>....] - ETA: 37s - loss: 0.5856
33280/36989 [=========================>....] - ETA: 35s - loss: 0.5850
33536/36989 [==========================>...] - ETA: 32s - loss: 0.5848
33792/36989 [==========================>...] - ETA: 30s - loss: 0.5843
34048/36989 [==========================>...] - ETA: 28s - loss: 0.5839
34304/36989 [==========================>...] - ETA: 25s - loss: 0.5832
34560/36989 [===========================>..] - ETA: 23s - loss: 0.5830
34816/36989 [===========================>..] - ETA: 20s - loss: 0.5826
35072/36989 [===========================>..] - ETA: 18s - loss: 0.5822
35328/36989 [===========================>..] - ETA: 15s - loss: 0.5817
35584/36989 [===========================>..] - ETA: 13s - loss: 0.5814
35840/36989 [============================>.] - ETA: 10s - loss: 0.5808
36096/36989 [============================>.] - ETA: 8s - loss: 0.5801 
36352/36989 [============================>.] - ETA: 6s - loss: 0.5796
36608/36989 [============================>.] - ETA: 3s - loss: 0.5788
36864/36989 [============================>.] - ETA: 1s - loss: 0.5784
36989/36989 [==============================] - 359s 10ms/step - loss: 0.5783 - val_loss: 0.4249
---
Input python program: 
n = int(input())
nums = list(map(int, input().split()))
sum = 0
for num in nums:
    sum += num
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
num = int(input())
sum = 0
for i in [int(x) for x in input().split()]:
    sum += i
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
map(int,input())
array = list(map(int,input().split()))
sum = 0
for x in array:
    sum += x
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
def arraySum(array):
    total = 0
    for num in array:
        total += num
    return total

n = int(input())
arrayString = input().split()
arrayInt = []

for i in range(0,n):
    arrayInt.append(int(arrayString[i]))

print (arraySum(arrayInt))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
# Inputs
n = int(input())
array = input().split()

# Solution
array = list(map(int, array))
print(sum(array))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
Epoch: 1/10
Train on 36989 samples, validate on 1947 samples
Epoch 1/1

  256/36989 [..............................] - ETA: 5:50 - loss: 0.5076
  512/36989 [..............................] - ETA: 5:49 - loss: 0.5010
  768/36989 [..............................] - ETA: 5:44 - loss: 0.5092
 1024/36989 [..............................] - ETA: 5:42 - loss: 0.5107
 1280/36989 [>.............................] - ETA: 5:41 - loss: 0.5078
 1536/36989 [>.............................] - ETA: 5:38 - loss: 0.5060
 1792/36989 [>.............................] - ETA: 5:36 - loss: 0.5033
 2048/36989 [>.............................] - ETA: 5:32 - loss: 0.5062
 2304/36989 [>.............................] - ETA: 5:28 - loss: 0.5100
 2560/36989 [=>............................] - ETA: 5:25 - loss: 0.5039
 2816/36989 [=>............................] - ETA: 5:22 - loss: 0.5033
 3072/36989 [=>............................] - ETA: 5:20 - loss: 0.5062
 3328/36989 [=>............................] - ETA: 5:18 - loss: 0.5039
 3584/36989 [=>............................] - ETA: 5:15 - loss: 0.5080
 3840/36989 [==>...........................] - ETA: 5:12 - loss: 0.5086
 4096/36989 [==>...........................] - ETA: 5:10 - loss: 0.5094
 4352/36989 [==>...........................] - ETA: 5:07 - loss: 0.5078
 4608/36989 [==>...........................] - ETA: 5:05 - loss: 0.5074
 4864/36989 [==>...........................] - ETA: 5:02 - loss: 0.5079
 5120/36989 [===>..........................] - ETA: 5:00 - loss: 0.5076
 5376/36989 [===>..........................] - ETA: 4:58 - loss: 0.5071
 5632/36989 [===>..........................] - ETA: 4:55 - loss: 0.5071
 5888/36989 [===>..........................] - ETA: 4:52 - loss: 0.5066
 6144/36989 [===>..........................] - ETA: 4:50 - loss: 0.5055
 6400/36989 [====>.........................] - ETA: 4:47 - loss: 0.5057
 6656/36989 [====>.........................] - ETA: 4:45 - loss: 0.5047
 6912/36989 [====>.........................] - ETA: 4:43 - loss: 0.5037
 7168/36989 [====>.........................] - ETA: 4:40 - loss: 0.5031
 7424/36989 [=====>........................] - ETA: 4:38 - loss: 0.5018
 7680/36989 [=====>........................] - ETA: 4:35 - loss: 0.5019
 7936/36989 [=====>........................] - ETA: 4:33 - loss: 0.5014
 8192/36989 [=====>........................] - ETA: 4:31 - loss: 0.5011
 8448/36989 [=====>........................] - ETA: 4:28 - loss: 0.5001
 8704/36989 [======>.......................] - ETA: 4:26 - loss: 0.5001
 8960/36989 [======>.......................] - ETA: 4:24 - loss: 0.5001
 9216/36989 [======>.......................] - ETA: 4:21 - loss: 0.4989
 9472/36989 [======>.......................] - ETA: 4:19 - loss: 0.4986
 9728/36989 [======>.......................] - ETA: 4:16 - loss: 0.4985
 9984/36989 [=======>......................] - ETA: 4:14 - loss: 0.4990
10240/36989 [=======>......................] - ETA: 4:12 - loss: 0.4985
10496/36989 [=======>......................] - ETA: 4:09 - loss: 0.4986
10752/36989 [=======>......................] - ETA: 4:07 - loss: 0.4987
11008/36989 [=======>......................] - ETA: 4:04 - loss: 0.4988
11264/36989 [========>.....................] - ETA: 4:02 - loss: 0.4992
11520/36989 [========>.....................] - ETA: 3:59 - loss: 0.4993
11776/36989 [========>.....................] - ETA: 3:57 - loss: 0.5002
12032/36989 [========>.....................] - ETA: 3:54 - loss: 0.4994
12288/36989 [========>.....................] - ETA: 3:52 - loss: 0.4990
12544/36989 [=========>....................] - ETA: 3:49 - loss: 0.4994
12800/36989 [=========>....................] - ETA: 3:47 - loss: 0.4995
13056/36989 [=========>....................] - ETA: 3:44 - loss: 0.4985
13312/36989 [=========>....................] - ETA: 3:42 - loss: 0.4984
13568/36989 [==========>...................] - ETA: 3:39 - loss: 0.4986
13824/36989 [==========>...................] - ETA: 3:37 - loss: 0.4983
14080/36989 [==========>...................] - ETA: 3:34 - loss: 0.4985
14336/36989 [==========>...................] - ETA: 3:32 - loss: 0.4982
14592/36989 [==========>...................] - ETA: 3:30 - loss: 0.4988
14848/36989 [===========>..................] - ETA: 3:27 - loss: 0.4985
15104/36989 [===========>..................] - ETA: 3:25 - loss: 0.4983
15360/36989 [===========>..................] - ETA: 3:22 - loss: 0.4982
15616/36989 [===========>..................] - ETA: 3:20 - loss: 0.4987
15872/36989 [===========>..................] - ETA: 3:18 - loss: 0.4980
16128/36989 [============>.................] - ETA: 3:16 - loss: 0.4982
16384/36989 [============>.................] - ETA: 3:13 - loss: 0.4982
16640/36989 [============>.................] - ETA: 3:11 - loss: 0.4977
16896/36989 [============>.................] - ETA: 3:08 - loss: 0.4967
17152/36989 [============>.................] - ETA: 3:06 - loss: 0.4963
17408/36989 [=============>................] - ETA: 3:03 - loss: 0.4967
17664/36989 [=============>................] - ETA: 3:01 - loss: 0.4964
17920/36989 [=============>................] - ETA: 2:58 - loss: 0.4961
18176/36989 [=============>................] - ETA: 2:56 - loss: 0.4957
18432/36989 [=============>................] - ETA: 2:54 - loss: 0.4955
18688/36989 [==============>...............] - ETA: 2:51 - loss: 0.4955
18944/36989 [==============>...............] - ETA: 2:49 - loss: 0.4954
19200/36989 [==============>...............] - ETA: 2:46 - loss: 0.4953
19456/36989 [==============>...............] - ETA: 2:44 - loss: 0.4957
19712/36989 [==============>...............] - ETA: 2:41 - loss: 0.4955
19968/36989 [===============>..............] - ETA: 2:39 - loss: 0.4952
20224/36989 [===============>..............] - ETA: 2:37 - loss: 0.4946
20480/36989 [===============>..............] - ETA: 2:34 - loss: 0.4942
20736/36989 [===============>..............] - ETA: 2:32 - loss: 0.4942
20992/36989 [================>.............] - ETA: 2:29 - loss: 0.4941
21248/36989 [================>.............] - ETA: 2:27 - loss: 0.4942
21504/36989 [================>.............] - ETA: 2:25 - loss: 0.4939
21760/36989 [================>.............] - ETA: 2:22 - loss: 0.4935
22016/36989 [================>.............] - ETA: 2:20 - loss: 0.4935
22272/36989 [=================>............] - ETA: 2:17 - loss: 0.4932
22528/36989 [=================>............] - ETA: 2:15 - loss: 0.4928
22784/36989 [=================>............] - ETA: 2:12 - loss: 0.4925
23040/36989 [=================>............] - ETA: 2:10 - loss: 0.4922
23296/36989 [=================>............] - ETA: 2:08 - loss: 0.4919
23552/36989 [==================>...........] - ETA: 2:05 - loss: 0.4918
23808/36989 [==================>...........] - ETA: 2:03 - loss: 0.4919
24064/36989 [==================>...........] - ETA: 2:00 - loss: 0.4914
24320/36989 [==================>...........] - ETA: 1:58 - loss: 0.4915
24576/36989 [==================>...........] - ETA: 1:56 - loss: 0.4915
24832/36989 [===================>..........] - ETA: 1:53 - loss: 0.4913
25088/36989 [===================>..........] - ETA: 1:51 - loss: 0.4906
25344/36989 [===================>..........] - ETA: 1:48 - loss: 0.4905
25600/36989 [===================>..........] - ETA: 1:46 - loss: 0.4904
25856/36989 [===================>..........] - ETA: 1:44 - loss: 0.4905
26112/36989 [====================>.........] - ETA: 1:41 - loss: 0.4903
26368/36989 [====================>.........] - ETA: 1:39 - loss: 0.4897
26624/36989 [====================>.........] - ETA: 1:36 - loss: 0.4897
26880/36989 [====================>.........] - ETA: 1:34 - loss: 0.4895
27136/36989 [=====================>........] - ETA: 1:32 - loss: 0.4893
27392/36989 [=====================>........] - ETA: 1:29 - loss: 0.4890
27648/36989 [=====================>........] - ETA: 1:27 - loss: 0.4891
27904/36989 [=====================>........] - ETA: 1:24 - loss: 0.4892
28160/36989 [=====================>........] - ETA: 1:22 - loss: 0.4892
28416/36989 [======================>.......] - ETA: 1:20 - loss: 0.4892
28672/36989 [======================>.......] - ETA: 1:17 - loss: 0.4889
28928/36989 [======================>.......] - ETA: 1:15 - loss: 0.4885
29184/36989 [======================>.......] - ETA: 1:12 - loss: 0.4883
29440/36989 [======================>.......] - ETA: 1:10 - loss: 0.4885
29696/36989 [=======================>......] - ETA: 1:08 - loss: 0.4883
29952/36989 [=======================>......] - ETA: 1:05 - loss: 0.4884
30208/36989 [=======================>......] - ETA: 1:03 - loss: 0.4883
30464/36989 [=======================>......] - ETA: 1:00 - loss: 0.4881
30720/36989 [=======================>......] - ETA: 58s - loss: 0.4880 
30976/36989 [========================>.....] - ETA: 56s - loss: 0.4877
31232/36989 [========================>.....] - ETA: 53s - loss: 0.4878
31488/36989 [========================>.....] - ETA: 51s - loss: 0.4876
31744/36989 [========================>.....] - ETA: 48s - loss: 0.4874
32000/36989 [========================>.....] - ETA: 46s - loss: 0.4871
32256/36989 [=========================>....] - ETA: 44s - loss: 0.4869
32512/36989 [=========================>....] - ETA: 41s - loss: 0.4869
32768/36989 [=========================>....] - ETA: 39s - loss: 0.4867
33024/36989 [=========================>....] - ETA: 37s - loss: 0.4863
33280/36989 [=========================>....] - ETA: 34s - loss: 0.4863
33536/36989 [==========================>...] - ETA: 32s - loss: 0.4862
33792/36989 [==========================>...] - ETA: 29s - loss: 0.4859
34048/36989 [==========================>...] - ETA: 27s - loss: 0.4855
34304/36989 [==========================>...] - ETA: 25s - loss: 0.4853
34560/36989 [===========================>..] - ETA: 22s - loss: 0.4851
34816/36989 [===========================>..] - ETA: 20s - loss: 0.4850
35072/36989 [===========================>..] - ETA: 17s - loss: 0.4850
35328/36989 [===========================>..] - ETA: 15s - loss: 0.4848
35584/36989 [===========================>..] - ETA: 13s - loss: 0.4845
35840/36989 [============================>.] - ETA: 10s - loss: 0.4844
36096/36989 [============================>.] - ETA: 8s - loss: 0.4843 
36352/36989 [============================>.] - ETA: 5s - loss: 0.4844
36608/36989 [============================>.] - ETA: 3s - loss: 0.4842
36864/36989 [============================>.] - ETA: 1s - loss: 0.4839
36989/36989 [==============================] - 352s 10ms/step - loss: 0.4838 - val_loss: 0.3897
---
Input python program: 
n = int(input())
nums = list(map(int, input().split()))
sum = 0
for num in nums:
    sum += num
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n, , sum=0;
    cin>>n;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
num = int(input())
sum = 0
for i in [int(x) for x in input().split()]:
    sum += i
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n, , sum=0;
    cin>>n;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
map(int,input())
array = list(map(int,input().split()))
sum = 0
for x in array:
    sum += x
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n, , sum=0;
    cin>>n;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
def arraySum(array):
    total = 0
    for num in array:
        total += num
    return total

n = int(input())
arrayString = input().split()
arrayInt = []

for i in range(0,n):
    arrayInt.append(int(arrayString[i]))

print (arraySum(arrayInt))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n, , sum=0;
    cin>>n;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
# Inputs
n = int(input())
array = input().split()

# Solution
array = list(map(int, array))
print(sum(array))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n, , sum=0;
    cin>>n;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
Epoch: 2/10
Train on 36989 samples, validate on 1947 samples
Epoch 1/1

  256/36989 [..............................] - ETA: 6:02 - loss: 0.4294
  512/36989 [..............................] - ETA: 5:48 - loss: 0.4488
  768/36989 [..............................] - ETA: 5:44 - loss: 0.4514
 1024/36989 [..............................] - ETA: 5:39 - loss: 0.4468
 1280/36989 [>.............................] - ETA: 5:35 - loss: 0.4476
 1536/36989 [>.............................] - ETA: 5:35 - loss: 0.4469
 1792/36989 [>.............................] - ETA: 5:30 - loss: 0.4454
 2048/36989 [>.............................] - ETA: 5:28 - loss: 0.4504
 2304/36989 [>.............................] - ETA: 5:25 - loss: 0.4532
 2560/36989 [=>............................] - ETA: 5:23 - loss: 0.4545
 2816/36989 [=>............................] - ETA: 5:21 - loss: 0.4522
 3072/36989 [=>............................] - ETA: 5:18 - loss: 0.4517
 3328/36989 [=>............................] - ETA: 5:16 - loss: 0.4512
 3584/36989 [=>............................] - ETA: 5:13 - loss: 0.4523
 3840/36989 [==>...........................] - ETA: 5:10 - loss: 0.4538
 4096/36989 [==>...........................] - ETA: 5:08 - loss: 0.4532
 4352/36989 [==>...........................] - ETA: 5:06 - loss: 0.4520
 4608/36989 [==>...........................] - ETA: 5:04 - loss: 0.4504
 4864/36989 [==>...........................] - ETA: 5:01 - loss: 0.4516
 5120/36989 [===>..........................] - ETA: 4:58 - loss: 0.4509
 5376/36989 [===>..........................] - ETA: 4:56 - loss: 0.4519
 5632/36989 [===>..........................] - ETA: 4:54 - loss: 0.4518
 5888/36989 [===>..........................] - ETA: 4:52 - loss: 0.4521
 6144/36989 [===>..........................] - ETA: 4:49 - loss: 0.4527
 6400/36989 [====>.........................] - ETA: 4:47 - loss: 0.4527
 6656/36989 [====>.........................] - ETA: 4:45 - loss: 0.4520
 6912/36989 [====>.........................] - ETA: 4:42 - loss: 0.4521
 7168/36989 [====>.........................] - ETA: 4:40 - loss: 0.4529
 7424/36989 [=====>........................] - ETA: 4:38 - loss: 0.4530
 7680/36989 [=====>........................] - ETA: 4:35 - loss: 0.4532
 7936/36989 [=====>........................] - ETA: 4:33 - loss: 0.4537
 8192/36989 [=====>........................] - ETA: 4:30 - loss: 0.4535
 8448/36989 [=====>........................] - ETA: 4:28 - loss: 0.4543
 8704/36989 [======>.......................] - ETA: 4:25 - loss: 0.4542
 8960/36989 [======>.......................] - ETA: 4:23 - loss: 0.4543
 9216/36989 [======>.......................] - ETA: 4:21 - loss: 0.4536
 9472/36989 [======>.......................] - ETA: 4:18 - loss: 0.4537
 9728/36989 [======>.......................] - ETA: 4:16 - loss: 0.4539
 9984/36989 [=======>......................] - ETA: 4:13 - loss: 0.4541
10240/36989 [=======>......................] - ETA: 4:11 - loss: 0.4537
10496/36989 [=======>......................] - ETA: 4:08 - loss: 0.4538
10752/36989 [=======>......................] - ETA: 4:06 - loss: 0.4534
11008/36989 [=======>......................] - ETA: 4:03 - loss: 0.4531
11264/36989 [========>.....................] - ETA: 4:01 - loss: 0.4534
11520/36989 [========>.....................] - ETA: 3:58 - loss: 0.4527
11776/36989 [========>.....................] - ETA: 3:56 - loss: 0.4525
12032/36989 [========>.....................] - ETA: 3:53 - loss: 0.4521
12288/36989 [========>.....................] - ETA: 3:51 - loss: 0.4528
12544/36989 [=========>....................] - ETA: 3:48 - loss: 0.4524
12800/36989 [=========>....................] - ETA: 3:46 - loss: 0.4519
13056/36989 [=========>....................] - ETA: 3:44 - loss: 0.4515
13312/36989 [=========>....................] - ETA: 3:41 - loss: 0.4514
13568/36989 [==========>...................] - ETA: 3:39 - loss: 0.4510
13824/36989 [==========>...................] - ETA: 3:36 - loss: 0.4507
14080/36989 [==========>...................] - ETA: 3:34 - loss: 0.4501
14336/36989 [==========>...................] - ETA: 3:31 - loss: 0.4499
14592/36989 [==========>...................] - ETA: 3:29 - loss: 0.4494
14848/36989 [===========>..................] - ETA: 3:27 - loss: 0.4490
15104/36989 [===========>..................] - ETA: 3:24 - loss: 0.4488
15360/36989 [===========>..................] - ETA: 3:22 - loss: 0.4487
15616/36989 [===========>..................] - ETA: 3:19 - loss: 0.4486
15872/36989 [===========>..................] - ETA: 3:17 - loss: 0.4486
16128/36989 [============>.................] - ETA: 3:15 - loss: 0.4486
16384/36989 [============>.................] - ETA: 3:12 - loss: 0.4486
16640/36989 [============>.................] - ETA: 3:10 - loss: 0.4484
16896/36989 [============>.................] - ETA: 3:08 - loss: 0.4476
17152/36989 [============>.................] - ETA: 3:05 - loss: 0.4480
17408/36989 [=============>................] - ETA: 3:03 - loss: 0.4480
17664/36989 [=============>................] - ETA: 3:01 - loss: 0.4477
17920/36989 [=============>................] - ETA: 2:58 - loss: 0.4476
18176/36989 [=============>................] - ETA: 2:56 - loss: 0.4475
18432/36989 [=============>................] - ETA: 2:53 - loss: 0.4472
18688/36989 [==============>...............] - ETA: 2:51 - loss: 0.4474
18944/36989 [==============>...............] - ETA: 2:48 - loss: 0.4478
19200/36989 [==============>...............] - ETA: 2:46 - loss: 0.4482
19456/36989 [==============>...............] - ETA: 2:44 - loss: 0.4479
19712/36989 [==============>...............] - ETA: 2:41 - loss: 0.4480
19968/36989 [===============>..............] - ETA: 2:39 - loss: 0.4477
20224/36989 [===============>..............] - ETA: 2:37 - loss: 0.4476
20480/36989 [===============>..............] - ETA: 2:34 - loss: 0.4475
20736/36989 [===============>..............] - ETA: 2:32 - loss: 0.4474
20992/36989 [================>.............] - ETA: 2:29 - loss: 0.4473
21248/36989 [================>.............] - ETA: 2:27 - loss: 0.4471
21504/36989 [================>.............] - ETA: 2:25 - loss: 0.4469
21760/36989 [================>.............] - ETA: 2:22 - loss: 0.4466
22016/36989 [================>.............] - ETA: 2:20 - loss: 0.4462
22272/36989 [=================>............] - ETA: 2:17 - loss: 0.4457
22528/36989 [=================>............] - ETA: 2:15 - loss: 0.4456
22784/36989 [=================>............] - ETA: 2:12 - loss: 0.4458
23040/36989 [=================>............] - ETA: 2:10 - loss: 0.4454
23296/36989 [=================>............] - ETA: 2:08 - loss: 0.4451
23552/36989 [==================>...........] - ETA: 2:05 - loss: 0.4447
23808/36989 [==================>...........] - ETA: 2:03 - loss: 0.4444
24064/36989 [==================>...........] - ETA: 2:01 - loss: 0.4445
24320/36989 [==================>...........] - ETA: 1:58 - loss: 0.4441
24576/36989 [==================>...........] - ETA: 1:56 - loss: 0.4438
24832/36989 [===================>..........] - ETA: 1:53 - loss: 0.4440
25088/36989 [===================>..........] - ETA: 1:51 - loss: 0.4436
25344/36989 [===================>..........] - ETA: 1:48 - loss: 0.4435
25600/36989 [===================>..........] - ETA: 1:46 - loss: 0.4434
25856/36989 [===================>..........] - ETA: 1:44 - loss: 0.4432
26112/36989 [====================>.........] - ETA: 1:41 - loss: 0.4432
26368/36989 [====================>.........] - ETA: 1:39 - loss: 0.4430
26624/36989 [====================>.........] - ETA: 1:37 - loss: 0.4431
26880/36989 [====================>.........] - ETA: 1:34 - loss: 0.4426
27136/36989 [=====================>........] - ETA: 1:32 - loss: 0.4427
27392/36989 [=====================>........] - ETA: 1:29 - loss: 0.4427
27648/36989 [=====================>........] - ETA: 1:27 - loss: 0.4424
27904/36989 [=====================>........] - ETA: 1:25 - loss: 0.4424
28160/36989 [=====================>........] - ETA: 1:22 - loss: 0.4422
28416/36989 [======================>.......] - ETA: 1:20 - loss: 0.4421
28672/36989 [======================>.......] - ETA: 1:17 - loss: 0.4420
28928/36989 [======================>.......] - ETA: 1:15 - loss: 0.4418
29184/36989 [======================>.......] - ETA: 1:13 - loss: 0.4418
29440/36989 [======================>.......] - ETA: 1:10 - loss: 0.4416
29696/36989 [=======================>......] - ETA: 1:08 - loss: 0.4413
29952/36989 [=======================>......] - ETA: 1:05 - loss: 0.4414
30208/36989 [=======================>......] - ETA: 1:03 - loss: 0.4414
30464/36989 [=======================>......] - ETA: 1:01 - loss: 0.4416
30720/36989 [=======================>......] - ETA: 58s - loss: 0.4414 
30976/36989 [========================>.....] - ETA: 56s - loss: 0.4412
31232/36989 [========================>.....] - ETA: 53s - loss: 0.4409
31488/36989 [========================>.....] - ETA: 51s - loss: 0.4408
31744/36989 [========================>.....] - ETA: 49s - loss: 0.4407
32000/36989 [========================>.....] - ETA: 46s - loss: 0.4407
32256/36989 [=========================>....] - ETA: 44s - loss: 0.4404
32512/36989 [=========================>....] - ETA: 41s - loss: 0.4404
32768/36989 [=========================>....] - ETA: 39s - loss: 0.4400
33024/36989 [=========================>....] - ETA: 37s - loss: 0.4397
33280/36989 [=========================>....] - ETA: 34s - loss: 0.4395
33536/36989 [==========================>...] - ETA: 32s - loss: 0.4395
33792/36989 [==========================>...] - ETA: 29s - loss: 0.4394
34048/36989 [==========================>...] - ETA: 27s - loss: 0.4393
34304/36989 [==========================>...] - ETA: 25s - loss: 0.4391
34560/36989 [===========================>..] - ETA: 22s - loss: 0.4390
34816/36989 [===========================>..] - ETA: 20s - loss: 0.4389
35072/36989 [===========================>..] - ETA: 17s - loss: 0.4388
35328/36989 [===========================>..] - ETA: 15s - loss: 0.4386
35584/36989 [===========================>..] - ETA: 13s - loss: 0.4383
35840/36989 [============================>.] - ETA: 10s - loss: 0.4385
36096/36989 [============================>.] - ETA: 8s - loss: 0.4388 
36352/36989 [============================>.] - ETA: 5s - loss: 0.4386
36608/36989 [============================>.] - ETA: 3s - loss: 0.4386
36864/36989 [============================>.] - ETA: 1s - loss: 0.4387
36989/36989 [==============================] - 353s 10ms/step - loss: 0.4387 - val_loss: 0.3636
---
Input python program: 
n = int(input())
nums = list(map(int, input().split()))
sum = 0
for num in nums:
    sum += num
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n;
    cin>>n;
    int a[n];
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
num = int(input())
sum = 0
for i in [int(x) for x in input().split()]:
    sum += i
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n;
    cin>>n;
    int a[n];
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
map(int,input())
array = list(map(int,input().split()))
sum = 0
for x in array:
    sum += x
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n;
    cin>>n;
    int a[n];
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
def arraySum(array):
    total = 0
    for num in array:
        total += num
    return total

n = int(input())
arrayString = input().split()
arrayInt = []

for i in range(0,n):
    arrayInt.append(int(arrayString[i]))

print (arraySum(arrayInt))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n;
    cin>>n;
    int a[n];
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
# Inputs
n = int(input())
array = input().split()

# Solution
array = list(map(int, array))
print(sum(array))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n;
    cin>>n;
    int a[n];
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
Epoch: 3/10
Train on 36989 samples, validate on 1947 samples
Epoch 1/1

  256/36989 [..............................] - ETA: 5:42 - loss: 0.4317
  512/36989 [..............................] - ETA: 5:41 - loss: 0.4208
  768/36989 [..............................] - ETA: 5:36 - loss: 0.4178
 1024/36989 [..............................] - ETA: 5:34 - loss: 0.4195
 1280/36989 [>.............................] - ETA: 5:35 - loss: 0.4212
 1536/36989 [>.............................] - ETA: 5:33 - loss: 0.4201
 1792/36989 [>.............................] - ETA: 5:32 - loss: 0.4225
 2048/36989 [>.............................] - ETA: 5:31 - loss: 0.4204
 2304/36989 [>.............................] - ETA: 5:28 - loss: 0.4194
 2560/36989 [=>............................] - ETA: 5:24 - loss: 0.4166
 2816/36989 [=>............................] - ETA: 5:21 - loss: 0.4163
 3072/36989 [=>............................] - ETA: 5:18 - loss: 0.4156
 3328/36989 [=>............................] - ETA: 5:16 - loss: 0.4162
 3584/36989 [=>............................] - ETA: 5:13 - loss: 0.4161
 3840/36989 [==>...........................] - ETA: 5:11 - loss: 0.4153
 4096/36989 [==>...........................] - ETA: 5:08 - loss: 0.4162
 4352/36989 [==>...........................] - ETA: 5:05 - loss: 0.4152
 4608/36989 [==>...........................] - ETA: 5:02 - loss: 0.4142
 4864/36989 [==>...........................] - ETA: 5:00 - loss: 0.4150
 5120/36989 [===>..........................] - ETA: 4:57 - loss: 0.4147
 5376/36989 [===>..........................] - ETA: 4:55 - loss: 0.4142
 5632/36989 [===>..........................] - ETA: 4:52 - loss: 0.4140
 5888/36989 [===>..........................] - ETA: 4:49 - loss: 0.4151
 6144/36989 [===>..........................] - ETA: 4:46 - loss: 0.4139
 6400/36989 [====>.........................] - ETA: 4:44 - loss: 0.4139
 6656/36989 [====>.........................] - ETA: 4:42 - loss: 0.4133
 6912/36989 [====>.........................] - ETA: 4:39 - loss: 0.4137
 7168/36989 [====>.........................] - ETA: 4:37 - loss: 0.4131
 7424/36989 [=====>........................] - ETA: 4:35 - loss: 0.4131
 7680/36989 [=====>........................] - ETA: 4:32 - loss: 0.4119
 7936/36989 [=====>........................] - ETA: 4:30 - loss: 0.4113
 8192/36989 [=====>........................] - ETA: 4:28 - loss: 0.4108
 8448/36989 [=====>........................] - ETA: 4:25 - loss: 0.4112
 8704/36989 [======>.......................] - ETA: 4:23 - loss: 0.4108
 8960/36989 [======>.......................] - ETA: 4:20 - loss: 0.4105
 9216/36989 [======>.......................] - ETA: 4:18 - loss: 0.4096
 9472/36989 [======>.......................] - ETA: 4:16 - loss: 0.4094
 9728/36989 [======>.......................] - ETA: 4:13 - loss: 0.4091
 9984/36989 [=======>......................] - ETA: 4:11 - loss: 0.4090
10240/36989 [=======>......................] - ETA: 4:09 - loss: 0.4087
10496/36989 [=======>......................] - ETA: 4:06 - loss: 0.4086
10752/36989 [=======>......................] - ETA: 4:04 - loss: 0.4079
11008/36989 [=======>......................] - ETA: 4:01 - loss: 0.4080
11264/36989 [========>.....................] - ETA: 3:59 - loss: 0.4083
11520/36989 [========>.....................] - ETA: 3:57 - loss: 0.4083
11776/36989 [========>.....................] - ETA: 3:54 - loss: 0.4079
12032/36989 [========>.....................] - ETA: 3:52 - loss: 0.4084
12288/36989 [========>.....................] - ETA: 3:49 - loss: 0.4085
12544/36989 [=========>....................] - ETA: 3:47 - loss: 0.4085
12800/36989 [=========>....................] - ETA: 3:45 - loss: 0.4089
13056/36989 [=========>....................] - ETA: 3:42 - loss: 0.4093
13312/36989 [=========>....................] - ETA: 3:40 - loss: 0.4094
13568/36989 [==========>...................] - ETA: 3:38 - loss: 0.4087
13824/36989 [==========>...................] - ETA: 3:35 - loss: 0.4094
14080/36989 [==========>...................] - ETA: 3:33 - loss: 0.4095
14336/36989 [==========>...................] - ETA: 3:30 - loss: 0.4099
14592/36989 [==========>...................] - ETA: 3:28 - loss: 0.4096
14848/36989 [===========>..................] - ETA: 3:26 - loss: 0.4093
15104/36989 [===========>..................] - ETA: 3:23 - loss: 0.4094
15360/36989 [===========>..................] - ETA: 3:21 - loss: 0.4094
15616/36989 [===========>..................] - ETA: 3:19 - loss: 0.4092
15872/36989 [===========>..................] - ETA: 3:16 - loss: 0.4091
16128/36989 [============>.................] - ETA: 3:14 - loss: 0.4089
16384/36989 [============>.................] - ETA: 3:12 - loss: 0.4084
16640/36989 [============>.................] - ETA: 3:09 - loss: 0.4082
16896/36989 [============>.................] - ETA: 3:07 - loss: 0.4078
17152/36989 [============>.................] - ETA: 3:05 - loss: 0.4078
17408/36989 [=============>................] - ETA: 3:02 - loss: 0.4079
17664/36989 [=============>................] - ETA: 3:00 - loss: 0.4078
17920/36989 [=============>................] - ETA: 2:57 - loss: 0.4073
18176/36989 [=============>................] - ETA: 2:55 - loss: 0.4072
18432/36989 [=============>................] - ETA: 2:53 - loss: 0.4069
18688/36989 [==============>...............] - ETA: 2:50 - loss: 0.4068
18944/36989 [==============>...............] - ETA: 2:48 - loss: 0.4066
19200/36989 [==============>...............] - ETA: 2:46 - loss: 0.4064
19456/36989 [==============>...............] - ETA: 2:43 - loss: 0.4064
19712/36989 [==============>...............] - ETA: 2:41 - loss: 0.4064
19968/36989 [===============>..............] - ETA: 2:39 - loss: 0.4063
20224/36989 [===============>..............] - ETA: 2:36 - loss: 0.4062
20480/36989 [===============>..............] - ETA: 2:34 - loss: 0.4061
20736/36989 [===============>..............] - ETA: 2:32 - loss: 0.4064
20992/36989 [================>.............] - ETA: 2:29 - loss: 0.4062
21248/36989 [================>.............] - ETA: 2:27 - loss: 0.4060
21504/36989 [================>.............] - ETA: 2:24 - loss: 0.4059
21760/36989 [================>.............] - ETA: 2:22 - loss: 0.4056
22016/36989 [================>.............] - ETA: 2:20 - loss: 0.4056
22272/36989 [=================>............] - ETA: 2:17 - loss: 0.4055
22528/36989 [=================>............] - ETA: 2:15 - loss: 0.4058
22784/36989 [=================>............] - ETA: 2:12 - loss: 0.4058
23040/36989 [=================>............] - ETA: 2:10 - loss: 0.4056
23296/36989 [=================>............] - ETA: 2:08 - loss: 0.4053
23552/36989 [==================>...........] - ETA: 2:05 - loss: 0.4052
23808/36989 [==================>...........] - ETA: 2:03 - loss: 0.4050
24064/36989 [==================>...........] - ETA: 2:00 - loss: 0.4048
24320/36989 [==================>...........] - ETA: 1:58 - loss: 0.4047
24576/36989 [==================>...........] - ETA: 1:56 - loss: 0.4047
24832/36989 [===================>..........] - ETA: 1:53 - loss: 0.4045
25088/36989 [===================>..........] - ETA: 1:51 - loss: 0.4043
25344/36989 [===================>..........] - ETA: 1:49 - loss: 0.4043
25600/36989 [===================>..........] - ETA: 1:46 - loss: 0.4041
25856/36989 [===================>..........] - ETA: 1:44 - loss: 0.4042
26112/36989 [====================>.........] - ETA: 1:41 - loss: 0.4041
26368/36989 [====================>.........] - ETA: 1:39 - loss: 0.4038
26624/36989 [====================>.........] - ETA: 1:37 - loss: 0.4037
26880/36989 [====================>.........] - ETA: 1:34 - loss: 0.4036
27136/36989 [=====================>........] - ETA: 1:32 - loss: 0.4031
27392/36989 [=====================>........] - ETA: 1:29 - loss: 0.4030
27648/36989 [=====================>........] - ETA: 1:27 - loss: 0.4028
27904/36989 [=====================>........] - ETA: 1:25 - loss: 0.4027
28160/36989 [=====================>........] - ETA: 1:22 - loss: 0.4028
28416/36989 [======================>.......] - ETA: 1:20 - loss: 0.4027
28672/36989 [======================>.......] - ETA: 1:18 - loss: 0.4024
28928/36989 [======================>.......] - ETA: 1:15 - loss: 0.4023
29184/36989 [======================>.......] - ETA: 1:13 - loss: 0.4021
29440/36989 [======================>.......] - ETA: 1:10 - loss: 0.4018
29696/36989 [=======================>......] - ETA: 1:08 - loss: 0.4018
29952/36989 [=======================>......] - ETA: 1:06 - loss: 0.4018
30208/36989 [=======================>......] - ETA: 1:03 - loss: 0.4017
30464/36989 [=======================>......] - ETA: 1:01 - loss: 0.4014
30720/36989 [=======================>......] - ETA: 58s - loss: 0.4013 
30976/36989 [========================>.....] - ETA: 56s - loss: 0.4014
31232/36989 [========================>.....] - ETA: 54s - loss: 0.4013
31488/36989 [========================>.....] - ETA: 51s - loss: 0.4012
31744/36989 [========================>.....] - ETA: 49s - loss: 0.4011
32000/36989 [========================>.....] - ETA: 46s - loss: 0.4010
32256/36989 [=========================>....] - ETA: 44s - loss: 0.4008
32512/36989 [=========================>....] - ETA: 42s - loss: 0.4006
32768/36989 [=========================>....] - ETA: 39s - loss: 0.4006
33024/36989 [=========================>....] - ETA: 37s - loss: 0.4005
33280/36989 [=========================>....] - ETA: 34s - loss: 0.4002
33536/36989 [==========================>...] - ETA: 32s - loss: 0.4001
33792/36989 [==========================>...] - ETA: 30s - loss: 0.4000
34048/36989 [==========================>...] - ETA: 27s - loss: 0.3999
34304/36989 [==========================>...] - ETA: 25s - loss: 0.3998
34560/36989 [===========================>..] - ETA: 22s - loss: 0.3995
34816/36989 [===========================>..] - ETA: 20s - loss: 0.3994
35072/36989 [===========================>..] - ETA: 18s - loss: 0.3994
35328/36989 [===========================>..] - ETA: 15s - loss: 0.3992
35584/36989 [===========================>..] - ETA: 13s - loss: 0.3991
35840/36989 [============================>.] - ETA: 10s - loss: 0.3991
36096/36989 [============================>.] - ETA: 8s - loss: 0.3989 
36352/36989 [============================>.] - ETA: 5s - loss: 0.3988
36608/36989 [============================>.] - ETA: 3s - loss: 0.3986
36864/36989 [============================>.] - ETA: 1s - loss: 0.3985
36989/36989 [==============================] - 354s 10ms/step - loss: 0.3984 - val_loss: 0.3418
---
Input python program: 
n = int(input())
nums = list(map(int, input().split()))
sum = 0
for num in nums:
    sum += num
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n;
    cin>>n;
    int a[n];
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
num = int(input())
sum = 0
for i in [int(x) for x in input().split()]:
    sum += i
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n;
    cin>>n;
    int a[n];
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
map(int,input())
array = list(map(int,input().split()))
sum = 0
for x in array:
    sum += x
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n;
    cin>>n;
    int a[n];
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
def arraySum(array):
    total = 0
    for num in array:
        total += num
    return total

n = int(input())
arrayString = input().split()
arrayInt = []

for i in range(0,n):
    arrayInt.append(int(arrayString[i]))

print (arraySum(arrayInt))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n;
    cin>>n;
    int a[n];
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
# Inputs
n = int(input())
array = input().split()

# Solution
array = list(map(int, array))
print(sum(array))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n;
    cin>>n;
    int a[n];
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
Epoch: 4/10
Train on 36989 samples, validate on 1947 samples
Epoch 1/1

  256/36989 [..............................] - ETA: 5:29 - loss: 0.4244
  512/36989 [..............................] - ETA: 5:35 - loss: 0.3969
  768/36989 [..............................] - ETA: 5:31 - loss: 0.3970
 1024/36989 [..............................] - ETA: 5:30 - loss: 0.3991
 1280/36989 [>.............................] - ETA: 5:28 - loss: 0.3938
 1536/36989 [>.............................] - ETA: 5:27 - loss: 0.3925
 1792/36989 [>.............................] - ETA: 5:23 - loss: 0.3880
 2048/36989 [>.............................] - ETA: 5:23 - loss: 0.3874
 2304/36989 [>.............................] - ETA: 5:21 - loss: 0.3836
 2560/36989 [=>............................] - ETA: 5:18 - loss: 0.3806
 2816/36989 [=>............................] - ETA: 5:16 - loss: 0.3810
 3072/36989 [=>............................] - ETA: 5:15 - loss: 0.3818
 3328/36989 [=>............................] - ETA: 5:13 - loss: 0.3818
 3584/36989 [=>............................] - ETA: 5:10 - loss: 0.3820
 3840/36989 [==>...........................] - ETA: 5:08 - loss: 0.3809
 4096/36989 [==>...........................] - ETA: 5:05 - loss: 0.3811
 4352/36989 [==>...........................] - ETA: 5:03 - loss: 0.3813
 4608/36989 [==>...........................] - ETA: 5:01 - loss: 0.3809
 4864/36989 [==>...........................] - ETA: 4:58 - loss: 0.3823
 5120/36989 [===>..........................] - ETA: 4:56 - loss: 0.3823
 5376/36989 [===>..........................] - ETA: 4:54 - loss: 0.3812
 5632/36989 [===>..........................] - ETA: 4:52 - loss: 0.3816
 5888/36989 [===>..........................] - ETA: 4:49 - loss: 0.3817
 6144/36989 [===>..........................] - ETA: 4:47 - loss: 0.3812
 6400/36989 [====>.........................] - ETA: 4:44 - loss: 0.3805
 6656/36989 [====>.........................] - ETA: 4:42 - loss: 0.3805
 6912/36989 [====>.........................] - ETA: 4:40 - loss: 0.3804
 7168/36989 [====>.........................] - ETA: 4:37 - loss: 0.3806
 7424/36989 [=====>........................] - ETA: 4:35 - loss: 0.3797
 7680/36989 [=====>........................] - ETA: 4:33 - loss: 0.3789
 7936/36989 [=====>........................] - ETA: 4:30 - loss: 0.3784
 8192/36989 [=====>........................] - ETA: 4:28 - loss: 0.3779
 8448/36989 [=====>........................] - ETA: 4:26 - loss: 0.3776
 8704/36989 [======>.......................] - ETA: 4:23 - loss: 0.3768
 8960/36989 [======>.......................] - ETA: 4:21 - loss: 0.3760
 9216/36989 [======>.......................] - ETA: 4:18 - loss: 0.3767
 9472/36989 [======>.......................] - ETA: 4:16 - loss: 0.3763
 9728/36989 [======>.......................] - ETA: 4:14 - loss: 0.3769
 9984/36989 [=======>......................] - ETA: 4:11 - loss: 0.3763
10240/36989 [=======>......................] - ETA: 4:09 - loss: 0.3758
10496/36989 [=======>......................] - ETA: 4:06 - loss: 0.3756
10752/36989 [=======>......................] - ETA: 4:04 - loss: 0.3756
11008/36989 [=======>......................] - ETA: 4:01 - loss: 0.3755
11264/36989 [========>.....................] - ETA: 3:59 - loss: 0.3754
11520/36989 [========>.....................] - ETA: 3:57 - loss: 0.3750
11776/36989 [========>.....................] - ETA: 3:54 - loss: 0.3757
12032/36989 [========>.....................] - ETA: 3:52 - loss: 0.3755
12288/36989 [========>.....................] - ETA: 3:49 - loss: 0.3760
12544/36989 [=========>....................] - ETA: 3:47 - loss: 0.3758
12800/36989 [=========>....................] - ETA: 3:45 - loss: 0.3756
13056/36989 [=========>....................] - ETA: 3:42 - loss: 0.3751
13312/36989 [=========>....................] - ETA: 3:40 - loss: 0.3746
13568/36989 [==========>...................] - ETA: 3:37 - loss: 0.3745
13824/36989 [==========>...................] - ETA: 3:35 - loss: 0.3743
14080/36989 [==========>...................] - ETA: 3:32 - loss: 0.3742
14336/36989 [==========>...................] - ETA: 3:30 - loss: 0.3740
14592/36989 [==========>...................] - ETA: 3:28 - loss: 0.3736
14848/36989 [===========>..................] - ETA: 3:26 - loss: 0.3734
15104/36989 [===========>..................] - ETA: 3:23 - loss: 0.3732
15360/36989 [===========>..................] - ETA: 3:21 - loss: 0.3728
15616/36989 [===========>..................] - ETA: 3:18 - loss: 0.3725
15872/36989 [===========>..................] - ETA: 3:16 - loss: 0.3720
16128/36989 [============>.................] - ETA: 3:13 - loss: 0.3718
16384/36989 [============>.................] - ETA: 3:11 - loss: 0.3716
16640/36989 [============>.................] - ETA: 3:08 - loss: 0.3717
16896/36989 [============>.................] - ETA: 3:06 - loss: 0.3717
17152/36989 [============>.................] - ETA: 3:04 - loss: 0.3719
17408/36989 [=============>................] - ETA: 3:01 - loss: 0.3719
17664/36989 [=============>................] - ETA: 2:59 - loss: 0.3720
17920/36989 [=============>................] - ETA: 2:57 - loss: 0.3720
18176/36989 [=============>................] - ETA: 2:54 - loss: 0.3718
18432/36989 [=============>................] - ETA: 2:52 - loss: 0.3721
18688/36989 [==============>...............] - ETA: 2:49 - loss: 0.3720
18944/36989 [==============>...............] - ETA: 2:47 - loss: 0.3720
19200/36989 [==============>...............] - ETA: 2:45 - loss: 0.3723
19456/36989 [==============>...............] - ETA: 2:42 - loss: 0.3723
19712/36989 [==============>...............] - ETA: 2:40 - loss: 0.3723
19968/36989 [===============>..............] - ETA: 2:38 - loss: 0.3722
20224/36989 [===============>..............] - ETA: 2:35 - loss: 0.3718
20480/36989 [===============>..............] - ETA: 2:33 - loss: 0.3718
20736/36989 [===============>..............] - ETA: 2:31 - loss: 0.3718
20992/36989 [================>.............] - ETA: 2:28 - loss: 0.3716
21248/36989 [================>.............] - ETA: 2:26 - loss: 0.3714
21504/36989 [================>.............] - ETA: 2:24 - loss: 0.3712
21760/36989 [================>.............] - ETA: 2:21 - loss: 0.3710
22016/36989 [================>.............] - ETA: 2:19 - loss: 0.3708
22272/36989 [=================>............] - ETA: 2:16 - loss: 0.3703
22528/36989 [=================>............] - ETA: 2:14 - loss: 0.3701
22784/36989 [=================>............] - ETA: 2:12 - loss: 0.3699
23040/36989 [=================>............] - ETA: 2:09 - loss: 0.3700
23296/36989 [=================>............] - ETA: 2:07 - loss: 0.3699
23552/36989 [==================>...........] - ETA: 2:05 - loss: 0.3699
23808/36989 [==================>...........] - ETA: 2:02 - loss: 0.3699
24064/36989 [==================>...........] - ETA: 2:00 - loss: 0.3700
24320/36989 [==================>...........] - ETA: 1:57 - loss: 0.3699
24576/36989 [==================>...........] - ETA: 1:55 - loss: 0.3699
24832/36989 [===================>..........] - ETA: 1:53 - loss: 0.3697
25088/36989 [===================>..........] - ETA: 1:50 - loss: 0.3693
25344/36989 [===================>..........] - ETA: 1:48 - loss: 0.3693
25600/36989 [===================>..........] - ETA: 1:45 - loss: 0.3695
25856/36989 [===================>..........] - ETA: 1:43 - loss: 0.3693
26112/36989 [====================>.........] - ETA: 1:41 - loss: 0.3692
26368/36989 [====================>.........] - ETA: 1:38 - loss: 0.3689
26624/36989 [====================>.........] - ETA: 1:36 - loss: 0.3688
26880/36989 [====================>.........] - ETA: 1:33 - loss: 0.3686
27136/36989 [=====================>........] - ETA: 1:31 - loss: 0.3683
27392/36989 [=====================>........] - ETA: 1:29 - loss: 0.3682
27648/36989 [=====================>........] - ETA: 1:26 - loss: 0.3684
27904/36989 [=====================>........] - ETA: 1:24 - loss: 0.3683
28160/36989 [=====================>........] - ETA: 1:22 - loss: 0.3684
28416/36989 [======================>.......] - ETA: 1:19 - loss: 0.3682
28672/36989 [======================>.......] - ETA: 1:17 - loss: 0.3681
28928/36989 [======================>.......] - ETA: 1:14 - loss: 0.3680
29184/36989 [======================>.......] - ETA: 1:12 - loss: 0.3679
29440/36989 [======================>.......] - ETA: 1:10 - loss: 0.3678
29696/36989 [=======================>......] - ETA: 1:07 - loss: 0.3676
29952/36989 [=======================>......] - ETA: 1:05 - loss: 0.3672
30208/36989 [=======================>......] - ETA: 1:03 - loss: 0.3671
30464/36989 [=======================>......] - ETA: 1:00 - loss: 0.3669
30720/36989 [=======================>......] - ETA: 58s - loss: 0.3669 
30976/36989 [========================>.....] - ETA: 55s - loss: 0.3669
31232/36989 [========================>.....] - ETA: 53s - loss: 0.3668
31488/36989 [========================>.....] - ETA: 51s - loss: 0.3667
31744/36989 [========================>.....] - ETA: 48s - loss: 0.3665
32000/36989 [========================>.....] - ETA: 46s - loss: 0.3666
32256/36989 [=========================>....] - ETA: 43s - loss: 0.3664
32512/36989 [=========================>....] - ETA: 41s - loss: 0.3661
32768/36989 [=========================>....] - ETA: 39s - loss: 0.3660
33024/36989 [=========================>....] - ETA: 36s - loss: 0.3658
33280/36989 [=========================>....] - ETA: 34s - loss: 0.3658
33536/36989 [==========================>...] - ETA: 32s - loss: 0.3659
33792/36989 [==========================>...] - ETA: 29s - loss: 0.3659
34048/36989 [==========================>...] - ETA: 27s - loss: 0.3656
34304/36989 [==========================>...] - ETA: 24s - loss: 0.3656
34560/36989 [===========================>..] - ETA: 22s - loss: 0.3655
34816/36989 [===========================>..] - ETA: 20s - loss: 0.3654
35072/36989 [===========================>..] - ETA: 17s - loss: 0.3651
35328/36989 [===========================>..] - ETA: 15s - loss: 0.3649
35584/36989 [===========================>..] - ETA: 13s - loss: 0.3649
35840/36989 [============================>.] - ETA: 10s - loss: 0.3648
36096/36989 [============================>.] - ETA: 8s - loss: 0.3647 
36352/36989 [============================>.] - ETA: 5s - loss: 0.3647
36608/36989 [============================>.] - ETA: 3s - loss: 0.3647
36864/36989 [============================>.] - ETA: 1s - loss: 0.3646
36989/36989 [==============================] - 350s 9ms/step - loss: 0.3645 - val_loss: 0.3233
---
Input python program: 
n = int(input())
nums = list(map(int, input().split()))
sum = 0
for num in nums:
    sum += num
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a[i];
        sum=sum+a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
num = int(input())
sum = 0
for i in [int(x) for x in input().split()]:
    sum += i
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a[i];
        sum=sum+a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
map(int,input())
array = list(map(int,input().split()))
sum = 0
for x in array:
    sum += x
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a[i];
        sum=sum+a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
def arraySum(array):
    total = 0
    for num in array:
        total += num
    return total

n = int(input())
arrayString = input().split()
arrayInt = []

for i in range(0,n):
    arrayInt.append(int(arrayString[i]))

print (arraySum(arrayInt))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a[i];
        sum=sum+a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
# Inputs
n = int(input())
array = input().split()

# Solution
array = list(map(int, array))
print(sum(array))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a[i];
        sum=sum+a[i];
    }
    cout<<sum;
    return 0;
}»
Epoch: 5/10
Train on 36989 samples, validate on 1947 samples
Epoch 1/1

  256/36989 [..............................] - ETA: 5:49 - loss: 0.3766
  512/36989 [..............................] - ETA: 5:48 - loss: 0.3604
  768/36989 [..............................] - ETA: 5:45 - loss: 0.3533
 1024/36989 [..............................] - ETA: 5:40 - loss: 0.3510
 1280/36989 [>.............................] - ETA: 5:39 - loss: 0.3523
 1536/36989 [>.............................] - ETA: 5:36 - loss: 0.3494
 1792/36989 [>.............................] - ETA: 5:34 - loss: 0.3496
 2048/36989 [>.............................] - ETA: 5:31 - loss: 0.3496
 2304/36989 [>.............................] - ETA: 5:28 - loss: 0.3481
 2560/36989 [=>............................] - ETA: 5:26 - loss: 0.3508
 2816/36989 [=>............................] - ETA: 5:24 - loss: 0.3481
 3072/36989 [=>............................] - ETA: 5:21 - loss: 0.3488
 3328/36989 [=>............................] - ETA: 5:19 - loss: 0.3478
 3584/36989 [=>............................] - ETA: 5:16 - loss: 0.3479
 3840/36989 [==>...........................] - ETA: 5:14 - loss: 0.3471
 4096/36989 [==>...........................] - ETA: 5:10 - loss: 0.3459
 4352/36989 [==>...........................] - ETA: 5:08 - loss: 0.3452
 4608/36989 [==>...........................] - ETA: 5:06 - loss: 0.3455
 4864/36989 [==>...........................] - ETA: 5:03 - loss: 0.3440
 5120/36989 [===>..........................] - ETA: 5:01 - loss: 0.3438
 5376/36989 [===>..........................] - ETA: 4:58 - loss: 0.3432
 5632/36989 [===>..........................] - ETA: 4:55 - loss: 0.3424
 5888/36989 [===>..........................] - ETA: 4:53 - loss: 0.3415
 6144/36989 [===>..........................] - ETA: 4:50 - loss: 0.3421
 6400/36989 [====>.........................] - ETA: 4:48 - loss: 0.3423
 6656/36989 [====>.........................] - ETA: 4:46 - loss: 0.3426
 6912/36989 [====>.........................] - ETA: 4:43 - loss: 0.3424
 7168/36989 [====>.........................] - ETA: 4:41 - loss: 0.3419
 7424/36989 [=====>........................] - ETA: 4:38 - loss: 0.3418
 7680/36989 [=====>........................] - ETA: 4:35 - loss: 0.3423
 7936/36989 [=====>........................] - ETA: 4:33 - loss: 0.3421
 8192/36989 [=====>........................] - ETA: 4:30 - loss: 0.3424
 8448/36989 [=====>........................] - ETA: 4:27 - loss: 0.3428
 8704/36989 [======>.......................] - ETA: 4:25 - loss: 0.3431
 8960/36989 [======>.......................] - ETA: 4:23 - loss: 0.3426
 9216/36989 [======>.......................] - ETA: 4:20 - loss: 0.3427
 9472/36989 [======>.......................] - ETA: 4:18 - loss: 0.3428
 9728/36989 [======>.......................] - ETA: 4:16 - loss: 0.3427
 9984/36989 [=======>......................] - ETA: 4:13 - loss: 0.3427
10240/36989 [=======>......................] - ETA: 4:11 - loss: 0.3425
10496/36989 [=======>......................] - ETA: 4:08 - loss: 0.3430
10752/36989 [=======>......................] - ETA: 4:06 - loss: 0.3435
11008/36989 [=======>......................] - ETA: 4:03 - loss: 0.3438
11264/36989 [========>.....................] - ETA: 4:01 - loss: 0.3438
11520/36989 [========>.....................] - ETA: 3:58 - loss: 0.3437
11776/36989 [========>.....................] - ETA: 3:56 - loss: 0.3434
12032/36989 [========>.....................] - ETA: 3:54 - loss: 0.3433
12288/36989 [========>.....................] - ETA: 3:51 - loss: 0.3431
12544/36989 [=========>....................] - ETA: 3:49 - loss: 0.3434
12800/36989 [=========>....................] - ETA: 3:46 - loss: 0.3429
13056/36989 [=========>....................] - ETA: 3:44 - loss: 0.3429
13312/36989 [=========>....................] - ETA: 3:42 - loss: 0.3429
13568/36989 [==========>...................] - ETA: 3:39 - loss: 0.3426
13824/36989 [==========>...................] - ETA: 3:37 - loss: 0.3427
14080/36989 [==========>...................] - ETA: 3:35 - loss: 0.3428
14336/36989 [==========>...................] - ETA: 3:32 - loss: 0.3426
14592/36989 [==========>...................] - ETA: 3:30 - loss: 0.3428
14848/36989 [===========>..................] - ETA: 3:27 - loss: 0.3429
15104/36989 [===========>..................] - ETA: 3:25 - loss: 0.3426
15360/36989 [===========>..................] - ETA: 3:22 - loss: 0.3423
15616/36989 [===========>..................] - ETA: 3:20 - loss: 0.3425
15872/36989 [===========>..................] - ETA: 3:18 - loss: 0.3425
16128/36989 [============>.................] - ETA: 3:15 - loss: 0.3426
16384/36989 [============>.................] - ETA: 3:13 - loss: 0.3431
16640/36989 [============>.................] - ETA: 3:10 - loss: 0.3430
16896/36989 [============>.................] - ETA: 3:08 - loss: 0.3427
17152/36989 [============>.................] - ETA: 3:06 - loss: 0.3425
17408/36989 [=============>................] - ETA: 3:03 - loss: 0.3423
17664/36989 [=============>................] - ETA: 3:01 - loss: 0.3424
17920/36989 [=============>................] - ETA: 2:58 - loss: 0.3424
18176/36989 [=============>................] - ETA: 2:56 - loss: 0.3423
18432/36989 [=============>................] - ETA: 2:53 - loss: 0.3420
18688/36989 [==============>...............] - ETA: 2:51 - loss: 0.3419
18944/36989 [==============>...............] - ETA: 2:49 - loss: 0.3419
19200/36989 [==============>...............] - ETA: 2:46 - loss: 0.3418
19456/36989 [==============>...............] - ETA: 2:44 - loss: 0.3418
19712/36989 [==============>...............] - ETA: 2:41 - loss: 0.3418
19968/36989 [===============>..............] - ETA: 2:39 - loss: 0.3414
20224/36989 [===============>..............] - ETA: 2:37 - loss: 0.3409
20480/36989 [===============>..............] - ETA: 2:34 - loss: 0.3407
20736/36989 [===============>..............] - ETA: 2:32 - loss: 0.3407
20992/36989 [================>.............] - ETA: 2:29 - loss: 0.3405
21248/36989 [================>.............] - ETA: 2:27 - loss: 0.3407
21504/36989 [================>.............] - ETA: 2:25 - loss: 0.3406
21760/36989 [================>.............] - ETA: 2:22 - loss: 0.3408
22016/36989 [================>.............] - ETA: 2:20 - loss: 0.3408
22272/36989 [=================>............] - ETA: 2:17 - loss: 0.3408
22528/36989 [=================>............] - ETA: 2:15 - loss: 0.3408
22784/36989 [=================>............] - ETA: 2:13 - loss: 0.3404
23040/36989 [=================>............] - ETA: 2:10 - loss: 0.3404
23296/36989 [=================>............] - ETA: 2:08 - loss: 0.3402
23552/36989 [==================>...........] - ETA: 2:05 - loss: 0.3403
23808/36989 [==================>...........] - ETA: 2:03 - loss: 0.3404
24064/36989 [==================>...........] - ETA: 2:01 - loss: 0.3403
24320/36989 [==================>...........] - ETA: 1:58 - loss: 0.3400
24576/36989 [==================>...........] - ETA: 1:56 - loss: 0.3400
24832/36989 [===================>..........] - ETA: 1:53 - loss: 0.3399
25088/36989 [===================>..........] - ETA: 1:51 - loss: 0.3396
25344/36989 [===================>..........] - ETA: 1:49 - loss: 0.3392
25600/36989 [===================>..........] - ETA: 1:46 - loss: 0.3390
25856/36989 [===================>..........] - ETA: 1:44 - loss: 0.3387
26112/36989 [====================>.........] - ETA: 1:41 - loss: 0.3385
26368/36989 [====================>.........] - ETA: 1:39 - loss: 0.3383
26624/36989 [====================>.........] - ETA: 1:37 - loss: 0.3379
26880/36989 [====================>.........] - ETA: 1:34 - loss: 0.3378
27136/36989 [=====================>........] - ETA: 1:32 - loss: 0.3377
27392/36989 [=====================>........] - ETA: 1:29 - loss: 0.3377
27648/36989 [=====================>........] - ETA: 1:27 - loss: 0.3374
27904/36989 [=====================>........] - ETA: 1:25 - loss: 0.3374
28160/36989 [=====================>........] - ETA: 1:22 - loss: 0.3373
28416/36989 [======================>.......] - ETA: 1:20 - loss: 0.3372
28672/36989 [======================>.......] - ETA: 1:17 - loss: 0.3370
28928/36989 [======================>.......] - ETA: 1:15 - loss: 0.3370
29184/36989 [======================>.......] - ETA: 1:13 - loss: 0.3368
29440/36989 [======================>.......] - ETA: 1:10 - loss: 0.3368
29696/36989 [=======================>......] - ETA: 1:08 - loss: 0.3369
29952/36989 [=======================>......] - ETA: 1:05 - loss: 0.3368
30208/36989 [=======================>......] - ETA: 1:03 - loss: 0.3364
30464/36989 [=======================>......] - ETA: 1:01 - loss: 0.3363
30720/36989 [=======================>......] - ETA: 58s - loss: 0.3362 
30976/36989 [========================>.....] - ETA: 56s - loss: 0.3360
31232/36989 [========================>.....] - ETA: 53s - loss: 0.3359
31488/36989 [========================>.....] - ETA: 51s - loss: 0.3359
31744/36989 [========================>.....] - ETA: 49s - loss: 0.3357
32000/36989 [========================>.....] - ETA: 46s - loss: 0.3357
32256/36989 [=========================>....] - ETA: 44s - loss: 0.3356
32512/36989 [=========================>....] - ETA: 41s - loss: 0.3355
32768/36989 [=========================>....] - ETA: 39s - loss: 0.3354
33024/36989 [=========================>....] - ETA: 37s - loss: 0.3352
33280/36989 [=========================>....] - ETA: 34s - loss: 0.3350
33536/36989 [==========================>...] - ETA: 32s - loss: 0.3349
33792/36989 [==========================>...] - ETA: 29s - loss: 0.3348
34048/36989 [==========================>...] - ETA: 27s - loss: 0.3347
34304/36989 [==========================>...] - ETA: 25s - loss: 0.3345
34560/36989 [===========================>..] - ETA: 22s - loss: 0.3345
34816/36989 [===========================>..] - ETA: 20s - loss: 0.3342
35072/36989 [===========================>..] - ETA: 17s - loss: 0.3343
35328/36989 [===========================>..] - ETA: 15s - loss: 0.3344
35584/36989 [===========================>..] - ETA: 13s - loss: 0.3342
35840/36989 [============================>.] - ETA: 10s - loss: 0.3340
36096/36989 [============================>.] - ETA: 8s - loss: 0.3341 
36352/36989 [============================>.] - ETA: 5s - loss: 0.3340
36608/36989 [============================>.] - ETA: 3s - loss: 0.3338
36864/36989 [============================>.] - ETA: 1s - loss: 0.3337
36989/36989 [==============================] - 352s 10ms/step - loss: 0.3336 - val_loss: 0.3146
---
Input python program: 
n = int(input())
nums = list(map(int, input().split()))
sum = 0
for num in nums:
    sum += num
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
num = int(input())
sum = 0
for i in [int(x) for x in input().split()]:
    sum += i
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
map(int,input())
array = list(map(int,input().split()))
sum = 0
for x in array:
    sum += x
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
def arraySum(array):
    total = 0
    for num in array:
        total += num
    return total

n = int(input())
arrayString = input().split()
arrayInt = []

for i in range(0,n):
    arrayInt.append(int(arrayString[i]))

print (arraySum(arrayInt))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
# Inputs
n = int(input())
array = input().split()

# Solution
array = list(map(int, array))
print(sum(array))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
Epoch: 6/10
Train on 36989 samples, validate on 1947 samples
Epoch 1/1

  256/36989 [..............................] - ETA: 5:59 - loss: 0.3174
  512/36989 [..............................] - ETA: 5:52 - loss: 0.3251
  768/36989 [..............................] - ETA: 5:45 - loss: 0.3242
 1024/36989 [..............................] - ETA: 5:40 - loss: 0.3254
 1280/36989 [>.............................] - ETA: 5:36 - loss: 0.3210
 1536/36989 [>.............................] - ETA: 5:37 - loss: 0.3208
 1792/36989 [>.............................] - ETA: 5:33 - loss: 0.3228
 2048/36989 [>.............................] - ETA: 5:33 - loss: 0.3228
 2304/36989 [>.............................] - ETA: 5:30 - loss: 0.3200
 2560/36989 [=>............................] - ETA: 5:26 - loss: 0.3195
 2816/36989 [=>............................] - ETA: 5:23 - loss: 0.3179
 3072/36989 [=>............................] - ETA: 5:20 - loss: 0.3171
 3328/36989 [=>............................] - ETA: 5:17 - loss: 0.3184
 3584/36989 [=>............................] - ETA: 5:15 - loss: 0.3188
 3840/36989 [==>...........................] - ETA: 5:13 - loss: 0.3188
 4096/36989 [==>...........................] - ETA: 5:10 - loss: 0.3200
 4352/36989 [==>...........................] - ETA: 5:09 - loss: 0.3194
 4608/36989 [==>...........................] - ETA: 5:05 - loss: 0.3186
 4864/36989 [==>...........................] - ETA: 5:03 - loss: 0.3184
 5120/36989 [===>..........................] - ETA: 5:01 - loss: 0.3175
 5376/36989 [===>..........................] - ETA: 4:59 - loss: 0.3172
 5632/36989 [===>..........................] - ETA: 4:56 - loss: 0.3173
 5888/36989 [===>..........................] - ETA: 4:54 - loss: 0.3163
 6144/36989 [===>..........................] - ETA: 4:51 - loss: 0.3157
 6400/36989 [====>.........................] - ETA: 4:49 - loss: 0.3165
 6656/36989 [====>.........................] - ETA: 4:47 - loss: 0.3167
 6912/36989 [====>.........................] - ETA: 4:44 - loss: 0.3157
 7168/36989 [====>.........................] - ETA: 4:42 - loss: 0.3157
 7424/36989 [=====>........................] - ETA: 4:40 - loss: 0.3163
 7680/36989 [=====>........................] - ETA: 4:37 - loss: 0.3163
 7936/36989 [=====>........................] - ETA: 4:34 - loss: 0.3163
 8192/36989 [=====>........................] - ETA: 4:33 - loss: 0.3161
 8448/36989 [=====>........................] - ETA: 4:30 - loss: 0.3158
 8704/36989 [======>.......................] - ETA: 4:28 - loss: 0.3157
 8960/36989 [======>.......................] - ETA: 4:25 - loss: 0.3149
 9216/36989 [======>.......................] - ETA: 4:22 - loss: 0.3152
 9472/36989 [======>.......................] - ETA: 4:20 - loss: 0.3154
 9728/36989 [======>.......................] - ETA: 4:18 - loss: 0.3151
 9984/36989 [=======>......................] - ETA: 4:15 - loss: 0.3150
10240/36989 [=======>......................] - ETA: 4:13 - loss: 0.3150
10496/36989 [=======>......................] - ETA: 4:11 - loss: 0.3147
10752/36989 [=======>......................] - ETA: 4:08 - loss: 0.3145
11008/36989 [=======>......................] - ETA: 4:06 - loss: 0.3142
11264/36989 [========>.....................] - ETA: 4:03 - loss: 0.3143
11520/36989 [========>.....................] - ETA: 4:01 - loss: 0.3139
11776/36989 [========>.....................] - ETA: 3:59 - loss: 0.3141
12032/36989 [========>.....................] - ETA: 3:56 - loss: 0.3141
12288/36989 [========>.....................] - ETA: 3:54 - loss: 0.3135
12544/36989 [=========>....................] - ETA: 3:51 - loss: 0.3139
12800/36989 [=========>....................] - ETA: 3:49 - loss: 0.3140
13056/36989 [=========>....................] - ETA: 3:46 - loss: 0.3137
13312/36989 [=========>....................] - ETA: 3:44 - loss: 0.3137
13568/36989 [==========>...................] - ETA: 3:41 - loss: 0.3136
13824/36989 [==========>...................] - ETA: 3:39 - loss: 0.3133
14080/36989 [==========>...................] - ETA: 3:36 - loss: 0.3132
14336/36989 [==========>...................] - ETA: 3:34 - loss: 0.3130
14592/36989 [==========>...................] - ETA: 3:31 - loss: 0.3131
14848/36989 [===========>..................] - ETA: 3:29 - loss: 0.3132
15104/36989 [===========>..................] - ETA: 3:27 - loss: 0.3128
15360/36989 [===========>..................] - ETA: 3:24 - loss: 0.3129
15616/36989 [===========>..................] - ETA: 3:22 - loss: 0.3128
15872/36989 [===========>..................] - ETA: 3:19 - loss: 0.3128
16128/36989 [============>.................] - ETA: 3:17 - loss: 0.3130
16384/36989 [============>.................] - ETA: 3:14 - loss: 0.3131
16640/36989 [============>.................] - ETA: 3:12 - loss: 0.3130
16896/36989 [============>.................] - ETA: 3:09 - loss: 0.3129
17152/36989 [============>.................] - ETA: 3:07 - loss: 0.3133
17408/36989 [=============>................] - ETA: 3:04 - loss: 0.3130
17664/36989 [=============>................] - ETA: 3:02 - loss: 0.3132
17920/36989 [=============>................] - ETA: 3:00 - loss: 0.3134
18176/36989 [=============>................] - ETA: 2:57 - loss: 0.3135
18432/36989 [=============>................] - ETA: 2:55 - loss: 0.3134
18688/36989 [==============>...............] - ETA: 2:52 - loss: 0.3132
18944/36989 [==============>...............] - ETA: 2:50 - loss: 0.3132
19200/36989 [==============>...............] - ETA: 2:48 - loss: 0.3132
19456/36989 [==============>...............] - ETA: 2:45 - loss: 0.3131
19712/36989 [==============>...............] - ETA: 2:43 - loss: 0.3129
19968/36989 [===============>..............] - ETA: 2:40 - loss: 0.3125
20224/36989 [===============>..............] - ETA: 2:38 - loss: 0.3127
20480/36989 [===============>..............] - ETA: 2:36 - loss: 0.3127
20736/36989 [===============>..............] - ETA: 2:33 - loss: 0.3130
20992/36989 [================>.............] - ETA: 2:31 - loss: 0.3129
21248/36989 [================>.............] - ETA: 2:28 - loss: 0.3128
21504/36989 [================>.............] - ETA: 2:26 - loss: 0.3127
21760/36989 [================>.............] - ETA: 2:24 - loss: 0.3128
22016/36989 [================>.............] - ETA: 2:21 - loss: 0.3131
22272/36989 [=================>............] - ETA: 2:19 - loss: 0.3132
22528/36989 [=================>............] - ETA: 2:16 - loss: 0.3133
22784/36989 [=================>............] - ETA: 2:14 - loss: 0.3130
23040/36989 [=================>............] - ETA: 2:12 - loss: 0.3132
23296/36989 [=================>............] - ETA: 2:09 - loss: 0.3131
23552/36989 [==================>...........] - ETA: 2:07 - loss: 0.3129
23808/36989 [==================>...........] - ETA: 2:04 - loss: 0.3127
24064/36989 [==================>...........] - ETA: 2:02 - loss: 0.3126
24320/36989 [==================>...........] - ETA: 2:00 - loss: 0.3124
24576/36989 [==================>...........] - ETA: 1:57 - loss: 0.3123
24832/36989 [===================>..........] - ETA: 1:55 - loss: 0.3121
25088/36989 [===================>..........] - ETA: 1:52 - loss: 0.3120
25344/36989 [===================>..........] - ETA: 1:50 - loss: 0.3121
25600/36989 [===================>..........] - ETA: 1:47 - loss: 0.3118
25856/36989 [===================>..........] - ETA: 1:45 - loss: 0.3118
26112/36989 [====================>.........] - ETA: 1:43 - loss: 0.3118
26368/36989 [====================>.........] - ETA: 1:40 - loss: 0.3117
26624/36989 [====================>.........] - ETA: 1:38 - loss: 0.3117
26880/36989 [====================>.........] - ETA: 1:35 - loss: 0.3116
27136/36989 [=====================>........] - ETA: 1:33 - loss: 0.3117
27392/36989 [=====================>........] - ETA: 1:31 - loss: 0.3117
27648/36989 [=====================>........] - ETA: 1:28 - loss: 0.3116
27904/36989 [=====================>........] - ETA: 1:26 - loss: 0.3116
28160/36989 [=====================>........] - ETA: 1:23 - loss: 0.3113
28416/36989 [======================>.......] - ETA: 1:21 - loss: 0.3113
28672/36989 [======================>.......] - ETA: 1:18 - loss: 0.3112
28928/36989 [======================>.......] - ETA: 1:16 - loss: 0.3112
29184/36989 [======================>.......] - ETA: 1:14 - loss: 0.3115
29440/36989 [======================>.......] - ETA: 1:11 - loss: 0.3114
29696/36989 [=======================>......] - ETA: 1:09 - loss: 0.3114
29952/36989 [=======================>......] - ETA: 1:06 - loss: 0.3111
30208/36989 [=======================>......] - ETA: 1:04 - loss: 0.3109
30464/36989 [=======================>......] - ETA: 1:01 - loss: 0.3107
30720/36989 [=======================>......] - ETA: 59s - loss: 0.3106 
30976/36989 [========================>.....] - ETA: 56s - loss: 0.3104
31232/36989 [========================>.....] - ETA: 54s - loss: 0.3103
31488/36989 [========================>.....] - ETA: 52s - loss: 0.3103
31744/36989 [========================>.....] - ETA: 49s - loss: 0.3104
32000/36989 [========================>.....] - ETA: 47s - loss: 0.3103
32256/36989 [=========================>....] - ETA: 44s - loss: 0.3101
32512/36989 [=========================>....] - ETA: 42s - loss: 0.3099
32768/36989 [=========================>....] - ETA: 40s - loss: 0.3098
33024/36989 [=========================>....] - ETA: 37s - loss: 0.3098
33280/36989 [=========================>....] - ETA: 35s - loss: 0.3096
33536/36989 [==========================>...] - ETA: 32s - loss: 0.3095
33792/36989 [==========================>...] - ETA: 30s - loss: 0.3094
34048/36989 [==========================>...] - ETA: 27s - loss: 0.3093
34304/36989 [==========================>...] - ETA: 25s - loss: 0.3092
34560/36989 [===========================>..] - ETA: 23s - loss: 0.3089
34816/36989 [===========================>..] - ETA: 20s - loss: 0.3088
35072/36989 [===========================>..] - ETA: 18s - loss: 0.3085
35328/36989 [===========================>..] - ETA: 15s - loss: 0.3082
35584/36989 [===========================>..] - ETA: 13s - loss: 0.3082
35840/36989 [============================>.] - ETA: 10s - loss: 0.3082
36096/36989 [============================>.] - ETA: 8s - loss: 0.3081 
36352/36989 [============================>.] - ETA: 6s - loss: 0.3081
36608/36989 [============================>.] - ETA: 3s - loss: 0.3080
36864/36989 [============================>.] - ETA: 1s - loss: 0.3080
36989/36989 [==============================] - 358s 10ms/step - loss: 0.3078 - val_loss: 0.3074
---
Input python program: 
n = int(input())
nums = list(map(int, input().split()))
sum = 0
for num in nums:
    sum += num
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a;
        sum+=a;
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
num = int(input())
sum = 0
for i in [int(x) for x in input().split()]:
    sum += i
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a;
        sum+=a;
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
map(int,input())
array = list(map(int,input().split()))
sum = 0
for x in array:
    sum += x
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a;
        sum+=a;
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
def arraySum(array):
    total = 0
    for num in array:
        total += num
    return total

n = int(input())
arrayString = input().split()
arrayInt = []

for i in range(0,n):
    arrayInt.append(int(arrayString[i]))

print (arraySum(arrayInt))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a;
        sum+=a;
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
# Inputs
n = int(input())
array = input().split()

# Solution
array = list(map(int, array))
print(sum(array))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a;
        sum+=a;
    }
    cout<<sum;
    return 0;
}»
Epoch: 7/10
Train on 36989 samples, validate on 1947 samples
Epoch 1/1

  256/36989 [..............................] - ETA: 6:05 - loss: 0.3071
  512/36989 [..............................] - ETA: 6:01 - loss: 0.2953
  768/36989 [..............................] - ETA: 5:53 - loss: 0.3041
 1024/36989 [..............................] - ETA: 5:46 - loss: 0.2987
 1280/36989 [>.............................] - ETA: 5:41 - loss: 0.2942
 1536/36989 [>.............................] - ETA: 5:40 - loss: 0.2974
 1792/36989 [>.............................] - ETA: 5:36 - loss: 0.2975
 2048/36989 [>.............................] - ETA: 5:32 - loss: 0.2977
 2304/36989 [>.............................] - ETA: 5:29 - loss: 0.2974
 2560/36989 [=>............................] - ETA: 5:26 - loss: 0.2971
 2816/36989 [=>............................] - ETA: 5:24 - loss: 0.2955
 3072/36989 [=>............................] - ETA: 5:20 - loss: 0.2942
 3328/36989 [=>............................] - ETA: 5:18 - loss: 0.2947
 3584/36989 [=>............................] - ETA: 5:14 - loss: 0.2941
 3840/36989 [==>...........................] - ETA: 5:12 - loss: 0.2932
 4096/36989 [==>...........................] - ETA: 5:09 - loss: 0.2928
 4352/36989 [==>...........................] - ETA: 5:07 - loss: 0.2933
 4608/36989 [==>...........................] - ETA: 5:04 - loss: 0.2934
 4864/36989 [==>...........................] - ETA: 5:01 - loss: 0.2935
 5120/36989 [===>..........................] - ETA: 4:58 - loss: 0.2940
 5376/36989 [===>..........................] - ETA: 4:56 - loss: 0.2927
 5632/36989 [===>..........................] - ETA: 4:54 - loss: 0.2928
 5888/36989 [===>..........................] - ETA: 4:51 - loss: 0.2934
 6144/36989 [===>..........................] - ETA: 4:49 - loss: 0.2938
 6400/36989 [====>.........................] - ETA: 4:46 - loss: 0.2936
 6656/36989 [====>.........................] - ETA: 4:44 - loss: 0.2936
 6912/36989 [====>.........................] - ETA: 4:42 - loss: 0.2927
 7168/36989 [====>.........................] - ETA: 4:39 - loss: 0.2931
 7424/36989 [=====>........................] - ETA: 4:37 - loss: 0.2927
 7680/36989 [=====>........................] - ETA: 4:34 - loss: 0.2934
 7936/36989 [=====>........................] - ETA: 4:32 - loss: 0.2935
 8192/36989 [=====>........................] - ETA: 4:29 - loss: 0.2938
 8448/36989 [=====>........................] - ETA: 4:27 - loss: 0.2946
 8704/36989 [======>.......................] - ETA: 4:24 - loss: 0.2953
 8960/36989 [======>.......................] - ETA: 4:22 - loss: 0.2950
 9216/36989 [======>.......................] - ETA: 4:19 - loss: 0.2947
 9472/36989 [======>.......................] - ETA: 4:17 - loss: 0.2953
 9728/36989 [======>.......................] - ETA: 4:14 - loss: 0.2955
 9984/36989 [=======>......................] - ETA: 4:12 - loss: 0.2953
10240/36989 [=======>......................] - ETA: 4:09 - loss: 0.2949
10496/36989 [=======>......................] - ETA: 4:07 - loss: 0.2945
10752/36989 [=======>......................] - ETA: 4:05 - loss: 0.2946
11008/36989 [=======>......................] - ETA: 4:02 - loss: 0.2947
11264/36989 [========>.....................] - ETA: 4:00 - loss: 0.2941
11520/36989 [========>.....................] - ETA: 3:57 - loss: 0.2945
11776/36989 [========>.....................] - ETA: 3:55 - loss: 0.2944
12032/36989 [========>.....................] - ETA: 3:52 - loss: 0.2939
12288/36989 [========>.....................] - ETA: 3:50 - loss: 0.2936
12544/36989 [=========>....................] - ETA: 3:48 - loss: 0.2934
12800/36989 [=========>....................] - ETA: 3:45 - loss: 0.2931
13056/36989 [=========>....................] - ETA: 3:43 - loss: 0.2926
13312/36989 [=========>....................] - ETA: 3:40 - loss: 0.2924
13568/36989 [==========>...................] - ETA: 3:38 - loss: 0.2921
13824/36989 [==========>...................] - ETA: 3:36 - loss: 0.2922
14080/36989 [==========>...................] - ETA: 3:33 - loss: 0.2923
14336/36989 [==========>...................] - ETA: 3:31 - loss: 0.2921
14592/36989 [==========>...................] - ETA: 3:28 - loss: 0.2918
14848/36989 [===========>..................] - ETA: 3:26 - loss: 0.2916
15104/36989 [===========>..................] - ETA: 3:23 - loss: 0.2916
15360/36989 [===========>..................] - ETA: 3:21 - loss: 0.2915
15616/36989 [===========>..................] - ETA: 3:18 - loss: 0.2909
15872/36989 [===========>..................] - ETA: 3:16 - loss: 0.2908
16128/36989 [============>.................] - ETA: 3:14 - loss: 0.2907
16384/36989 [============>.................] - ETA: 3:11 - loss: 0.2906
16640/36989 [============>.................] - ETA: 3:09 - loss: 0.2906
16896/36989 [============>.................] - ETA: 3:06 - loss: 0.2906
17152/36989 [============>.................] - ETA: 3:04 - loss: 0.2905
17408/36989 [=============>................] - ETA: 3:02 - loss: 0.2906
17664/36989 [=============>................] - ETA: 2:59 - loss: 0.2906
17920/36989 [=============>................] - ETA: 2:57 - loss: 0.2907
18176/36989 [=============>................] - ETA: 2:55 - loss: 0.2907
18432/36989 [=============>................] - ETA: 2:52 - loss: 0.2908
18688/36989 [==============>...............] - ETA: 2:50 - loss: 0.2904
18944/36989 [==============>...............] - ETA: 2:47 - loss: 0.2902
19200/36989 [==============>...............] - ETA: 2:45 - loss: 0.2899
19456/36989 [==============>...............] - ETA: 2:43 - loss: 0.2897
19712/36989 [==============>...............] - ETA: 2:40 - loss: 0.2898
19968/36989 [===============>..............] - ETA: 2:38 - loss: 0.2896
20224/36989 [===============>..............] - ETA: 2:35 - loss: 0.2897
20480/36989 [===============>..............] - ETA: 2:33 - loss: 0.2896
20736/36989 [===============>..............] - ETA: 2:31 - loss: 0.2895
20992/36989 [================>.............] - ETA: 2:28 - loss: 0.2893
21248/36989 [================>.............] - ETA: 2:26 - loss: 0.2893
21504/36989 [================>.............] - ETA: 2:24 - loss: 0.2891
21760/36989 [================>.............] - ETA: 2:21 - loss: 0.2889
22016/36989 [================>.............] - ETA: 2:19 - loss: 0.2888
22272/36989 [=================>............] - ETA: 2:17 - loss: 0.2886
22528/36989 [=================>............] - ETA: 2:14 - loss: 0.2886
22784/36989 [=================>............] - ETA: 2:12 - loss: 0.2885
23040/36989 [=================>............] - ETA: 2:09 - loss: 0.2882
23296/36989 [=================>............] - ETA: 2:07 - loss: 0.2879
23552/36989 [==================>...........] - ETA: 2:05 - loss: 0.2878
23808/36989 [==================>...........] - ETA: 2:02 - loss: 0.2877
24064/36989 [==================>...........] - ETA: 2:00 - loss: 0.2876
24320/36989 [==================>...........] - ETA: 1:57 - loss: 0.2876
24576/36989 [==================>...........] - ETA: 1:55 - loss: 0.2877
24832/36989 [===================>..........] - ETA: 1:53 - loss: 0.2876
25088/36989 [===================>..........] - ETA: 1:50 - loss: 0.2875
25344/36989 [===================>..........] - ETA: 1:48 - loss: 0.2876
25600/36989 [===================>..........] - ETA: 1:46 - loss: 0.2875
25856/36989 [===================>..........] - ETA: 1:43 - loss: 0.2876
26112/36989 [====================>.........] - ETA: 1:41 - loss: 0.2874
26368/36989 [====================>.........] - ETA: 1:38 - loss: 0.2872
26624/36989 [====================>.........] - ETA: 1:36 - loss: 0.2872
26880/36989 [====================>.........] - ETA: 1:34 - loss: 0.2872
27136/36989 [=====================>........] - ETA: 1:31 - loss: 0.2873
27392/36989 [=====================>........] - ETA: 1:29 - loss: 0.2871
27648/36989 [=====================>........] - ETA: 1:27 - loss: 0.2871
27904/36989 [=====================>........] - ETA: 1:24 - loss: 0.2869
28160/36989 [=====================>........] - ETA: 1:22 - loss: 0.2868
28416/36989 [======================>.......] - ETA: 1:19 - loss: 0.2866
28672/36989 [======================>.......] - ETA: 1:17 - loss: 0.2864
28928/36989 [======================>.......] - ETA: 1:15 - loss: 0.2864
29184/36989 [======================>.......] - ETA: 1:12 - loss: 0.2864
29440/36989 [======================>.......] - ETA: 1:10 - loss: 0.2862
29696/36989 [=======================>......] - ETA: 1:07 - loss: 0.2862
29952/36989 [=======================>......] - ETA: 1:05 - loss: 0.2861
30208/36989 [=======================>......] - ETA: 1:03 - loss: 0.2860
30464/36989 [=======================>......] - ETA: 1:00 - loss: 0.2858
30720/36989 [=======================>......] - ETA: 58s - loss: 0.2856 
30976/36989 [========================>.....] - ETA: 56s - loss: 0.2857
31232/36989 [========================>.....] - ETA: 53s - loss: 0.2855
31488/36989 [========================>.....] - ETA: 51s - loss: 0.2854
31744/36989 [========================>.....] - ETA: 48s - loss: 0.2852
32000/36989 [========================>.....] - ETA: 46s - loss: 0.2852
32256/36989 [=========================>....] - ETA: 44s - loss: 0.2850
32512/36989 [=========================>....] - ETA: 41s - loss: 0.2848
32768/36989 [=========================>....] - ETA: 39s - loss: 0.2847
33024/36989 [=========================>....] - ETA: 36s - loss: 0.2846
33280/36989 [=========================>....] - ETA: 34s - loss: 0.2846
33536/36989 [==========================>...] - ETA: 32s - loss: 0.2846
33792/36989 [==========================>...] - ETA: 29s - loss: 0.2842
34048/36989 [==========================>...] - ETA: 27s - loss: 0.2843
34304/36989 [==========================>...] - ETA: 24s - loss: 0.2841
34560/36989 [===========================>..] - ETA: 22s - loss: 0.2841
34816/36989 [===========================>..] - ETA: 20s - loss: 0.2841
35072/36989 [===========================>..] - ETA: 17s - loss: 0.2841
35328/36989 [===========================>..] - ETA: 15s - loss: 0.2840
35584/36989 [===========================>..] - ETA: 13s - loss: 0.2839
35840/36989 [============================>.] - ETA: 10s - loss: 0.2837
36096/36989 [============================>.] - ETA: 8s - loss: 0.2836 
36352/36989 [============================>.] - ETA: 5s - loss: 0.2835
36608/36989 [============================>.] - ETA: 3s - loss: 0.2834
36864/36989 [============================>.] - ETA: 1s - loss: 0.2834
36989/36989 [==============================] - 351s 9ms/step - loss: 0.2833 - val_loss: 0.3027
---
Input python program: 
n = int(input())
nums = list(map(int, input().split()))
sum = 0
for num in nums:
    sum += num
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a[i];
        sum=sum+a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
num = int(input())
sum = 0
for i in [int(x) for x in input().split()]:
    sum += i
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a[i];
        sum=sum+a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
map(int,input())
array = list(map(int,input().split()))
sum = 0
for x in array:
    sum += x
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a[i];
        sum=sum+a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
def arraySum(array):
    total = 0
    for num in array:
        total += num
    return total

n = int(input())
arrayString = input().split()
arrayInt = []

for i in range(0,n):
    arrayInt.append(int(arrayString[i]))

print (arraySum(arrayInt))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a[i];
        sum=sum+a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
# Inputs
n = int(input())
array = input().split()

# Solution
array = list(map(int, array))
print(sum(array))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a[i];
        sum=sum+a[i];
    }
    cout<<sum;
    return 0;
}»
Epoch: 8/10
Train on 36989 samples, validate on 1947 samples
Epoch 1/1

  256/36989 [..............................] - ETA: 5:49 - loss: 0.2960
  512/36989 [..............................] - ETA: 5:40 - loss: 0.2865
  768/36989 [..............................] - ETA: 5:35 - loss: 0.2832
 1024/36989 [..............................] - ETA: 5:30 - loss: 0.2792
 1280/36989 [>.............................] - ETA: 5:28 - loss: 0.2787
 1536/36989 [>.............................] - ETA: 5:25 - loss: 0.2787
 1792/36989 [>.............................] - ETA: 5:23 - loss: 0.2765
 2048/36989 [>.............................] - ETA: 5:20 - loss: 0.2749
 2304/36989 [>.............................] - ETA: 5:19 - loss: 0.2757
 2560/36989 [=>............................] - ETA: 5:17 - loss: 0.2750
 2816/36989 [=>............................] - ETA: 5:15 - loss: 0.2761
 3072/36989 [=>............................] - ETA: 5:13 - loss: 0.2773
 3328/36989 [=>............................] - ETA: 5:11 - loss: 0.2756
 3584/36989 [=>............................] - ETA: 5:09 - loss: 0.2752
 3840/36989 [==>...........................] - ETA: 5:07 - loss: 0.2754
 4096/36989 [==>...........................] - ETA: 5:04 - loss: 0.2759
 4352/36989 [==>...........................] - ETA: 5:03 - loss: 0.2762
 4608/36989 [==>...........................] - ETA: 5:00 - loss: 0.2768
 4864/36989 [==>...........................] - ETA: 4:58 - loss: 0.2772
 5120/36989 [===>..........................] - ETA: 4:55 - loss: 0.2769
 5376/36989 [===>..........................] - ETA: 4:53 - loss: 0.2760
 5632/36989 [===>..........................] - ETA: 4:50 - loss: 0.2755
 5888/36989 [===>..........................] - ETA: 4:48 - loss: 0.2756
 6144/36989 [===>..........................] - ETA: 4:45 - loss: 0.2746
 6400/36989 [====>.........................] - ETA: 4:43 - loss: 0.2735
 6656/36989 [====>.........................] - ETA: 4:41 - loss: 0.2739
 6912/36989 [====>.........................] - ETA: 4:38 - loss: 0.2739
 7168/36989 [====>.........................] - ETA: 4:37 - loss: 0.2737
 7424/36989 [=====>........................] - ETA: 4:34 - loss: 0.2725
 7680/36989 [=====>........................] - ETA: 4:32 - loss: 0.2718
 7936/36989 [=====>........................] - ETA: 4:30 - loss: 0.2717
 8192/36989 [=====>........................] - ETA: 4:28 - loss: 0.2711
 8448/36989 [=====>........................] - ETA: 4:25 - loss: 0.2709
 8704/36989 [======>.......................] - ETA: 4:23 - loss: 0.2707
 8960/36989 [======>.......................] - ETA: 4:21 - loss: 0.2708
 9216/36989 [======>.......................] - ETA: 4:19 - loss: 0.2706
 9472/36989 [======>.......................] - ETA: 4:16 - loss: 0.2704
 9728/36989 [======>.......................] - ETA: 4:14 - loss: 0.2708
 9984/36989 [=======>......................] - ETA: 4:12 - loss: 0.2711
10240/36989 [=======>......................] - ETA: 4:09 - loss: 0.2712
10496/36989 [=======>......................] - ETA: 4:07 - loss: 0.2711
10752/36989 [=======>......................] - ETA: 4:04 - loss: 0.2711
11008/36989 [=======>......................] - ETA: 4:02 - loss: 0.2707
11264/36989 [========>.....................] - ETA: 4:00 - loss: 0.2705
11520/36989 [========>.....................] - ETA: 3:57 - loss: 0.2703
11776/36989 [========>.....................] - ETA: 3:55 - loss: 0.2700
12032/36989 [========>.....................] - ETA: 3:52 - loss: 0.2696
12288/36989 [========>.....................] - ETA: 3:50 - loss: 0.2694
12544/36989 [=========>....................] - ETA: 3:47 - loss: 0.2690
12800/36989 [=========>....................] - ETA: 3:45 - loss: 0.2686
13056/36989 [=========>....................] - ETA: 3:43 - loss: 0.2684
13312/36989 [=========>....................] - ETA: 3:40 - loss: 0.2680
13568/36989 [==========>...................] - ETA: 3:38 - loss: 0.2678
13824/36989 [==========>...................] - ETA: 3:35 - loss: 0.2679
14080/36989 [==========>...................] - ETA: 3:33 - loss: 0.2677
14336/36989 [==========>...................] - ETA: 3:31 - loss: 0.2676
14592/36989 [==========>...................] - ETA: 3:28 - loss: 0.2675
14848/36989 [===========>..................] - ETA: 3:26 - loss: 0.2671
15104/36989 [===========>..................] - ETA: 3:24 - loss: 0.2667
15360/36989 [===========>..................] - ETA: 3:21 - loss: 0.2667
15616/36989 [===========>..................] - ETA: 3:19 - loss: 0.2668
15872/36989 [===========>..................] - ETA: 3:16 - loss: 0.2665
16128/36989 [============>.................] - ETA: 3:14 - loss: 0.2662
16384/36989 [============>.................] - ETA: 3:12 - loss: 0.2662
16640/36989 [============>.................] - ETA: 3:09 - loss: 0.2663
16896/36989 [============>.................] - ETA: 3:07 - loss: 0.2659
17152/36989 [============>.................] - ETA: 3:04 - loss: 0.2656
17408/36989 [=============>................] - ETA: 3:02 - loss: 0.2653
17664/36989 [=============>................] - ETA: 3:00 - loss: 0.2651
17920/36989 [=============>................] - ETA: 2:57 - loss: 0.2651
18176/36989 [=============>................] - ETA: 2:55 - loss: 0.2652
18432/36989 [=============>................] - ETA: 2:52 - loss: 0.2650
18688/36989 [==============>...............] - ETA: 2:50 - loss: 0.2650
18944/36989 [==============>...............] - ETA: 2:47 - loss: 0.2649
19200/36989 [==============>...............] - ETA: 2:45 - loss: 0.2649
19456/36989 [==============>...............] - ETA: 2:43 - loss: 0.2650
19712/36989 [==============>...............] - ETA: 2:40 - loss: 0.2652
19968/36989 [===============>..............] - ETA: 2:38 - loss: 0.2653
20224/36989 [===============>..............] - ETA: 2:36 - loss: 0.2652
20480/36989 [===============>..............] - ETA: 2:33 - loss: 0.2650
20736/36989 [===============>..............] - ETA: 2:31 - loss: 0.2650
20992/36989 [================>.............] - ETA: 2:28 - loss: 0.2650
21248/36989 [================>.............] - ETA: 2:26 - loss: 0.2647
21504/36989 [================>.............] - ETA: 2:24 - loss: 0.2646
21760/36989 [================>.............] - ETA: 2:21 - loss: 0.2645
22016/36989 [================>.............] - ETA: 2:19 - loss: 0.2643
22272/36989 [=================>............] - ETA: 2:16 - loss: 0.2641
22528/36989 [=================>............] - ETA: 2:14 - loss: 0.2640
22784/36989 [=================>............] - ETA: 2:12 - loss: 0.2638
23040/36989 [=================>............] - ETA: 2:09 - loss: 0.2636
23296/36989 [=================>............] - ETA: 2:07 - loss: 0.2635
23552/36989 [==================>...........] - ETA: 2:05 - loss: 0.2634
23808/36989 [==================>...........] - ETA: 2:02 - loss: 0.2633
24064/36989 [==================>...........] - ETA: 2:00 - loss: 0.2634
24320/36989 [==================>...........] - ETA: 1:58 - loss: 0.2634
24576/36989 [==================>...........] - ETA: 1:55 - loss: 0.2635
24832/36989 [===================>..........] - ETA: 1:53 - loss: 0.2634
25088/36989 [===================>..........] - ETA: 1:51 - loss: 0.2632
25344/36989 [===================>..........] - ETA: 1:48 - loss: 0.2631
25600/36989 [===================>..........] - ETA: 1:46 - loss: 0.2633
25856/36989 [===================>..........] - ETA: 1:43 - loss: 0.2632
26112/36989 [====================>.........] - ETA: 1:41 - loss: 0.2629
26368/36989 [====================>.........] - ETA: 1:39 - loss: 0.2629
26624/36989 [====================>.........] - ETA: 1:36 - loss: 0.2627
26880/36989 [====================>.........] - ETA: 1:34 - loss: 0.2627
27136/36989 [=====================>........] - ETA: 1:31 - loss: 0.2626
27392/36989 [=====================>........] - ETA: 1:29 - loss: 0.2626
27648/36989 [=====================>........] - ETA: 1:27 - loss: 0.2625
27904/36989 [=====================>........] - ETA: 1:24 - loss: 0.2624
28160/36989 [=====================>........] - ETA: 1:22 - loss: 0.2623
28416/36989 [======================>.......] - ETA: 1:20 - loss: 0.2622
28672/36989 [======================>.......] - ETA: 1:17 - loss: 0.2622
28928/36989 [======================>.......] - ETA: 1:15 - loss: 0.2621
29184/36989 [======================>.......] - ETA: 1:12 - loss: 0.2619
29440/36989 [======================>.......] - ETA: 1:10 - loss: 0.2619
29696/36989 [=======================>......] - ETA: 1:08 - loss: 0.2617
29952/36989 [=======================>......] - ETA: 1:05 - loss: 0.2616
30208/36989 [=======================>......] - ETA: 1:03 - loss: 0.2615
30464/36989 [=======================>......] - ETA: 1:00 - loss: 0.2615
30720/36989 [=======================>......] - ETA: 58s - loss: 0.2615 
30976/36989 [========================>.....] - ETA: 56s - loss: 0.2614
31232/36989 [========================>.....] - ETA: 53s - loss: 0.2613
31488/36989 [========================>.....] - ETA: 51s - loss: 0.2612
31744/36989 [========================>.....] - ETA: 49s - loss: 0.2612
32000/36989 [========================>.....] - ETA: 46s - loss: 0.2610
32256/36989 [=========================>....] - ETA: 44s - loss: 0.2609
32512/36989 [=========================>....] - ETA: 41s - loss: 0.2608
32768/36989 [=========================>....] - ETA: 39s - loss: 0.2608
33024/36989 [=========================>....] - ETA: 37s - loss: 0.2607
33280/36989 [=========================>....] - ETA: 34s - loss: 0.2607
33536/36989 [==========================>...] - ETA: 32s - loss: 0.2607
33792/36989 [==========================>...] - ETA: 29s - loss: 0.2605
34048/36989 [==========================>...] - ETA: 27s - loss: 0.2603
34304/36989 [==========================>...] - ETA: 25s - loss: 0.2603
34560/36989 [===========================>..] - ETA: 22s - loss: 0.2602
34816/36989 [===========================>..] - ETA: 20s - loss: 0.2601
35072/36989 [===========================>..] - ETA: 17s - loss: 0.2599
35328/36989 [===========================>..] - ETA: 15s - loss: 0.2599
35584/36989 [===========================>..] - ETA: 13s - loss: 0.2598
35840/36989 [============================>.] - ETA: 10s - loss: 0.2596
36096/36989 [============================>.] - ETA: 8s - loss: 0.2596 
36352/36989 [============================>.] - ETA: 5s - loss: 0.2594
36608/36989 [============================>.] - ETA: 3s - loss: 0.2594
36864/36989 [============================>.] - ETA: 1s - loss: 0.2594
36989/36989 [==============================] - 354s 10ms/step - loss: 0.2594 - val_loss: 0.3022
---
Input python program: 
n = int(input())
nums = list(map(int, input().split()))
sum = 0
for num in nums:
    sum += num
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    long long int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>arr[i];
        sum=sum+arr[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
num = int(input())
sum = 0
for i in [int(x) for x in input().split()]:
    sum += i
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    long long int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>arr[i];
        sum=sum+arr[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
map(int,input())
array = list(map(int,input().split()))
sum = 0
for x in array:
    sum += x
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    long long int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>arr[i];
        sum=sum+arr[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
def arraySum(array):
    total = 0
    for num in array:
        total += num
    return total

n = int(input())
arrayString = input().split()
arrayInt = []

for i in range(0,n):
    arrayInt.append(int(arrayString[i]))

print (arraySum(arrayInt))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    long long int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>arr[i];
        sum=sum+arr[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
# Inputs
n = int(input())
array = input().split()

# Solution
array = list(map(int, array))
print(sum(array))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    long long int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>arr[i];
        sum=sum+arr[i];
    }
    cout<<sum;
    return 0;
}»
Epoch: 9/10
Train on 36989 samples, validate on 1947 samples
Epoch 1/1

  256/36989 [..............................] - ETA: 5:41 - loss: 0.2649
  512/36989 [..............................] - ETA: 5:37 - loss: 0.2658
  768/36989 [..............................] - ETA: 5:34 - loss: 0.2566
 1024/36989 [..............................] - ETA: 5:29 - loss: 0.2549
 1280/36989 [>.............................] - ETA: 5:26 - loss: 0.2544
 1536/36989 [>.............................] - ETA: 5:25 - loss: 0.2543
 1792/36989 [>.............................] - ETA: 5:25 - loss: 0.2548
 2048/36989 [>.............................] - ETA: 5:24 - loss: 0.2524
 2304/36989 [>.............................] - ETA: 5:22 - loss: 0.2518
 2560/36989 [=>............................] - ETA: 5:20 - loss: 0.2505
 2816/36989 [=>............................] - ETA: 5:19 - loss: 0.2511
 3072/36989 [=>............................] - ETA: 5:19 - loss: 0.2497
 3328/36989 [=>............................] - ETA: 5:17 - loss: 0.2511
 3584/36989 [=>............................] - ETA: 5:15 - loss: 0.2513
 3840/36989 [==>...........................] - ETA: 5:13 - loss: 0.2509
 4096/36989 [==>...........................] - ETA: 5:11 - loss: 0.2502
 4352/36989 [==>...........................] - ETA: 5:09 - loss: 0.2493
 4608/36989 [==>...........................] - ETA: 5:07 - loss: 0.2478
 4864/36989 [==>...........................] - ETA: 5:05 - loss: 0.2481
 5120/36989 [===>..........................] - ETA: 5:03 - loss: 0.2484
 5376/36989 [===>..........................] - ETA: 5:00 - loss: 0.2486
 5632/36989 [===>..........................] - ETA: 4:58 - loss: 0.2482
 5888/36989 [===>..........................] - ETA: 4:57 - loss: 0.2475
 6144/36989 [===>..........................] - ETA: 4:54 - loss: 0.2479
 6400/36989 [====>.........................] - ETA: 4:52 - loss: 0.2476
 6656/36989 [====>.........................] - ETA: 4:49 - loss: 0.2470
 6912/36989 [====>.........................] - ETA: 4:47 - loss: 0.2477
 7168/36989 [====>.........................] - ETA: 4:44 - loss: 0.2484
 7424/36989 [=====>........................] - ETA: 4:42 - loss: 0.2486
 7680/36989 [=====>........................] - ETA: 4:39 - loss: 0.2484
 7936/36989 [=====>........................] - ETA: 4:37 - loss: 0.2483
 8192/36989 [=====>........................] - ETA: 4:35 - loss: 0.2483
 8448/36989 [=====>........................] - ETA: 4:32 - loss: 0.2486
 8704/36989 [======>.......................] - ETA: 4:30 - loss: 0.2484
 8960/36989 [======>.......................] - ETA: 4:27 - loss: 0.2481
 9216/36989 [======>.......................] - ETA: 4:25 - loss: 0.2481
 9472/36989 [======>.......................] - ETA: 4:22 - loss: 0.2482
 9728/36989 [======>.......................] - ETA: 4:20 - loss: 0.2486
 9984/36989 [=======>......................] - ETA: 4:18 - loss: 0.2485
10240/36989 [=======>......................] - ETA: 4:15 - loss: 0.2483
10496/36989 [=======>......................] - ETA: 4:13 - loss: 0.2481
10752/36989 [=======>......................] - ETA: 4:11 - loss: 0.2479
11008/36989 [=======>......................] - ETA: 4:08 - loss: 0.2480
11264/36989 [========>.....................] - ETA: 4:06 - loss: 0.2475
11520/36989 [========>.....................] - ETA: 4:04 - loss: 0.2478
11776/36989 [========>.....................] - ETA: 4:01 - loss: 0.2478
12032/36989 [========>.....................] - ETA: 3:59 - loss: 0.2478
12288/36989 [========>.....................] - ETA: 3:56 - loss: 0.2476
12544/36989 [=========>....................] - ETA: 3:54 - loss: 0.2477
12800/36989 [=========>....................] - ETA: 3:51 - loss: 0.2475
13056/36989 [=========>....................] - ETA: 3:49 - loss: 0.2475
13312/36989 [=========>....................] - ETA: 3:46 - loss: 0.2477
13568/36989 [==========>...................] - ETA: 3:44 - loss: 0.2473
13824/36989 [==========>...................] - ETA: 3:41 - loss: 0.2471
14080/36989 [==========>...................] - ETA: 3:39 - loss: 0.2472
14336/36989 [==========>...................] - ETA: 3:36 - loss: 0.2469
14592/36989 [==========>...................] - ETA: 3:34 - loss: 0.2466
14848/36989 [===========>..................] - ETA: 3:32 - loss: 0.2465
15104/36989 [===========>..................] - ETA: 3:29 - loss: 0.2466
15360/36989 [===========>..................] - ETA: 3:26 - loss: 0.2465
15616/36989 [===========>..................] - ETA: 3:24 - loss: 0.2463
15872/36989 [===========>..................] - ETA: 3:21 - loss: 0.2462
16128/36989 [============>.................] - ETA: 3:19 - loss: 0.2463
16384/36989 [============>.................] - ETA: 3:16 - loss: 0.2460
16640/36989 [============>.................] - ETA: 3:14 - loss: 0.2458
16896/36989 [============>.................] - ETA: 3:12 - loss: 0.2458
17152/36989 [============>.................] - ETA: 3:09 - loss: 0.2457
17408/36989 [=============>................] - ETA: 3:06 - loss: 0.2456
17664/36989 [=============>................] - ETA: 3:04 - loss: 0.2453
17920/36989 [=============>................] - ETA: 3:01 - loss: 0.2453
18176/36989 [=============>................] - ETA: 2:59 - loss: 0.2452
18432/36989 [=============>................] - ETA: 2:57 - loss: 0.2450
18688/36989 [==============>...............] - ETA: 2:54 - loss: 0.2448
18944/36989 [==============>...............] - ETA: 2:52 - loss: 0.2446
19200/36989 [==============>...............] - ETA: 2:49 - loss: 0.2444
19456/36989 [==============>...............] - ETA: 2:47 - loss: 0.2440
19712/36989 [==============>...............] - ETA: 2:44 - loss: 0.2437
19968/36989 [===============>..............] - ETA: 2:42 - loss: 0.2438
20224/36989 [===============>..............] - ETA: 2:40 - loss: 0.2437
20480/36989 [===============>..............] - ETA: 2:37 - loss: 0.2437
20736/36989 [===============>..............] - ETA: 2:35 - loss: 0.2437
20992/36989 [================>.............] - ETA: 2:32 - loss: 0.2436
21248/36989 [================>.............] - ETA: 2:30 - loss: 0.2433
21504/36989 [================>.............] - ETA: 2:27 - loss: 0.2434
21760/36989 [================>.............] - ETA: 2:25 - loss: 0.2435
22016/36989 [================>.............] - ETA: 2:23 - loss: 0.2436
22272/36989 [=================>............] - ETA: 2:20 - loss: 0.2436
22528/36989 [=================>............] - ETA: 2:18 - loss: 0.2434
22784/36989 [=================>............] - ETA: 2:15 - loss: 0.2433
23040/36989 [=================>............] - ETA: 2:13 - loss: 0.2430
23296/36989 [=================>............] - ETA: 2:10 - loss: 0.2429
23552/36989 [==================>...........] - ETA: 2:08 - loss: 0.2429
23808/36989 [==================>...........] - ETA: 2:05 - loss: 0.2427
24064/36989 [==================>...........] - ETA: 2:03 - loss: 0.2425
24320/36989 [==================>...........] - ETA: 2:00 - loss: 0.2425
24576/36989 [==================>...........] - ETA: 1:58 - loss: 0.2424
24832/36989 [===================>..........] - ETA: 1:56 - loss: 0.2423
25088/36989 [===================>..........] - ETA: 1:53 - loss: 0.2422
25344/36989 [===================>..........] - ETA: 1:51 - loss: 0.2422
25600/36989 [===================>..........] - ETA: 1:48 - loss: 0.2423
25856/36989 [===================>..........] - ETA: 1:46 - loss: 0.2421
26112/36989 [====================>.........] - ETA: 1:44 - loss: 0.2420
26368/36989 [====================>.........] - ETA: 1:41 - loss: 0.2419
26624/36989 [====================>.........] - ETA: 1:39 - loss: 0.2418
26880/36989 [====================>.........] - ETA: 1:36 - loss: 0.2419
27136/36989 [=====================>........] - ETA: 1:34 - loss: 0.2417
27392/36989 [=====================>........] - ETA: 1:31 - loss: 0.2417
27648/36989 [=====================>........] - ETA: 1:29 - loss: 0.2417
27904/36989 [=====================>........] - ETA: 1:26 - loss: 0.2416
28160/36989 [=====================>........] - ETA: 1:24 - loss: 0.2417
28416/36989 [======================>.......] - ETA: 1:22 - loss: 0.2415
28672/36989 [======================>.......] - ETA: 1:19 - loss: 0.2413
28928/36989 [======================>.......] - ETA: 1:17 - loss: 0.2411
29184/36989 [======================>.......] - ETA: 1:14 - loss: 0.2410
29440/36989 [======================>.......] - ETA: 1:12 - loss: 0.2410
29696/36989 [=======================>......] - ETA: 1:09 - loss: 0.2410
29952/36989 [=======================>......] - ETA: 1:07 - loss: 0.2410
30208/36989 [=======================>......] - ETA: 1:04 - loss: 0.2408
30464/36989 [=======================>......] - ETA: 1:02 - loss: 0.2406
30720/36989 [=======================>......] - ETA: 1:00 - loss: 0.2406
30976/36989 [========================>.....] - ETA: 57s - loss: 0.2406 
31232/36989 [========================>.....] - ETA: 55s - loss: 0.2405
31488/36989 [========================>.....] - ETA: 52s - loss: 0.2404
31744/36989 [========================>.....] - ETA: 50s - loss: 0.2405
32000/36989 [========================>.....] - ETA: 47s - loss: 0.2404
32256/36989 [=========================>....] - ETA: 45s - loss: 0.2405
32512/36989 [=========================>....] - ETA: 42s - loss: 0.2403
32768/36989 [=========================>....] - ETA: 40s - loss: 0.2403
33024/36989 [=========================>....] - ETA: 38s - loss: 0.2403
33280/36989 [=========================>....] - ETA: 35s - loss: 0.2402
33536/36989 [==========================>...] - ETA: 33s - loss: 0.2403
33792/36989 [==========================>...] - ETA: 30s - loss: 0.2403
34048/36989 [==========================>...] - ETA: 28s - loss: 0.2402
34304/36989 [==========================>...] - ETA: 25s - loss: 0.2402
34560/36989 [===========================>..] - ETA: 23s - loss: 0.2401
34816/36989 [===========================>..] - ETA: 20s - loss: 0.2401
35072/36989 [===========================>..] - ETA: 18s - loss: 0.2401
35328/36989 [===========================>..] - ETA: 15s - loss: 0.2399
35584/36989 [===========================>..] - ETA: 13s - loss: 0.2399
35840/36989 [============================>.] - ETA: 11s - loss: 0.2398
36096/36989 [============================>.] - ETA: 8s - loss: 0.2396 
36352/36989 [============================>.] - ETA: 6s - loss: 0.2396
36608/36989 [============================>.] - ETA: 3s - loss: 0.2395
36864/36989 [============================>.] - ETA: 1s - loss: 0.2395
36989/36989 [==============================] - 362s 10ms/step - loss: 0.2394 - val_loss: 0.2936
---
Input python program: 
n = int(input())
nums = list(map(int, input().split()))
sum = 0
for num in nums:
    sum += num
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n;
    cin>>n;
    int a[n];
    int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum=sum+a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
num = int(input())
sum = 0
for i in [int(x) for x in input().split()]:
    sum += i
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n;
    cin>>n;
    int a[n];
    int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum=sum+a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
map(int,input())
array = list(map(int,input().split()))
sum = 0
for x in array:
    sum += x
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n;
    cin>>n;
    int a[n];
    int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum=sum+a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
def arraySum(array):
    total = 0
    for num in array:
        total += num
    return total

n = int(input())
arrayString = input().split()
arrayInt = []

for i in range(0,n):
    arrayInt.append(int(arrayString[i]))

print (arraySum(arrayInt))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n;
    cin>>n;
    int a[n];
    int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum=sum+a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
# Inputs
n = int(input())
array = input().split()

# Solution
array = list(map(int, array))
print(sum(array))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n;
    cin>>n;
    int a[n];
    int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum=sum+a[i];
    }
    cout<<sum;
    return 0;
}»
38936
« »
Number of samples: 38936
Number of unique input tokens: 128
Number of unique output tokens: 130
Max sequence length for inputs: 498
Max sequence length for outputs: 500
Epoch: 0/10
Train on 36989 samples, validate on 1947 samples
Epoch 1/1

  256/36989 [..............................] - ETA: 15:01 - loss: 0.2174
  512/36989 [..............................] - ETA: 10:22 - loss: 0.5307
  768/36989 [..............................] - ETA: 8:47 - loss: 0.5494 
 1024/36989 [..............................] - ETA: 7:54 - loss: 0.5307
 1280/36989 [>.............................] - ETA: 7:25 - loss: 0.5109
 1536/36989 [>.............................] - ETA: 7:06 - loss: 0.4934
 1792/36989 [>.............................] - ETA: 6:53 - loss: 0.4810
 2048/36989 [>.............................] - ETA: 6:42 - loss: 0.4717
 2304/36989 [>.............................] - ETA: 6:32 - loss: 0.4592
 2560/36989 [=>............................] - ETA: 6:21 - loss: 0.4511
 2816/36989 [=>............................] - ETA: 6:13 - loss: 0.4437
 3072/36989 [=>............................] - ETA: 6:05 - loss: 0.4337
 3328/36989 [=>............................] - ETA: 5:59 - loss: 0.4274
 3584/36989 [=>............................] - ETA: 5:53 - loss: 0.4210
 3840/36989 [==>...........................] - ETA: 5:47 - loss: 0.4149
 4096/36989 [==>...........................] - ETA: 5:43 - loss: 0.4092
 4352/36989 [==>...........................] - ETA: 5:39 - loss: 0.4047
 4608/36989 [==>...........................] - ETA: 5:35 - loss: 0.3992
 4864/36989 [==>...........................] - ETA: 5:31 - loss: 0.3948
 5120/36989 [===>..........................] - ETA: 5:27 - loss: 0.3909
 5376/36989 [===>..........................] - ETA: 5:23 - loss: 0.3877
 5632/36989 [===>..........................] - ETA: 5:19 - loss: 0.3846
 5888/36989 [===>..........................] - ETA: 5:15 - loss: 0.3813
 6144/36989 [===>..........................] - ETA: 5:12 - loss: 0.3777
 6400/36989 [====>.........................] - ETA: 5:09 - loss: 0.3743
 6656/36989 [====>.........................] - ETA: 5:05 - loss: 0.3711
 6912/36989 [====>.........................] - ETA: 5:02 - loss: 0.3686
 7168/36989 [====>.........................] - ETA: 4:59 - loss: 0.3652
 7424/36989 [=====>........................] - ETA: 4:56 - loss: 0.3629
 7680/36989 [=====>........................] - ETA: 4:53 - loss: 0.3601
 7936/36989 [=====>........................] - ETA: 4:49 - loss: 0.3574
 8192/36989 [=====>........................] - ETA: 4:46 - loss: 0.3552
 8448/36989 [=====>........................] - ETA: 4:43 - loss: 0.3531
 8704/36989 [======>.......................] - ETA: 4:40 - loss: 0.3509
 8960/36989 [======>.......................] - ETA: 4:37 - loss: 0.3493
 9216/36989 [======>.......................] - ETA: 4:34 - loss: 0.3468
 9472/36989 [======>.......................] - ETA: 4:31 - loss: 0.3446
 9728/36989 [======>.......................] - ETA: 4:28 - loss: 0.3423
 9984/36989 [=======>......................] - ETA: 4:25 - loss: 0.3403
10240/36989 [=======>......................] - ETA: 4:23 - loss: 0.3383
10496/36989 [=======>......................] - ETA: 4:20 - loss: 0.3363
10752/36989 [=======>......................] - ETA: 4:17 - loss: 0.3346
11008/36989 [=======>......................] - ETA: 4:14 - loss: 0.3333
11264/36989 [========>.....................] - ETA: 4:11 - loss: 0.3314
11520/36989 [========>.....................] - ETA: 4:08 - loss: 0.3298
11776/36989 [========>.....................] - ETA: 4:06 - loss: 0.3285
12032/36989 [========>.....................] - ETA: 4:03 - loss: 0.3267
12288/36989 [========>.....................] - ETA: 4:00 - loss: 0.3251
12544/36989 [=========>....................] - ETA: 3:57 - loss: 0.3236
12800/36989 [=========>....................] - ETA: 3:55 - loss: 0.3225
13056/36989 [=========>....................] - ETA: 3:52 - loss: 0.3211
13312/36989 [=========>....................] - ETA: 3:49 - loss: 0.3198
13568/36989 [==========>...................] - ETA: 3:46 - loss: 0.3185
13824/36989 [==========>...................] - ETA: 3:43 - loss: 0.3172
14080/36989 [==========>...................] - ETA: 3:41 - loss: 0.3161
14336/36989 [==========>...................] - ETA: 3:38 - loss: 0.3149
14592/36989 [==========>...................] - ETA: 3:35 - loss: 0.3139
14848/36989 [===========>..................] - ETA: 3:32 - loss: 0.3129
15104/36989 [===========>..................] - ETA: 3:30 - loss: 0.3115
15360/36989 [===========>..................] - ETA: 3:27 - loss: 0.3100
15616/36989 [===========>..................] - ETA: 3:25 - loss: 0.3091
15872/36989 [===========>..................] - ETA: 3:22 - loss: 0.3081
16128/36989 [============>.................] - ETA: 3:19 - loss: 0.3073
16384/36989 [============>.................] - ETA: 3:17 - loss: 0.3065
16640/36989 [============>.................] - ETA: 3:14 - loss: 0.3053
16896/36989 [============>.................] - ETA: 3:12 - loss: 0.3042
17152/36989 [============>.................] - ETA: 3:09 - loss: 0.3033
17408/36989 [=============>................] - ETA: 3:06 - loss: 0.3026
17664/36989 [=============>................] - ETA: 3:04 - loss: 0.3016
17920/36989 [=============>................] - ETA: 3:01 - loss: 0.3008
18176/36989 [=============>................] - ETA: 2:59 - loss: 0.3000
18432/36989 [=============>................] - ETA: 2:56 - loss: 0.2990
18688/36989 [==============>...............] - ETA: 2:54 - loss: 0.2981
18944/36989 [==============>...............] - ETA: 2:51 - loss: 0.2971
19200/36989 [==============>...............] - ETA: 2:48 - loss: 0.2962
19456/36989 [==============>...............] - ETA: 2:46 - loss: 0.2952
19712/36989 [==============>...............] - ETA: 2:43 - loss: 0.2944
19968/36989 [===============>..............] - ETA: 2:41 - loss: 0.2936
20224/36989 [===============>..............] - ETA: 2:38 - loss: 0.2930
20480/36989 [===============>..............] - ETA: 2:36 - loss: 0.2924
20736/36989 [===============>..............] - ETA: 2:33 - loss: 0.2916
20992/36989 [================>.............] - ETA: 2:31 - loss: 0.2909
21248/36989 [================>.............] - ETA: 2:28 - loss: 0.2902
21504/36989 [================>.............] - ETA: 2:26 - loss: 0.2896
21760/36989 [================>.............] - ETA: 2:23 - loss: 0.2890
22016/36989 [================>.............] - ETA: 2:21 - loss: 0.2883
22272/36989 [=================>............] - ETA: 2:18 - loss: 0.2877
22528/36989 [=================>............] - ETA: 2:16 - loss: 0.2871
22784/36989 [=================>............] - ETA: 2:13 - loss: 0.2864
23040/36989 [=================>............] - ETA: 2:11 - loss: 0.2859
23296/36989 [=================>............] - ETA: 2:08 - loss: 0.2853
23552/36989 [==================>...........] - ETA: 2:06 - loss: 0.2846
23808/36989 [==================>...........] - ETA: 2:04 - loss: 0.2838
24064/36989 [==================>...........] - ETA: 2:01 - loss: 0.2833
24320/36989 [==================>...........] - ETA: 1:59 - loss: 0.2826
24576/36989 [==================>...........] - ETA: 1:56 - loss: 0.2820
24832/36989 [===================>..........] - ETA: 1:54 - loss: 0.2814
25088/36989 [===================>..........] - ETA: 1:51 - loss: 0.2807
25344/36989 [===================>..........] - ETA: 1:49 - loss: 0.2802
25600/36989 [===================>..........] - ETA: 1:46 - loss: 0.2796
25856/36989 [===================>..........] - ETA: 1:44 - loss: 0.2791
26112/36989 [====================>.........] - ETA: 1:41 - loss: 0.2787
26368/36989 [====================>.........] - ETA: 1:39 - loss: 0.2782
26624/36989 [====================>.........] - ETA: 1:37 - loss: 0.2777
26880/36989 [====================>.........] - ETA: 1:34 - loss: 0.2772
27136/36989 [=====================>........] - ETA: 1:32 - loss: 0.2767
27392/36989 [=====================>........] - ETA: 1:29 - loss: 0.2761
27648/36989 [=====================>........] - ETA: 1:27 - loss: 0.2756
27904/36989 [=====================>........] - ETA: 1:25 - loss: 0.2752
28160/36989 [=====================>........] - ETA: 1:22 - loss: 0.2748
28416/36989 [======================>.......] - ETA: 1:20 - loss: 0.2744
28672/36989 [======================>.......] - ETA: 1:17 - loss: 0.2739
28928/36989 [======================>.......] - ETA: 1:15 - loss: 0.2733
29184/36989 [======================>.......] - ETA: 1:12 - loss: 0.2729
29440/36989 [======================>.......] - ETA: 1:10 - loss: 0.2724
29696/36989 [=======================>......] - ETA: 1:08 - loss: 0.2720
29952/36989 [=======================>......] - ETA: 1:05 - loss: 0.2715
30208/36989 [=======================>......] - ETA: 1:03 - loss: 0.2712
30464/36989 [=======================>......] - ETA: 1:00 - loss: 0.2706
30720/36989 [=======================>......] - ETA: 58s - loss: 0.2702 
30976/36989 [========================>.....] - ETA: 56s - loss: 0.2697
31232/36989 [========================>.....] - ETA: 53s - loss: 0.2695
31488/36989 [========================>.....] - ETA: 51s - loss: 0.2691
31744/36989 [========================>.....] - ETA: 48s - loss: 0.2686
32000/36989 [========================>.....] - ETA: 46s - loss: 0.2682
32256/36989 [=========================>....] - ETA: 44s - loss: 0.2678
32512/36989 [=========================>....] - ETA: 41s - loss: 0.2674
32768/36989 [=========================>....] - ETA: 39s - loss: 0.2672
33024/36989 [=========================>....] - ETA: 36s - loss: 0.2667
33280/36989 [=========================>....] - ETA: 34s - loss: 0.2663
33536/36989 [==========================>...] - ETA: 32s - loss: 0.2660
33792/36989 [==========================>...] - ETA: 29s - loss: 0.2656
34048/36989 [==========================>...] - ETA: 27s - loss: 0.2653
34304/36989 [==========================>...] - ETA: 25s - loss: 0.2650
34560/36989 [===========================>..] - ETA: 22s - loss: 0.2646
34816/36989 [===========================>..] - ETA: 20s - loss: 0.2643
35072/36989 [===========================>..] - ETA: 17s - loss: 0.2639
35328/36989 [===========================>..] - ETA: 15s - loss: 0.2637
35584/36989 [===========================>..] - ETA: 13s - loss: 0.2634
35840/36989 [============================>.] - ETA: 10s - loss: 0.2631
36096/36989 [============================>.] - ETA: 8s - loss: 0.2628 
36352/36989 [============================>.] - ETA: 5s - loss: 0.2625
36608/36989 [============================>.] - ETA: 3s - loss: 0.2622
36864/36989 [============================>.] - ETA: 1s - loss: 0.2618
36989/36989 [==============================] - 350s 9ms/step - loss: 0.2617 - val_loss: 0.2891
---
Input python program: 
n = int(input())
nums = list(map(int, input().split()))
sum = 0
for num in nums:
    sum += num
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
num = int(input())
sum = 0
for i in [int(x) for x in input().split()]:
    sum += i
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
map(int,input())
array = list(map(int,input().split()))
sum = 0
for x in array:
    sum += x
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
def arraySum(array):
    total = 0
    for num in array:
        total += num
    return total

n = int(input())
arrayString = input().split()
arrayInt = []

for i in range(0,n):
    arrayInt.append(int(arrayString[i]))

print (arraySum(arrayInt))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
# Inputs
n = int(input())
array = input().split()

# Solution
array = list(map(int, array))
print(sum(array))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
Epoch: 1/10
Train on 36989 samples, validate on 1947 samples
Epoch 1/1

  256/36989 [..............................] - ETA: 5:48 - loss: 0.2351
  512/36989 [..............................] - ETA: 5:41 - loss: 0.2297
  768/36989 [..............................] - ETA: 5:39 - loss: 0.2269
 1024/36989 [..............................] - ETA: 5:38 - loss: 0.2254
 1280/36989 [>.............................] - ETA: 5:36 - loss: 0.2222
 1536/36989 [>.............................] - ETA: 5:32 - loss: 0.2195
 1792/36989 [>.............................] - ETA: 5:30 - loss: 0.2184
 2048/36989 [>.............................] - ETA: 5:28 - loss: 0.2168
 2304/36989 [>.............................] - ETA: 5:25 - loss: 0.2183
 2560/36989 [=>............................] - ETA: 5:21 - loss: 0.2189
 2816/36989 [=>............................] - ETA: 5:18 - loss: 0.2189
 3072/36989 [=>............................] - ETA: 5:15 - loss: 0.2190
 3328/36989 [=>............................] - ETA: 5:13 - loss: 0.2180
 3584/36989 [=>............................] - ETA: 5:10 - loss: 0.2168
 3840/36989 [==>...........................] - ETA: 5:08 - loss: 0.2172
 4096/36989 [==>...........................] - ETA: 5:05 - loss: 0.2168
 4352/36989 [==>...........................] - ETA: 5:03 - loss: 0.2171
 4608/36989 [==>...........................] - ETA: 5:00 - loss: 0.2165
 4864/36989 [==>...........................] - ETA: 4:57 - loss: 0.2169
 5120/36989 [===>..........................] - ETA: 4:54 - loss: 0.2176
 5376/36989 [===>..........................] - ETA: 4:52 - loss: 0.2175
 5632/36989 [===>..........................] - ETA: 4:50 - loss: 0.2184
 5888/36989 [===>..........................] - ETA: 4:48 - loss: 0.2182
 6144/36989 [===>..........................] - ETA: 4:46 - loss: 0.2180
 6400/36989 [====>.........................] - ETA: 4:44 - loss: 0.2176
 6656/36989 [====>.........................] - ETA: 4:42 - loss: 0.2171
 6912/36989 [====>.........................] - ETA: 4:40 - loss: 0.2174
 7168/36989 [====>.........................] - ETA: 4:38 - loss: 0.2173
 7424/36989 [=====>........................] - ETA: 4:36 - loss: 0.2176
 7680/36989 [=====>........................] - ETA: 4:34 - loss: 0.2178
 7936/36989 [=====>........................] - ETA: 4:32 - loss: 0.2179
 8192/36989 [=====>........................] - ETA: 4:31 - loss: 0.2182
 8448/36989 [=====>........................] - ETA: 4:28 - loss: 0.2181
 8704/36989 [======>.......................] - ETA: 4:26 - loss: 0.2181
 8960/36989 [======>.......................] - ETA: 4:23 - loss: 0.2181
 9216/36989 [======>.......................] - ETA: 4:21 - loss: 0.2176
 9472/36989 [======>.......................] - ETA: 4:19 - loss: 0.2174
 9728/36989 [======>.......................] - ETA: 4:16 - loss: 0.217138936
« »
Number of samples: 38936
Number of unique input tokens: 128
Number of unique output tokens: 130
Max sequence length for inputs: 498
Max sequence length for outputs: 500
Epoch: 0/10
Train on 36989 samples, validate on 1947 samples
Epoch 1/1

 9984/36989 [=======>......................] - ETA: 4:14 - loss: 0.2172
10240/36989 [=======>......................] - ETA: 4:12 - loss: 0.2171
10496/36989 [=======>......................] - ETA: 4:09 - loss: 0.2168
10752/36989 [=======>......................] - ETA: 4:07 - loss: 0.2165
11008/36989 [=======>......................] - ETA: 4:05 - loss: 0.2163
11264/36989 [========>.....................] - ETA: 4:02 - loss: 0.2164
11520/36989 [========>.....................] - ETA: 4:00 - loss: 0.2162
11776/36989 [========>.....................] - ETA: 3:58 - loss: 0.2164
12032/36989 [========>.....................] - ETA: 3:55 - loss: 0.2167
12288/36989 [========>.....................] - ETA: 3:53 - loss: 0.2168
12544/36989 [=========>....................] - ETA: 3:50 - loss: 0.2168
12800/36989 [=========>....................] - ETA: 3:48 - loss: 0.2168
13056/36989 [=========>....................] - ETA: 3:46 - loss: 0.2167
13312/36989 [=========>....................] - ETA: 3:44 - loss: 0.2163
13568/36989 [==========>...................] - ETA: 3:41 - loss: 0.2161
13824/36989 [==========>...................] - ETA: 3:39 - loss: 0.2161
14080/36989 [==========>...................] - ETA: 3:36 - loss: 0.2161
14336/36989 [==========>...................] - ETA: 3:34 - loss: 0.2162
14592/36989 [==========>...................] - ETA: 3:31 - loss: 0.2163
14848/36989 [===========>..................] - ETA: 3:29 - loss: 0.2162
15104/36989 [===========>..................] - ETA: 3:27 - loss: 0.2162
15360/36989 [===========>..................] - ETA: 3:24 - loss: 0.2159
15616/36989 [===========>..................] - ETA: 3:22 - loss: 0.2155
15872/36989 [===========>..................] - ETA: 3:19 - loss: 0.2154
16128/36989 [============>.................] - ETA: 3:17 - loss: 0.2153
16384/36989 [============>.................] - ETA: 3:14 - loss: 0.2153
16640/36989 [============>.................] - ETA: 3:12 - loss: 0.2151
16896/36989 [============>.................] - ETA: 3:10 - loss: 0.2148
17152/36989 [============>.................] - ETA: 3:07 - loss: 0.2147
17408/36989 [=============>................] - ETA: 3:05 - loss: 0.2148
17664/36989 [=============>................] - ETA: 3:02 - loss: 0.2148
17920/36989 [=============>................] - ETA: 3:00 - loss: 0.2147
18176/36989 [=============>................] - ETA: 2:57 - loss: 0.2145
18432/36989 [=============>................] - ETA: 2:55 - loss: 0.2145
18688/36989 [==============>...............] - ETA: 2:53 - loss: 0.2144
18944/36989 [==============>...............] - ETA: 2:50 - loss: 0.2141
19200/36989 [==============>...............] - ETA: 2:48 - loss: 0.2140
19456/36989 [==============>...............] - ETA: 2:45 - loss: 0.2139
19712/36989 [==============>...............] - ETA: 2:43 - loss: 0.2137
19968/36989 [===============>..............] - ETA: 2:40 - loss: 0.2136
20224/36989 [===============>..............] - ETA: 2:38 - loss: 0.2133
20480/36989 [===============>..............] - ETA: 2:35 - loss: 0.2133
20736/36989 [===============>..............] - ETA: 2:33 - loss: 0.2132
20992/36989 [================>.............] - ETA: 2:31 - loss: 0.2132
21248/36989 [================>.............] - ETA: 2:28 - loss: 0.2130
21504/36989 [================>.............] - ETA: 2:26 - loss: 0.2130
21760/36989 [================>.............] - ETA: 2:23 - loss: 0.2128
22016/36989 [================>.............] - ETA: 2:21 - loss: 0.2126
22272/36989 [=================>............] - ETA: 2:19 - loss: 0.2125
22528/36989 [=================>............] - ETA: 2:16 - loss: 0.2124
22784/36989 [=================>............] - ETA: 2:14 - loss: 0.2122
23040/36989 [=================>............] - ETA: 2:12 - loss: 0.2120
23296/36989 [=================>............] - ETA: 2:09 - loss: 0.2120
23552/36989 [==================>...........] - ETA: 2:07 - loss: 0.2120
23808/36989 [==================>...........] - ETA: 2:04 - loss: 0.2120
24064/36989 [==================>...........] - ETA: 2:02 - loss: 0.2119
24320/36989 [==================>...........] - ETA: 2:00 - loss: 0.2119
24576/36989 [==================>...........] - ETA: 1:57 - loss: 0.2121
24832/36989 [===================>..........] - ETA: 1:55 - loss: 0.2121
25088/36989 [===================>..........] - ETA: 1:52 - loss: 0.2121
25344/36989 [===================>..........] - ETA: 1:50 - loss: 0.2120
25600/36989 [===================>..........] - ETA: 1:47 - loss: 0.2118
25856/36989 [===================>..........] - ETA: 1:45 - loss: 0.2118
26112/36989 [====================>.........] - ETA: 1:43 - loss: 0.2116
26368/36989 [====================>.........] - ETA: 1:40 - loss: 0.2116
26624/36989 [====================>.........] - ETA: 1:38 - loss: 0.2116
26880/36989 [====================>.........] - ETA: 1:35 - loss: 0.2115
27136/36989 [=====================>........] - ETA: 1:33 - loss: 0.2113
27392/36989 [=====================>........] - ETA: 1:31 - loss: 0.2114
27648/36989 [=====================>........] - ETA: 1:28 - loss: 0.2112
27904/36989 [=====================>........] - ETA: 1:26 - loss: 0.2113
28160/36989 [=====================>........] - ETA: 1:23 - loss: 0.2113
28416/36989 [======================>.......] - ETA: 1:21 - loss: 0.2112
28672/36989 [======================>.......] - ETA: 1:18 - loss: 0.2114
28928/36989 [======================>.......] - ETA: 1:16 - loss: 0.2114
29184/36989 [======================>.......] - ETA: 1:14 - loss: 0.2114
29440/36989 [======================>.......] - ETA: 1:11 - loss: 0.2116
29696/36989 [=======================>......] - ETA: 1:09 - loss: 0.2115
29952/36989 [=======================>......] - ETA: 1:06 - loss: 0.2115
30208/36989 [=======================>......] - ETA: 1:04 - loss: 0.2115
30464/36989 [=======================>......] - ETA: 1:01 - loss: 0.2113
30720/36989 [=======================>......] - ETA: 59s - loss: 0.2112 
30976/36989 [========================>.....] - ETA: 57s - loss: 0.2112
31232/36989 [========================>.....] - ETA: 54s - loss: 0.2111
31488/36989 [========================>.....] - ETA: 52s - loss: 0.2111
31744/36989 [========================>.....] - ETA: 49s - loss: 0.2111
32000/36989 [========================>.....] - ETA: 47s - loss: 0.2111
32256/36989 [=========================>....] - ETA: 45s - loss: 0.2110
32512/36989 [=========================>....] - ETA: 42s - loss: 0.2109
32768/36989 [=========================>....] - ETA: 40s - loss: 0.2108
33024/36989 [=========================>....] - ETA: 37s - loss: 0.2108
33280/36989 [=========================>....] - ETA: 35s - loss: 0.2107
33536/36989 [==========================>...] - ETA: 32s - loss: 0.2106
33792/36989 [==========================>...] - ETA: 30s - loss: 0.2106
34048/36989 [==========================>...] - ETA: 27s - loss: 0.2107
34304/36989 [==========================>...] - ETA: 25s - loss: 0.2106
34560/36989 [===========================>..] - ETA: 23s - loss: 0.2107
34816/36989 [===========================>..] - ETA: 20s - loss: 0.2106
35072/36989 [===========================>..] - ETA: 18s - loss: 0.2107
35328/36989 [===========================>..] - ETA: 15s - loss: 0.2106
35584/36989 [===========================>..] - ETA: 13s - loss: 0.2107
35840/36989 [============================>.] - ETA: 10s - loss: 0.2108
36096/36989 [============================>.] - ETA: 8s - loss: 0.2107 
36352/36989 [============================>.] - ETA: 6s - loss: 0.2107
36608/36989 [============================>.] - ETA: 3s - loss: 0.2107
36864/36989 [============================>.] - ETA: 1s - loss: 0.2106
36989/36989 [==============================] - 358s 10ms/step - loss: 0.2106 - val_loss: 0.2932
---
Input python program: 
n = int(input())
nums = list(map(int, input().split()))
sum = 0
for num in nums:
    sum += num
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    int sum=0;
    for(int i=0;i<n;i++){
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
num = int(input())
sum = 0
for i in [int(x) for x in input().split()]:
    sum += i
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    int sum=0;
    for(int i=0;i<n;i++){
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
map(int,input())
array = list(map(int,input().split()))
sum = 0
for x in array:
    sum += x
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    int sum=0;
    for(int i=0;i<n;i++){
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
def arraySum(array):
    total = 0
    for num in array:
        total += num
    return total

n = int(input())
arrayString = input().split()
arrayInt = []

for i in range(0,n):
    arrayInt.append(int(arrayString[i]))

print (arraySum(arrayInt))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    int sum=0;
    for(int i=0;i<n;i++){
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
# Inputs
n = int(input())
array = input().split()

# Solution
array = list(map(int, array))
print(sum(array))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    int sum=0;
    for(int i=0;i<n;i++){
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
Epoch: 2/10
Train on 36989 samples, validate on 1947 samples
Epoch 1/1

  256/36989 [..............................] - ETA: 6:05 - loss: 0.1784
  512/36989 [..............................] - ETA: 5:55 - loss: 0.1881
  768/36989 [..............................] - ETA: 5:47 - loss: 0.1980
 1024/36989 [..............................] - ETA: 5:43 - loss: 0.1953
 1280/36989 [>.............................] - ETA: 5:39 - loss: 0.1956
 1536/36989 [>.............................] - ETA: 5:34 - loss: 0.1977
 1792/36989 [>.............................] - ETA: 5:30 - loss: 0.1991
 2048/36989 [>.............................] - ETA: 5:25 - loss: 0.2013
 2304/36989 [>.............................] - ETA: 5:22 - loss: 0.2003
 2560/36989 [=>............................] - ETA: 5:20 - loss: 0.2006
 2816/36989 [=>............................] - ETA: 5:17 - loss: 0.2012
 3072/36989 [=>............................] - ETA: 5:14 - loss: 0.2021
 3328/36989 [=>............................] - ETA: 5:12 - loss: 0.2023
 3584/36989 [=>............................] - ETA: 5:09 - loss: 0.2020
 3840/36989 [==>...........................] - ETA: 5:07 - loss: 0.2023
 4096/36989 [==>...........................] - ETA: 5:05 - loss: 0.2017
 4352/36989 [==>...........................] - ETA: 5:03 - loss: 0.2020
 4608/36989 [==>...........................] - ETA: 5:01 - loss: 0.2013
 4864/36989 [==>...........................] - ETA: 5:00 - loss: 0.2010
 5120/36989 [===>..........................] - ETA: 4:57 - loss: 0.2005
 5376/36989 [===>..........................] - ETA: 4:55 - loss: 0.2004
 5632/36989 [===>..........................] - ETA: 4:52 - loss: 0.2001
 5888/36989 [===>..........................] - ETA: 4:50 - loss: 0.1996
 6144/36989 [===>..........................] - ETA: 4:47 - loss: 0.1995
 6400/36989 [====>.........................] - ETA: 4:45 - loss: 0.1997
 6656/36989 [====>.........................] - ETA: 4:42 - loss: 0.1994
 6912/36989 [====>.........................] - ETA: 4:40 - loss: 0.1996
 7168/36989 [====>.........................] - ETA: 4:37 - loss: 0.1996
 7424/36989 [=====>........................] - ETA: 4:35 - loss: 0.1995
 7680/36989 [=====>........................] - ETA: 4:33 - loss: 0.1993
 7936/36989 [=====>........................] - ETA: 4:30 - loss: 0.1997
 8192/36989 [=====>........................] - ETA: 4:28 - loss: 0.1997
 8448/36989 [=====>........................] - ETA: 4:26 - loss: 0.1995
 8704/36989 [======>.......................] - ETA: 4:23 - loss: 0.1997
 8960/36989 [======>.......................] - ETA: 4:21 - loss: 0.2000
 9216/36989 [======>.......................] - ETA: 4:19 - loss: 0.2001
 9472/36989 [======>.......................] - ETA: 4:16 - loss: 0.1997
 9728/36989 [======>.......................] - ETA: 4:14 - loss: 0.1993
 9984/36989 [=======>......................] - ETA: 4:12 - loss: 0.1990
10240/36989 [=======>......................] - ETA: 4:09 - loss: 0.1991
10496/36989 [=======>......................] - ETA: 4:07 - loss: 0.1990
10752/36989 [=======>......................] - ETA: 4:04 - loss: 0.1991
11008/36989 [=======>......................] - ETA: 4:02 - loss: 0.1993
11264/36989 [========>.....................] - ETA: 4:00 - loss: 0.1990
11520/36989 [========>.....................] - ETA: 3:57 - loss: 0.1987
11776/36989 [========>.....................] - ETA: 3:55 - loss: 0.1986
12032/36989 [========>.....................] - ETA: 3:52 - loss: 0.1985
12288/36989 [========>.....................] - ETA: 3:50 - loss: 0.1985
12544/36989 [=========>....................] - ETA: 3:47 - loss: 0.1984
12800/36989 [=========>....................] - ETA: 3:45 - loss: 0.1984
13056/36989 [=========>....................] - ETA: 3:42 - loss: 0.1983
13312/36989 [=========>....................] - ETA: 3:40 - loss: 0.1981
13568/36989 [==========>...................] - ETA: 3:38 - loss: 0.1982
13824/36989 [==========>...................] - ETA: 3:35 - loss: 0.1981
14080/36989 [==========>...................] - ETA: 3:33 - loss: 0.1977
14336/36989 [==========>...................] - ETA: 3:31 - loss: 0.1976
14592/36989 [==========>...................] - ETA: 3:29 - loss: 0.1975
14848/36989 [===========>..................] - ETA: 3:26 - loss: 0.1975
15104/36989 [===========>..................] - ETA: 3:24 - loss: 0.1976
15360/36989 [===========>..................] - ETA: 3:21 - loss: 0.1974
15616/36989 [===========>..................] - ETA: 3:19 - loss: 0.1976
15872/36989 [===========>..................] - ETA: 3:16 - loss: 0.1978
16128/36989 [============>.................] - ETA: 3:14 - loss: 0.1980
16384/36989 [============>.................] - ETA: 3:12 - loss: 0.1978
16640/36989 [============>.................] - ETA: 3:09 - loss: 0.1978
16896/36989 [============>.................] - ETA: 3:07 - loss: 0.1977
17152/36989 [============>.................] - ETA: 3:04 - loss: 0.1979
17408/36989 [=============>................] - ETA: 3:01 - loss: 0.1978
17664/36989 [=============>................] - ETA: 2:59 - loss: 0.1977
17920/36989 [=============>................] - ETA: 2:57 - loss: 0.1976
18176/36989 [=============>................] - ETA: 2:54 - loss: 0.1976
18432/36989 [=============>................] - ETA: 2:52 - loss: 0.1976
18688/36989 [==============>...............] - ETA: 2:49 - loss: 0.1973
18944/36989 [==============>...............] - ETA: 2:47 - loss: 0.1971
19200/36989 [==============>...............] - ETA: 2:44 - loss: 0.1972
19456/36989 [==============>...............] - ETA: 2:42 - loss: 0.1973
19712/36989 [==============>...............] - ETA: 2:40 - loss: 0.1973
19968/36989 [===============>..............] - ETA: 2:37 - loss: 0.1973
20224/36989 [===============>..............] - ETA: 2:35 - loss: 0.1974
20480/36989 [===============>..............] - ETA: 2:32 - loss: 0.1974
20736/36989 [===============>..............] - ETA: 2:30 - loss: 0.1973
20992/36989 [================>.............] - ETA: 2:28 - loss: 0.1975
21248/36989 [================>.............] - ETA: 2:25 - loss: 0.1975
21504/36989 [================>.............] - ETA: 2:23 - loss: 0.1975
21760/36989 [================>.............] - ETA: 2:20 - loss: 0.1973
22016/36989 [================>.............] - ETA: 2:18 - loss: 0.1974
22272/36989 [=================>............] - ETA: 2:16 - loss: 0.1972
22528/36989 [=================>............] - ETA: 2:13 - loss: 0.1970
22784/36989 [=================>............] - ETA: 2:11 - loss: 0.1970
23040/36989 [=================>............] - ETA: 2:08 - loss: 0.1970
23296/36989 [=================>............] - ETA: 2:06 - loss: 0.1969
23552/36989 [==================>...........] - ETA: 2:04 - loss: 0.1969
23808/36989 [==================>...........] - ETA: 2:01 - loss: 0.1968
24064/36989 [==================>...........] - ETA: 1:59 - loss: 0.1967
24320/36989 [==================>...........] - ETA: 1:57 - loss: 0.1966
24576/36989 [==================>...........] - ETA: 1:54 - loss: 0.1964
24832/36989 [===================>..........] - ETA: 1:52 - loss: 0.1964
25088/36989 [===================>..........] - ETA: 1:49 - loss: 0.1962
25344/36989 [===================>..........] - ETA: 1:47 - loss: 0.1961
25600/36989 [===================>..........] - ETA: 1:45 - loss: 0.1960
25856/36989 [===================>..........] - ETA: 1:42 - loss: 0.1959
26112/36989 [====================>.........] - ETA: 1:40 - loss: 0.1958
26368/36989 [====================>.........] - ETA: 1:38 - loss: 0.1957
26624/36989 [====================>.........] - ETA: 1:35 - loss: 0.1956
26880/36989 [====================>.........] - ETA: 1:33 - loss: 0.1953
27136/36989 [=====================>........] - ETA: 1:30 - loss: 0.1952
27392/36989 [=====================>........] - ETA: 1:28 - loss: 0.1952
27648/36989 [=====================>........] - ETA: 1:26 - loss: 0.1952
27904/36989 [=====================>........] - ETA: 1:23 - loss: 0.1953
28160/36989 [=====================>........] - ETA: 1:21 - loss: 0.1951
28416/36989 [======================>.......] - ETA: 1:19 - loss: 0.1950
28672/36989 [======================>.......] - ETA: 1:16 - loss: 0.1948
28928/36989 [======================>.......] - ETA: 1:14 - loss: 0.1948
29184/36989 [======================>.......] - ETA: 1:12 - loss: 0.1947
29440/36989 [======================>.......] - ETA: 1:09 - loss: 0.1947
29696/36989 [=======================>......] - ETA: 1:07 - loss: 0.1946
29952/36989 [=======================>......] - ETA: 1:05 - loss: 0.1945
30208/36989 [=======================>......] - ETA: 1:02 - loss: 0.1946
30464/36989 [=======================>......] - ETA: 1:00 - loss: 0.1945
30720/36989 [=======================>......] - ETA: 58s - loss: 0.1945 
30976/36989 [========================>.....] - ETA: 55s - loss: 0.1945
31232/36989 [========================>.....] - ETA: 53s - loss: 0.1945
31488/36989 [========================>.....] - ETA: 50s - loss: 0.1944
31744/36989 [========================>.....] - ETA: 48s - loss: 0.1943
32000/36989 [========================>.....] - ETA: 46s - loss: 0.1944
32256/36989 [=========================>....] - ETA: 43s - loss: 0.1943
32512/36989 [=========================>....] - ETA: 41s - loss: 0.1943
32768/36989 [=========================>....] - ETA: 39s - loss: 0.1943
33024/36989 [=========================>....] - ETA: 36s - loss: 0.1943
33280/36989 [=========================>....] - ETA: 34s - loss: 0.1942
33536/36989 [==========================>...] - ETA: 31s - loss: 0.1941
33792/36989 [==========================>...] - ETA: 29s - loss: 0.1940
34048/36989 [==========================>...] - ETA: 27s - loss: 0.1940
34304/36989 [==========================>...] - ETA: 24s - loss: 0.1940
34560/36989 [===========================>..] - ETA: 22s - loss: 0.1940
34816/36989 [===========================>..] - ETA: 20s - loss: 0.1940
35072/36989 [===========================>..] - ETA: 17s - loss: 0.1940
35328/36989 [===========================>..] - ETA: 15s - loss: 0.1940
35584/36989 [===========================>..] - ETA: 12s - loss: 0.1940
35840/36989 [============================>.] - ETA: 10s - loss: 0.1939
36096/36989 [============================>.] - ETA: 8s - loss: 0.1937 
36352/36989 [============================>.] - ETA: 5s - loss: 0.1936
36608/36989 [============================>.] - ETA: 3s - loss: 0.1936
36864/36989 [============================>.] - ETA: 1s - loss: 0.1936
36989/36989 [==============================] - 348s 9ms/step - loss: 0.1935 - val_loss: 0.2987
---
Input python program: 
n = int(input())
nums = list(map(int, input().split()))
sum = 0
for num in nums:
    sum += num
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a;
        sum+=a;
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
num = int(input())
sum = 0
for i in [int(x) for x in input().split()]:
    sum += i
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a;
        sum+=a;
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
map(int,input())
array = list(map(int,input().split()))
sum = 0
for x in array:
    sum += x
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a;
        sum+=a;
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
def arraySum(array):
    total = 0
    for num in array:
        total += num
    return total

n = int(input())
arrayString = input().split()
arrayInt = []

for i in range(0,n):
    arrayInt.append(int(arrayString[i]))

print (arraySum(arrayInt))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a;
        sum+=a;
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
# Inputs
n = int(input())
array = input().split()

# Solution
array = list(map(int, array))
print(sum(array))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a;
        sum+=a;
    }
    cout<<sum;
    return 0;
}»
Epoch: 3/10
Train on 36989 samples, validate on 1947 samples
Epoch 1/1

  256/36989 [..............................] - ETA: 5:37 - loss: 0.1782
  512/36989 [..............................] - ETA: 5:36 - loss: 0.1868
  768/36989 [..............................] - ETA: 5:32 - loss: 0.1833
 1024/36989 [..............................] - ETA: 5:26 - loss: 0.1857
 1280/36989 [>.............................] - ETA: 5:22 - loss: 0.1859
 1536/36989 [>.............................] - ETA: 5:20 - loss: 0.1846
 1792/36989 [>.............................] - ETA: 5:17 - loss: 0.1841
 2048/36989 [>.............................] - ETA: 5:15 - loss: 0.1839
 2304/36989 [>.............................] - ETA: 5:12 - loss: 0.1852
 2560/36989 [=>............................] - ETA: 5:11 - loss: 0.1860
 2816/36989 [=>............................] - ETA: 5:09 - loss: 0.1857
 3072/36989 [=>............................] - ETA: 5:07 - loss: 0.1862
 3328/36989 [=>............................] - ETA: 5:05 - loss: 0.1860
 3584/36989 [=>............................] - ETA: 5:03 - loss: 0.1858
 3840/36989 [==>...........................] - ETA: 5:00 - loss: 0.1866
 4096/36989 [==>...........................] - ETA: 4:58 - loss: 0.1860
 4352/36989 [==>...........................] - ETA: 4:55 - loss: 0.1858
 4608/36989 [==>...........................] - ETA: 4:53 - loss: 0.1862
 4864/36989 [==>...........................] - ETA: 4:50 - loss: 0.1861
 5120/36989 [===>..........................] - ETA: 4:48 - loss: 0.1858
 5376/36989 [===>..........................] - ETA: 4:46 - loss: 0.1858
 5632/36989 [===>..........................] - ETA: 4:43 - loss: 0.1861
 5888/36989 [===>..........................] - ETA: 4:41 - loss: 0.1858
 6144/36989 [===>..........................] - ETA: 4:39 - loss: 0.1866
 6400/36989 [====>.........................] - ETA: 4:37 - loss: 0.1866
 6656/36989 [====>.........................] - ETA: 4:35 - loss: 0.1872
 6912/36989 [====>.........................] - ETA: 4:33 - loss: 0.1874
 7168/36989 [====>.........................] - ETA: 4:31 - loss: 0.1870
 7424/36989 [=====>........................] - ETA: 4:29 - loss: 0.1875
 7680/36989 [=====>........................] - ETA: 4:26 - loss: 0.1869
 7936/36989 [=====>........................] - ETA: 4:24 - loss: 0.1867
 8192/36989 [=====>........................] - ETA: 4:21 - loss: 0.1860
 8448/36989 [=====>........................] - ETA: 4:19 - loss: 0.1859
 8704/36989 [======>.......................] - ETA: 4:17 - loss: 0.1859
 8960/36989 [======>.......................] - ETA: 4:14 - loss: 0.1858
 9216/36989 [======>.......................] - ETA: 4:12 - loss: 0.1856
 9472/36989 [======>.......................] - ETA: 4:10 - loss: 0.1853
 9728/36989 [======>.......................] - ETA: 4:07 - loss: 0.1853
 9984/36989 [=======>......................] - ETA: 4:05 - loss: 0.1854
10240/36989 [=======>......................] - ETA: 4:03 - loss: 0.1855
10496/36989 [=======>......................] - ETA: 4:00 - loss: 0.1851
10752/36989 [=======>......................] - ETA: 3:58 - loss: 0.1852
11008/36989 [=======>......................] - ETA: 3:56 - loss: 0.1850
11264/36989 [========>.....................] - ETA: 3:54 - loss: 0.1851
11520/36989 [========>.....................] - ETA: 3:51 - loss: 0.1854
11776/36989 [========>.....................] - ETA: 3:49 - loss: 0.1856
12032/36989 [========>.....................] - ETA: 3:47 - loss: 0.1856
12288/36989 [========>.....................] - ETA: 3:44 - loss: 0.1856
12544/36989 [=========>....................] - ETA: 3:42 - loss: 0.1853
12800/36989 [=========>....................] - ETA: 3:40 - loss: 0.1852
13056/36989 [=========>....................] - ETA: 3:37 - loss: 0.1849
13312/36989 [=========>....................] - ETA: 3:35 - loss: 0.1847
13568/36989 [==========>...................] - ETA: 3:33 - loss: 0.1843
13824/36989 [==========>...................] - ETA: 3:30 - loss: 0.1840
14080/36989 [==========>...................] - ETA: 3:28 - loss: 0.1840
14336/36989 [==========>...................] - ETA: 3:26 - loss: 0.1839
14592/36989 [==========>...................] - ETA: 3:23 - loss: 0.1838
14848/36989 [===========>..................] - ETA: 3:21 - loss: 0.1837
15104/36989 [===========>..................] - ETA: 3:19 - loss: 0.1835
15360/36989 [===========>..................] - ETA: 3:17 - loss: 0.1834
15616/36989 [===========>..................] - ETA: 3:15 - loss: 0.1834
15872/36989 [===========>..................] - ETA: 3:12 - loss: 0.1832
16128/36989 [============>.................] - ETA: 3:10 - loss: 0.1832
16384/36989 [============>.................] - ETA: 3:08 - loss: 0.1832
16640/36989 [============>.................] - ETA: 3:05 - loss: 0.1831
16896/36989 [============>.................] - ETA: 3:03 - loss: 0.1831
17152/36989 [============>.................] - ETA: 3:01 - loss: 0.1830
17408/36989 [=============>................] - ETA: 2:58 - loss: 0.1829
17664/36989 [=============>................] - ETA: 2:56 - loss: 0.1830
17920/36989 [=============>................] - ETA: 2:54 - loss: 0.1830
18176/36989 [=============>................] - ETA: 2:51 - loss: 0.1829
18432/36989 [=============>................] - ETA: 2:49 - loss: 0.1828
18688/36989 [==============>...............] - ETA: 2:47 - loss: 0.1827
18944/36989 [==============>...............] - ETA: 2:44 - loss: 0.1829
19200/36989 [==============>...............] - ETA: 2:42 - loss: 0.1828
19456/36989 [==============>...............] - ETA: 2:40 - loss: 0.1827
19712/36989 [==============>...............] - ETA: 2:37 - loss: 0.1828
19968/36989 [===============>..............] - ETA: 2:35 - loss: 0.1828
20224/36989 [===============>..............] - ETA: 2:33 - loss: 0.1828
20480/36989 [===============>..............] - ETA: 2:30 - loss: 0.1827
20736/36989 [===============>..............] - ETA: 2:28 - loss: 0.1827
20992/36989 [================>.............] - ETA: 2:26 - loss: 0.1827
21248/36989 [================>.............] - ETA: 2:23 - loss: 0.1826
21504/36989 [================>.............] - ETA: 2:21 - loss: 0.1827
21760/36989 [================>.............] - ETA: 2:19 - loss: 0.1828
22016/36989 [================>.............] - ETA: 2:17 - loss: 0.1829
22272/36989 [=================>............] - ETA: 2:14 - loss: 0.1828
22528/36989 [=================>............] - ETA: 2:12 - loss: 0.1828
22784/36989 [=================>............] - ETA: 2:10 - loss: 0.1827
23040/36989 [=================>............] - ETA: 2:07 - loss: 0.1828
23296/36989 [=================>............] - ETA: 2:05 - loss: 0.1827
23552/36989 [==================>...........] - ETA: 2:03 - loss: 0.1828
23808/36989 [==================>...........] - ETA: 2:00 - loss: 0.1826
24064/36989 [==================>...........] - ETA: 1:58 - loss: 0.1825
24320/36989 [==================>...........] - ETA: 1:56 - loss: 0.1825
24576/36989 [==================>...........] - ETA: 1:53 - loss: 0.1824
24832/36989 [===================>..........] - ETA: 1:51 - loss: 0.1824
25088/36989 [===================>..........] - ETA: 1:49 - loss: 0.1823
25344/36989 [===================>..........] - ETA: 1:46 - loss: 0.1821
25600/36989 [===================>..........] - ETA: 1:44 - loss: 0.1821
25856/36989 [===================>..........] - ETA: 1:42 - loss: 0.1820
26112/36989 [====================>.........] - ETA: 1:39 - loss: 0.1820
26368/36989 [====================>.........] - ETA: 1:37 - loss: 0.1818
26624/36989 [====================>.........] - ETA: 1:35 - loss: 0.1817
26880/36989 [====================>.........] - ETA: 1:32 - loss: 0.1817
27136/36989 [=====================>........] - ETA: 1:30 - loss: 0.1817
27392/36989 [=====================>........] - ETA: 1:28 - loss: 0.1818
27648/36989 [=====================>........] - ETA: 1:25 - loss: 0.1816
27904/36989 [=====================>........] - ETA: 1:23 - loss: 0.1815
28160/36989 [=====================>........] - ETA: 1:21 - loss: 0.1814
28416/36989 [======================>.......] - ETA: 1:18 - loss: 0.1814
28672/36989 [======================>.......] - ETA: 1:16 - loss: 0.1812
28928/36989 [======================>.......] - ETA: 1:13 - loss: 0.1811
29184/36989 [======================>.......] - ETA: 1:11 - loss: 0.1811
29440/36989 [======================>.......] - ETA: 1:09 - loss: 0.1810
29696/36989 [=======================>......] - ETA: 1:06 - loss: 0.1809
29952/36989 [=======================>......] - ETA: 1:04 - loss: 0.1809
30208/36989 [=======================>......] - ETA: 1:02 - loss: 0.1809
30464/36989 [=======================>......] - ETA: 59s - loss: 0.1808 
30720/36989 [=======================>......] - ETA: 57s - loss: 0.1807
30976/36989 [========================>.....] - ETA: 55s - loss: 0.1808
31232/36989 [========================>.....] - ETA: 52s - loss: 0.1806
31488/36989 [========================>.....] - ETA: 50s - loss: 0.1805
31744/36989 [========================>.....] - ETA: 48s - loss: 0.1804
32000/36989 [========================>.....] - ETA: 45s - loss: 0.1804
32256/36989 [=========================>....] - ETA: 43s - loss: 0.1802
32512/36989 [=========================>....] - ETA: 41s - loss: 0.1801
32768/36989 [=========================>....] - ETA: 38s - loss: 0.1801
33024/36989 [=========================>....] - ETA: 36s - loss: 0.1800
33280/36989 [=========================>....] - ETA: 34s - loss: 0.1799
33536/36989 [==========================>...] - ETA: 31s - loss: 0.1798
33792/36989 [==========================>...] - ETA: 29s - loss: 0.1796
34048/36989 [==========================>...] - ETA: 27s - loss: 0.1796
34304/36989 [==========================>...] - ETA: 24s - loss: 0.1796
34560/36989 [===========================>..] - ETA: 22s - loss: 0.1795
34816/36989 [===========================>..] - ETA: 19s - loss: 0.1793
35072/36989 [===========================>..] - ETA: 17s - loss: 0.1793
35328/36989 [===========================>..] - ETA: 15s - loss: 0.1793
35584/36989 [===========================>..] - ETA: 12s - loss: 0.1792
35840/36989 [============================>.] - ETA: 10s - loss: 0.1791
36096/36989 [============================>.] - ETA: 8s - loss: 0.1790 
36352/36989 [============================>.] - ETA: 5s - loss: 0.1790
36608/36989 [============================>.] - ETA: 3s - loss: 0.1790
36864/36989 [============================>.] - ETA: 1s - loss: 0.1789
36989/36989 [==============================] - 345s 9ms/step - loss: 0.1789 - val_loss: 0.2970
---
Input python program: 
n = int(input())
nums = list(map(int, input().split()))
sum = 0
for num in nums:
    sum += num
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
num = int(input())
sum = 0
for i in [int(x) for x in input().split()]:
    sum += i
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
map(int,input())
array = list(map(int,input().split()))
sum = 0
for x in array:
    sum += x
print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
def arraySum(array):
    total = 0
    for num in array:
        total += num
    return total

n = int(input())
arrayString = input().split()
arrayInt = []

for i in range(0,n):
    arrayInt.append(int(arrayString[i]))

print (arraySum(arrayInt))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
---
Input python program: 
# Inputs
n = int(input())
array = input().split()

# Solution
array = list(map(int, array))
print(sum(array))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
Epoch: 4/10
Train on 36989 samples, validate on 1947 samples
Epoch 1/1

  256/36989 [..............................] - ETA: 5:39 - loss: 0.1644
  512/36989 [..............................] - ETA: 5:33 - loss: 0.1682
  768/36989 [..............................] - ETA: 5:28 - loss: 0.1686
 1024/36989 [..............................] - ETA: 5:25 - loss: 0.1688
 1280/36989 [>.............................] - ETA: 5:23 - loss: 0.1700
 1536/36989 [>.............................] - ETA: 5:19 - loss: 0.1709
 1792/36989 [>.............................] - ETA: 5:17 - loss: 0.1718
 2048/36989 [>.............................] - ETA: 5:14 - loss: 0.1729
 2304/36989 [>.............................] - ETA: 5:12 - loss: 0.1726
 2560/36989 [=>............................] - ETA: 5:09 - loss: 0.1728
 2816/36989 [=>............................] - ETA: 5:07 - loss: 0.1712
 3072/36989 [=>............................] - ETA: 5:04 - loss: 0.1705
 3328/36989 [=>............................] - ETA: 5:02 - loss: 0.1705
 3584/36989 [=>............................] - ETA: 4:59 - loss: 0.1694
 3840/36989 [==>...........................] - ETA: 4:58 - loss: 0.1690
 4096/36989 [==>...........................] - ETA: 4:56 - loss: 0.1692
 4352/36989 [==>...........................] - ETA: 4:54 - loss: 0.1698
 4608/36989 [==>...........................] - ETA: 4:52 - loss: 0.1694
 4864/36989 [==>...........................] - ETA: 4:50 - loss: 0.1695
 5120/36989 [===>..........................] - ETA: 4:47 - loss: 0.1696
 5376/36989 [===>..........................] - ETA: 4:45 - loss: 0.1694
 5632/36989 [===>..........................] - ETA: 4:43 - loss: 0.1685
 5888/36989 [===>..........................] - ETA: 4:41 - loss: 0.1690
 6144/36989 [===>..........................] - ETA: 4:39 - loss: 0.1688
 6400/36989 [====>.........................] - ETA: 4:37 - loss: 0.1686
 6656/36989 [====>.........................] - ETA: 4:35 - loss: 0.1685
 6912/36989 [====>.........................] - ETA: 4:33 - loss: 0.1689
 7168/36989 [====>.........................] - ETA: 4:30 - loss: 0.1692
 7424/36989 [=====>........................] - ETA: 4:28 - loss: 0.1692
 7680/36989 [=====>........................] - ETA: 4:25 - loss: 0.1694
 7936/36989 [=====>........................] - ETA: 4:23 - loss: 0.1695
 8192/36989 [=====>........................] - ETA: 4:21 - loss: 0.1695
 8448/36989 [=====>........................] - ETA: 4:19 - loss: 0.1694
 8704/36989 [======>.......................] - ETA: 4:17 - loss: 0.1692
 8960/36989 [======>.......................] - ETA: 4:14 - loss: 0.1693
 9216/36989 [======>.......................] - ETA: 4:12 - loss: 0.1692
 9472/36989 [======>.......................] - ETA: 4:10 - loss: 0.1693
 9728/36989 [======>.......................] - ETA: 4:08 - loss: 0.1693
 9984/36989 [=======>......................] - ETA: 4:06 - loss: 0.1694
10240/36989 [=======>......................] - ETA: 4:03 - loss: 0.1693
10496/36989 [=======>......................] - ETA: 4:01 - loss: 0.1692
10752/36989 [=======>......................] - ETA: 3:58 - loss: 0.1694
11008/36989 [=======>......................] - ETA: 3:56 - loss: 0.1694
11264/36989 [========>.....................] - ETA: 3:54 - loss: 0.1694
11520/36989 [========>.....................] - ETA: 3:51 - loss: 0.1692
11776/36989 [========>.....................] - ETA: 3:49 - loss: 0.1690
12032/36989 [========>.....................] - ETA: 3:47 - loss: 0.1693
12288/36989 [========>.....................] - ETA: 3:45 - loss: 0.1693
12544/36989 [=========>....................] - ETA: 3:42 - loss: 0.1691
12800/36989 [=========>....................] - ETA: 3:40 - loss: 0.1690
13056/36989 [=========>....................] - ETA: 3:38 - loss: 0.1687
13312/36989 [=========>....................] - ETA: 3:35 - loss: 0.1687
13568/36989 [==========>...................] - ETA: 3:33 - loss: 0.1686
13824/36989 [==========>...................] - ETA: 3:31 - loss: 0.1683
14080/36989 [==========>...................] - ETA: 3:28 - loss: 0.1682
14336/36989 [==========>...................] - ETA: 3:26 - loss: 0.1681
14592/36989 [==========>...................] - ETA: 3:24 - loss: 0.1683
14848/36989 [===========>..................] - ETA: 3:21 - loss: 0.1683
15104/36989 [===========>..................] - ETA: 3:19 - loss: 0.1683
15360/36989 [===========>..................] - ETA: 3:16 - loss: 0.1683
15616/36989 [===========>..................] - ETA: 3:14 - loss: 0.1683
15872/36989 [===========>..................] - ETA: 3:12 - loss: 0.1681
16128/36989 [============>.................] - ETA: 3:09 - loss: 0.1681
16384/36989 [============>.................] - ETA: 3:07 - loss: 0.1680
16640/36989 [============>.................] - ETA: 3:05 - loss: 0.1680
16896/36989 [============>.................] - ETA: 3:03 - loss: 0.168038936
« »
Number of samples: 38936
Number of unique input tokens: 128
Number of unique output tokens: 130
Max sequence length for inputs: 498
Max sequence length for outputs: 500
Epoch: 0/10
Train on 36989 samples, validate on 1947 samples
Epoch 1/1

  256/36989 [..............................] - ETA: 14:52 - loss: 0.1653
  512/36989 [..............................] - ETA: 10:13 - loss: 0.4878
  768/36989 [..............................] - ETA: 8:41 - loss: 0.4735 
 1024/36989 [..............................] - ETA: 7:52 - loss: 0.4486
 1280/36989 [>.............................] - ETA: 7:21 - loss: 0.4333
 1536/36989 [>.............................] - ETA: 7:01 - loss: 0.4201
 1792/36989 [>.............................] - ETA: 6:45 - loss: 0.4055
 2048/36989 [>.............................] - ETA: 6:33 - loss: 0.3947
 2304/36989 [>.............................] - ETA: 6:24 - loss: 0.3876
 2560/36989 [=>............................] - ETA: 6:15 - loss: 0.3795
 2816/36989 [=>............................] - ETA: 6:08 - loss: 0.3731
 3072/36989 [=>............................] - ETA: 6:01 - loss: 0.3681
 3328/36989 [=>............................] - ETA: 5:55 - loss: 0.3613
 3584/36989 [=>............................] - ETA: 5:49 - loss: 0.3559
 3840/36989 [==>...........................] - ETA: 5:44 - loss: 0.3495
 4096/36989 [==>...........................] - ETA: 5:41 - loss: 0.3456
 4352/36989 [==>...........................] - ETA: 5:37 - loss: 0.3414
 4608/36989 [==>...........................] - ETA: 5:33 - loss: 0.3372
 4864/36989 [==>...........................] - ETA: 5:30 - loss: 0.3334
 5120/36989 [===>..........................] - ETA: 5:26 - loss: 0.3300
 5376/36989 [===>..........................] - ETA: 5:22 - loss: 0.3257
 5632/36989 [===>..........................] - ETA: 5:18 - loss: 0.3221
 5888/36989 [===>..........................] - ETA: 5:15 - loss: 0.3194
 6144/36989 [===>..........................] - ETA: 5:12 - loss: 0.3163
 6400/36989 [====>.........................] - ETA: 5:09 - loss: 0.3138
 6656/36989 [====>.........................] - ETA: 5:05 - loss: 0.3107
 6912/36989 [====>.........................] - ETA: 5:01 - loss: 0.3083
 7168/36989 [====>.........................] - ETA: 4:58 - loss: 0.3053
 7424/36989 [=====>........................] - ETA: 4:54 - loss: 0.3026
 7680/36989 [=====>........................] - ETA: 4:51 - loss: 0.3000
 7936/36989 [=====>........................] - ETA: 4:49 - loss: 0.2973
 8192/36989 [=====>........................] - ETA: 4:46 - loss: 0.2951
 8448/36989 [=====>........................] - ETA: 4:43 - loss: 0.2928
 8704/36989 [======>.......................] - ETA: 4:40 - loss: 0.2907
 8960/36989 [======>.......................] - ETA: 4:37 - loss: 0.2889
 9216/36989 [======>.......................] - ETA: 4:34 - loss: 0.2866
 9472/36989 [======>.......................] - ETA: 4:31 - loss: 0.2845
 9728/36989 [======>.......................] - ETA: 4:28 - loss: 0.2826
 9984/36989 [=======>......................] - ETA: 4:25 - loss: 0.2804
10240/36989 [=======>......................] - ETA: 4:22 - loss: 0.2788
10496/36989 [=======>......................] - ETA: 4:19 - loss: 0.2773
10752/36989 [=======>......................] - ETA: 4:16 - loss: 0.2756
11008/36989 [=======>......................] - ETA: 4:13 - loss: 0.2740
11264/36989 [========>.....................] - ETA: 4:10 - loss: 0.2724
11520/36989 [========>.....................] - ETA: 4:07 - loss: 0.2711
11776/36989 [========>.....................] - ETA: 4:05 - loss: 0.2691
12032/36989 [========>.....................] - ETA: 4:02 - loss: 0.2677
12288/36989 [========>.....................] - ETA: 3:59 - loss: 0.2663
12544/36989 [=========>....................] - ETA: 3:56 - loss: 0.2647
12800/36989 [=========>....................] - ETA: 3:54 - loss: 0.2632
13056/36989 [=========>....................] - ETA: 3:51 - loss: 0.2618
13312/36989 [=========>....................] - ETA: 3:48 - loss: 0.2606
13568/36989 [==========>...................] - ETA: 3:45 - loss: 0.2591
13824/36989 [==========>...................] - ETA: 3:43 - loss: 0.2577
14080/36989 [==========>...................] - ETA: 3:40 - loss: 0.2561
14336/36989 [==========>...................] - ETA: 3:37 - loss: 0.2548
14592/36989 [==========>...................] - ETA: 3:35 - loss: 0.2534
14848/36989 [===========>..................] - ETA: 3:32 - loss: 0.2520
15104/36989 [===========>..................] - ETA: 3:30 - loss: 0.2511
15360/36989 [===========>..................] - ETA: 3:27 - loss: 0.2501
15616/36989 [===========>..................] - ETA: 3:24 - loss: 0.2490
15872/36989 [===========>..................] - ETA: 3:22 - loss: 0.2480
16128/36989 [============>.................] - ETA: 3:19 - loss: 0.2472
16384/36989 [============>.................] - ETA: 3:17 - loss: 0.2464
16640/36989 [============>.................] - ETA: 3:14 - loss: 0.2454
16896/36989 [============>.................] - ETA: 3:11 - loss: 0.2443
17152/36989 [============>.................] - ETA: 3:09 - loss: 0.2434
17408/36989 [=============>................] - ETA: 3:06 - loss: 0.2422
17664/36989 [=============>................] - ETA: 3:04 - loss: 0.2414
17920/36989 [=============>................] - ETA: 3:01 - loss: 0.2404
18176/36989 [=============>................] - ETA: 2:59 - loss: 0.2394
18432/36989 [=============>................] - ETA: 2:56 - loss: 0.2384
18688/36989 [==============>...............] - ETA: 2:54 - loss: 0.2377
18944/36989 [==============>...............] - ETA: 2:51 - loss: 0.2369
19200/36989 [==============>...............] - ETA: 2:49 - loss: 0.2362
19456/36989 [==============>...............] - ETA: 2:46 - loss: 0.2353
19712/36989 [==============>...............] - ETA: 2:44 - loss: 0.2344
19968/36989 [===============>..............] - ETA: 2:41 - loss: 0.2336
20224/36989 [===============>..............] - ETA: 2:39 - loss: 0.2330
20480/36989 [===============>..............] - ETA: 2:36 - loss: 0.2321
20736/36989 [===============>..............] - ETA: 2:34 - loss: 0.2314
20992/36989 [================>.............] - ETA: 2:31 - loss: 0.2308
21248/36989 [================>.............] - ETA: 2:29 - loss: 0.2300
21504/36989 [================>.............] - ETA: 2:26 - loss: 0.2293
21760/36989 [================>.............] - ETA: 2:24 - loss: 0.2287
22016/36989 [================>.............] - ETA: 2:21 - loss: 0.2279
22272/36989 [=================>............] - ETA: 2:19 - loss: 0.2273
22528/36989 [=================>............] - ETA: 2:17 - loss: 0.2267
22784/36989 [=================>............] - ETA: 2:14 - loss: 0.2261
23040/36989 [=================>............] - ETA: 2:12 - loss: 0.2252
23296/36989 [=================>............] - ETA: 2:09 - loss: 0.2247
23552/36989 [==================>...........] - ETA: 2:07 - loss: 0.2240
23808/36989 [==================>...........] - ETA: 2:04 - loss: 0.2234
24064/36989 [==================>...........] - ETA: 2:02 - loss: 0.2229
24320/36989 [==================>...........] - ETA: 1:59 - loss: 0.2223
24576/36989 [==================>...........] - ETA: 1:57 - loss: 0.2219
24832/36989 [===================>..........] - ETA: 1:54 - loss: 0.2213
25088/36989 [===================>..........] - ETA: 1:52 - loss: 0.2208
25344/36989 [===================>..........] - ETA: 1:49 - loss: 0.2202
25600/36989 [===================>..........] - ETA: 1:47 - loss: 0.2198
25856/36989 [===================>..........] - ETA: 1:45 - loss: 0.2194
26112/36989 [====================>.........] - ETA: 1:42 - loss: 0.2188
26368/36989 [====================>.........] - ETA: 1:40 - loss: 0.2185
26624/36989 [====================>.........] - ETA: 1:37 - loss: 0.2181
26880/36989 [====================>.........] - ETA: 1:35 - loss: 0.2177
27136/36989 [=====================>........] - ETA: 1:32 - loss: 0.2172
27392/36989 [=====================>........] - ETA: 1:30 - loss: 0.2168
27648/36989 [=====================>........] - ETA: 1:27 - loss: 0.2164
27904/36989 [=====================>........] - ETA: 1:25 - loss: 0.2160
28160/36989 [=====================>........] - ETA: 1:23 - loss: 0.2157
28416/36989 [======================>.......] - ETA: 1:20 - loss: 0.2152
28672/36989 [======================>.......] - ETA: 1:18 - loss: 0.2147
28928/36989 [======================>.......] - ETA: 1:15 - loss: 0.2143
29184/36989 [======================>.......] - ETA: 1:13 - loss: 0.2139
29440/36989 [======================>.......] - ETA: 1:10 - loss: 0.2134
29696/36989 [=======================>......] - ETA: 1:08 - loss: 0.2129
29952/36989 [=======================>......] - ETA: 1:06 - loss: 0.2127
30208/36989 [=======================>......] - ETA: 1:03 - loss: 0.2122
30464/36989 [=======================>......] - ETA: 1:01 - loss: 0.2119
30720/36989 [=======================>......] - ETA: 58s - loss: 0.2115 
30976/36989 [========================>.....] - ETA: 56s - loss: 0.2112
31232/36989 [========================>.....] - ETA: 54s - loss: 0.2108
31488/36989 [========================>.....] - ETA: 51s - loss: 0.2107
31744/36989 [========================>.....] - ETA: 49s - loss: 0.2104
32000/36989 [========================>.....] - ETA: 46s - loss: 0.2100
32256/36989 [=========================>....] - ETA: 44s - loss: 0.2097
32512/36989 [=========================>....] - ETA: 41s - loss: 0.2093
32768/36989 [=========================>....] - ETA: 39s - loss: 0.2089
33024/36989 [=========================>....] - ETA: 37s - loss: 0.2086
33280/36989 [=========================>....] - ETA: 34s - loss: 0.2082
33536/36989 [==========================>...] - ETA: 32s - loss: 0.2078
33792/36989 [==========================>...] - ETA: 29s - loss: 0.2076
34048/36989 [==========================>...] - ETA: 27s - loss: 0.2073
34304/36989 [==========================>...] - ETA: 25s - loss: 0.2071
34560/36989 [===========================>..] - ETA: 22s - loss: 0.2068
34816/36989 [===========================>..] - ETA: 20s - loss: 0.2064
35072/36989 [===========================>..] - ETA: 17s - loss: 0.2061
35328/36989 [===========================>..] - ETA: 15s - loss: 0.2058
35584/36989 [===========================>..] - ETA: 13s - loss: 0.2055
35840/36989 [============================>.] - ETA: 10s - loss: 0.2052
36096/36989 [============================>.] - ETA: 8s - loss: 0.2050 
36352/36989 [============================>.] - ETA: 5s - loss: 0.2048
36608/36989 [============================>.] - ETA: 3s - loss: 0.2046
36864/36989 [============================>.] - ETA: 1s - loss: 0.2044
36989/36989 [==============================] - 353s 10ms/step - loss: 0.2043 - val_loss: 0.2989
Simple Array Sum
---
Input python program: 
input()
print(sum(map(lambda x: int(x),list(input().split()))))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    long long int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
Pangrams
---
Input python program: 
def isPanagram(aString):
    letterlist = [None] *26
    aString = (aString.replace(' ','')).lower()
    #print (aString)
    for letter in aString:
        letterlist[ord(letter)-97] = 1
    for x in letterlist:
        if x == None:
            return False
    return True
    #print (letterlist)
    
    


def main():
    sample = input()
    if(isPanagram(sample)):
        print("pangram")
    else:
        print("not pangram")
main()
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    long long int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
Simple Array Sum
---
Input python program: 
n = int(input())
a = list(map(int, input().split()))
print(sum(a))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    long long int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
A Very Big Sum
---
Input python program: 
input()
s = 0
for x in [int(y) for y in input().split(' ')]:
    s += x
    
print(s)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    long long int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
The Love-Letter Mystery
---
Input python program: 
T = int(input())
for i in range(T):
    sum = 0
    a = input()
    l = len(a) // 2
    if len(a) % 2 == 0:
        part1 = a[:l]
        part2 = a[l:]
        part2 = part2[::-1]
    else:
        part1 = a[:l]
        part2 = a[l + 1:]
        part2 = part2[::-1]
    for i in range(l):
        o1 = ord(part1[i])
        o2 = ord(part2[i])
        if o1 >= o2:
            sum += (o1 - o2)
        else:
            sum += (o2 - o1)
    print(sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    long long int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
Gemstones
---
Input python program: 
N=int(input())
elem=[list('abcdefghijklmnopqrstuvwxyz'),[1]*26]
for _ in range(N):
    tgem=input()
    for i in range(26):
        if not elem[0][i] in tgem:
            elem[1][i]*=0
print(sum(elem[1]))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    long long int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
Counting Sort 1
---
Input python program: 
size=int(input())
numlist=input().strip()
nums=[int(num) for num in numlist.split()]
output=''
count=[0 for num in range(0,100)]
for num in nums:
    count[num]+=1
for num in count:
    output=output+str(num)+' '
print(output.strip())
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    long long int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
Making Anagrams
---
Input python program: 
from collections import Counter

a = Counter(input())
b = Counter(input())

changes = 0
for letter in set(a.keys()).difference(b.keys()):
    changes += a[letter]
    
for letter in set(b.keys()).difference(a.keys()):
    changes += b[letter]

for letter in set(a.keys()).intersection(b.keys()):
    changes += abs(a[letter]-b[letter])
                   
print(changes)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    long long int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
Alternating Characters
---
Input python program: 
def min_deletions(word):
    steps = 0
    previous_char = None
    for char in word:
        if char == previous_char:
            steps += 1
        previous_char = char
    return steps

count = int(input())
for i in range(0, count):
    word = input()
    print(min_deletions(word))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    long long int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
Simple Array Sum
---
Input python program: 
lenght = input()
mysum = 0
index = 0
items = input()
for i in items.split():
    index+=1
    if index <= int(lenght):
       mysum += int(i)
    else:
      break
    
print(mysum)
    
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    long long int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
A Very Big Sum
---
Input python program: 
n = input()
lst = [int(i) for i in input().split()]
print(sum(lst))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    long long int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
Migratory Birds
---
Input python program: 
#!/bin/python3

import sys


n = int(input().strip())
types = list(map(int, input().strip().split(' ')))

counter = {}
for x in types:
    counter[x] = counter.get(x, 0) + 1

s = sorted(counter.items(), key=lambda x: x[1], reverse=True)
top_value = s[0][1]

v = list(filter((lambda x: x[1] == top_value), s))[0][0]
print(v)
-
Decoded C++ program:
#include <bits/stdc++.h>

using namespace std;

int main(){
    int n;
    cin >> n;
    int a[100];
    memset(ar, 0, sizeof (in));
    for (int i = 0; i < n; i++) {
        scanf("%d", &va);
        sum += a;
    }
    cout << sum << endl;
    return 0;
}»
A Very Big Sum
---
Input python program: 
def read_line():
    """
    read line of input as a string and convert to list to capture integers
    :return:  a list of the items in the line of data
    """
    line_list = []
    line_string = input()
    for a in line_string.split(' '):
        b = int(a)
        line_list.append(b)
    return line_list

num_to_sum = input ()
sum_list = read_line()
net_sum = 0

for a in sum_list:
    net_sum += a
    
print (net_sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    long long int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
Flipping bits
---
Input python program: 
sample_amounts = int(input())

invert = lambda x: '0' if x == '1' else '1'

for sample in range(sample_amounts):
    test_int = int(input())
    test_bin = bin(test_int)
    bin_digits = "".join(test_bin[2:]).zfill(32)
    
    # Invert all bits
    inverted = "".join([invert(x) for x in bin_digits])
    
    # Convert back to binary
    print (int(inverted, 2))
    
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    long long int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
Counting Sort 1
---
Input python program: 
n = int(input().strip())
ar = [int(tmp) for tmp in input().strip().split(' ')]
d = {i:0 for i in range(100)}
for i in ar:
    d[i] += 1
l = [d[i] for i in range(100)]
print(*l)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    long long int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
Sherlock and Squares
---
Input python program: 
t = int(input())

for i in range (0,t):
 count = 0
 a,b = [int(j) for j in input().strip().split()]
 square1 = a ** (.5)
 if (square1 != int(square1)):
  a1 = int(square1) + 1
 else:
  a1 = int(square1)
 square2 = b ** (.5)
 b1 = int(square2)
 count = b1 - a1 +1
 print(count)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    long long int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
A Very Big Sum
---
Input python program: 
a =  int(input())
line = input()
n = list(map(int, line.split()))
print(sum(n))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    long long int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
The Love-Letter Mystery
---
Input python program: 
numCase = int(input())
for x in range(0, numCase):
    string = input()
    compteur = 0
    for i in range(0,len(string)//2):
        compteur+= abs(ord(string[i])-ord(string[-i-1]))
    print(compteur)
        
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    long long int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
Simple Array Sum
---
Input python program: 
t = input()
n = [int(i) for i in input().split()]
r = 0

for i in n:
    r += i
    
print(r)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    long long int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
A Very Big Sum
---
Input python program: 
N = input()
A = [int(x) for x in input().split()]
print(sum(A))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n;
    cin>>n;
    int a[n];
    long long int sum=0;
    for(int i=0;i<n;i++)
        {
        cin>>a[i];
        sum+=a[i];
    }
    cout<<sum;
    return 0;
}»
Epoch: 1/10
Train on 36989 samples, validate on 1947 samples
Epoch 1/1

  256/36989 [..............................] - ETA: 5:42 - loss: 0.1711
  512/36989 [..............................] - ETA: 5:42 - loss: 0.1688
  768/36989 [..............................] - ETA: 5:38 - loss: 0.1730
 1024/36989 [..............................] - ETA: 5:37 - loss: 0.1718
 1280/36989 [>.............................] - ETA: 5:33 - loss: 0.1684
 1536/36989 [>.............................] - ETA: 5:31 - loss: 0.1695
 1792/36989 [>.............................] - ETA: 5:28 - loss: 0.1690
 2048/36989 [>.............................] - ETA: 5:27 - loss: 0.1698
 2304/36989 [>.............................] - ETA: 5:24 - loss: 0.1684
 2560/36989 [=>............................] - ETA: 5:20 - loss: 0.1689
 2816/36989 [=>............................] - ETA: 5:18 - loss: 0.1702
 3072/36989 [=>............................] - ETA: 5:15 - loss: 0.1683
 3328/36989 [=>............................] - ETA: 5:12 - loss: 0.1682
 3584/36989 [=>............................] - ETA: 5:10 - loss: 0.1692
 3840/36989 [==>...........................] - ETA: 5:08 - loss: 0.1690
 4096/36989 [==>...........................] - ETA: 5:05 - loss: 0.1696
 4352/36989 [==>...........................] - ETA: 5:03 - loss: 0.1690
 4608/36989 [==>...........................] - ETA: 5:00 - loss: 0.1688
 4864/36989 [==>...........................] - ETA: 4:58 - loss: 0.1691
 5120/36989 [===>..........................] - ETA: 4:55 - loss: 0.1688
 5376/36989 [===>..........................] - ETA: 4:53 - loss: 0.1683
 5632/36989 [===>..........................] - ETA: 4:50 - loss: 0.1678
 5888/36989 [===>..........................] - ETA: 4:48 - loss: 0.1676
 6144/36989 [===>..........................] - ETA: 4:46 - loss: 0.1673
 6400/36989 [====>.........................] - ETA: 4:44 - loss: 0.1673
 6656/36989 [====>.........................] - ETA: 4:42 - loss: 0.1673
 6912/36989 [====>.........................] - ETA: 4:39 - loss: 0.1675
 7168/36989 [====>.........................] - ETA: 4:37 - loss: 0.1675
 7424/36989 [=====>........................] - ETA: 4:35 - loss: 0.1671
 7680/36989 [=====>........................] - ETA: 4:32 - loss: 0.1666
 7936/36989 [=====>........................] - ETA: 4:30 - loss: 0.1660
 8192/36989 [=====>........................] - ETA: 4:28 - loss: 0.1659
 8448/36989 [=====>........................] - ETA: 4:25 - loss: 0.1662
 8704/36989 [======>.......................] - ETA: 4:23 - loss: 0.1662
 8960/36989 [======>.......................] - ETA: 4:20 - loss: 0.1662
 9216/36989 [======>.......................] - ETA: 4:18 - loss: 0.1661
 9472/36989 [======>.......................] - ETA: 4:16 - loss: 0.1663
 9728/36989 [======>.......................] - ETA: 4:13 - loss: 0.1659
 9984/36989 [=======>......................] - ETA: 4:11 - loss: 0.1657
10240/36989 [=======>......................] - ETA: 4:08 - loss: 0.1657
10496/36989 [=======>......................] - ETA: 4:06 - loss: 0.1654
10752/36989 [=======>......................] - ETA: 4:03 - loss: 0.1652
11008/36989 [=======>......................] - ETA: 4:01 - loss: 0.1651
11264/36989 [========>.....................] - ETA: 3:59 - loss: 0.1651
11520/36989 [========>.....................] - ETA: 3:57 - loss: 0.1650
11776/36989 [========>.....................] - ETA: 3:54 - loss: 0.1648
12032/36989 [========>.....................] - ETA: 3:52 - loss: 0.1646
12288/36989 [========>.....................] - ETA: 3:49 - loss: 0.1647
12544/36989 [=========>....................] - ETA: 3:47 - loss: 0.1645
12800/36989 [=========>....................] - ETA: 3:44 - loss: 0.1641
13056/36989 [=========>....................] - ETA: 3:42 - loss: 0.1640
13312/36989 [=========>....................] - ETA: 3:39 - loss: 0.1637
13568/36989 [==========>...................] - ETA: 3:37 - loss: 0.1636
13824/36989 [==========>...................] - ETA: 3:34 - loss: 0.1635
14080/36989 [==========>...................] - ETA: 3:32 - loss: 0.1636
14336/36989 [==========>...................] - ETA: 3:30 - loss: 0.1635
14592/36989 [==========>...................] - ETA: 3:27 - loss: 0.1635
14848/36989 [===========>..................] - ETA: 3:25 - loss: 0.1634
15104/36989 [===========>..................] - ETA: 3:23 - loss: 0.1633
15360/36989 [===========>..................] - ETA: 3:20 - loss: 0.1631
15616/36989 [===========>..................] - ETA: 3:18 - loss: 0.1630
15872/36989 [===========>..................] - ETA: 3:15 - loss: 0.1632
16128/36989 [============>.................] - ETA: 3:13 - loss: 0.1632
16384/36989 [============>.................] - ETA: 3:10 - loss: 0.1630
16640/36989 [============>.................] - ETA: 3:08 - loss: 0.1631
16896/36989 [============>.................] - ETA: 3:06 - loss: 0.1630
17152/36989 [============>.................] - ETA: 3:03 - loss: 0.1629
17408/36989 [=============>................] - ETA: 3:01 - loss: 0.1626
17664/36989 [=============>................] - ETA: 2:58 - loss: 0.1626
17920/36989 [=============>................] - ETA: 2:56 - loss: 0.1625
18176/36989 [=============>................] - ETA: 2:54 - loss: 0.1624
18432/36989 [=============>................] - ETA: 2:51 - loss: 0.1623
18688/36989 [==============>...............] - ETA: 2:49 - loss: 0.1622
18944/36989 [==============>...............] - ETA: 2:46 - loss: 0.1622
19200/36989 [==============>...............] - ETA: 2:44 - loss: 0.1621
19456/36989 [==============>...............] - ETA: 2:42 - loss: 0.1621
19712/36989 [==============>...............] - ETA: 2:39 - loss: 0.1621
19968/36989 [===============>..............] - ETA: 2:37 - loss: 0.1621
20224/36989 [===============>..............] - ETA: 2:35 - loss: 0.1620
20480/36989 [===============>..............] - ETA: 2:32 - loss: 0.1618
20736/36989 [===============>..............] - ETA: 2:30 - loss: 0.1617
20992/36989 [================>.............] - ETA: 2:27 - loss: 0.1615
21248/36989 [================>.............] - ETA: 2:25 - loss: 0.1613
21504/36989 [================>.............] - ETA: 2:23 - loss: 0.1611
21760/36989 [================>.............] - ETA: 2:20 - loss: 0.1611
22016/36989 [================>.............] - ETA: 2:18 - loss: 0.1609
22272/36989 [=================>............] - ETA: 2:16 - loss: 0.1610
22528/36989 [=================>............] - ETA: 2:13 - loss: 0.1610
22784/36989 [=================>............] - ETA: 2:11 - loss: 0.1609
23040/36989 [=================>............] - ETA: 2:08 - loss: 0.1609
23296/36989 [=================>............] - ETA: 2:06 - loss: 0.1610
23552/36989 [==================>...........] - ETA: 2:04 - loss: 0.1610
23808/36989 [==================>...........] - ETA: 2:01 - loss: 0.1609
24064/36989 [==================>...........] - ETA: 1:59 - loss: 0.1608
24320/36989 [==================>...........] - ETA: 1:57 - loss: 0.1606
24576/36989 [==================>...........] - ETA: 1:54 - loss: 0.1605
24832/36989 [===================>..........] - ETA: 1:52 - loss: 0.1604
25088/36989 [===================>..........] - ETA: 1:49 - loss: 0.1602
25344/36989 [===================>..........] - ETA: 1:47 - loss: 0.1602
25600/36989 [===================>..........] - ETA: 1:45 - loss: 0.1600
25856/36989 [===================>..........] - ETA: 1:42 - loss: 0.1599
26112/36989 [====================>.........] - ETA: 1:40 - loss: 0.1598
26368/36989 [====================>.........] - ETA: 1:37 - loss: 0.1598
26624/36989 [====================>.........] - ETA: 1:35 - loss: 0.1596
26880/36989 [====================>.........] - ETA: 1:33 - loss: 0.1596
27136/36989 [=====================>........] - ETA: 1:30 - loss: 0.1595
27392/36989 [=====================>........] - ETA: 1:28 - loss: 0.1594
27648/36989 [=====================>........] - ETA: 1:26 - loss: 0.1593
27904/36989 [=====================>........] - ETA: 1:23 - loss: 0.1592
28160/36989 [=====================>........] - ETA: 1:21 - loss: 0.1591
28416/36989 [======================>.......] - ETA: 1:19 - loss: 0.1592
28672/36989 [======================>.......] - ETA: 1:16 - loss: 0.1591
28928/36989 [======================>.......] - ETA: 1:14 - loss: 0.1591
29184/36989 [======================>.......] - ETA: 1:11 - loss: 0.1590
29440/36989 [======================>.......] - ETA: 1:09 - loss: 0.1590
29696/36989 [=======================>......] - ETA: 1:07 - loss: 0.1589
29952/36989 [=======================>......] - ETA: 1:04 - loss: 0.1589
30208/36989 [=======================>......] - ETA: 1:02 - loss: 0.1589
30464/36989 [=======================>......] - ETA: 1:00 - loss: 0.1588
30720/36989 [=======================>......] - ETA: 57s - loss: 0.1588 
30976/36989 [========================>.....] - ETA: 55s - loss: 0.1588
31232/36989 [========================>.....] - ETA: 53s - loss: 0.1588
31488/36989 [========================>.....] - ETA: 50s - loss: 0.1588
31744/36989 [========================>.....] - ETA: 48s - loss: 0.1588
32000/36989 [========================>.....] - ETA: 45s - loss: 0.1587
32256/36989 [=========================>....] - ETA: 43s - loss: 0.1587
32512/36989 [=========================>....] - ETA: 41s - loss: 0.1587
32768/36989 [=========================>....] - ETA: 38s - loss: 0.1586
33024/36989 [=========================>....] - ETA: 36s - loss: 0.1587
33280/36989 [=========================>....] - ETA: 34s - loss: 0.1587
33536/36989 [==========================>...] - ETA: 31s - loss: 0.1587
33792/36989 [==========================>...] - ETA: 29s - loss: 0.1587
34048/36989 [==========================>...] - ETA: 27s - loss: 0.1587
34304/36989 [==========================>...] - ETA: 24s - loss: 0.1587
34560/36989 [===========================>..] - ETA: 22s - loss: 0.1586
34816/36989 [===========================>..] - ETA: 20s - loss: 0.1586
35072/36989 [===========================>..] - ETA: 17s - loss: 0.1586
35328/36989 [===========================>..] - ETA: 15s - loss: 0.1586
35584/36989 [===========================>..] - ETA: 12s - loss: 0.1585
35840/36989 [============================>.] - ETA: 10s - loss: 0.1585
36096/36989 [============================>.] - ETA: 8s - loss: 0.1585 
36352/36989 [============================>.] - ETA: 5s - loss: 0.1584
36608/36989 [============================>.] - ETA: 3s - loss: 0.1584
36864/36989 [============================>.] - ETA: 1s - loss: 0.1583
36989/36989 [==============================] - 347s 9ms/step - loss: 0.1583 - val_loss: 0.2991
Simple Array Sum
---
Input python program: 
count = int(input())
print(sum(int(s) for s in input().split()[:count]))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a;
        sum=sum+a;
    }
    cout<<sum;
    return 0;
}»
Drawing Book
---
Input python program: 
#!/bin/python3

import sys


n = int(input().strip())
p = int(input().strip())
# your code goes here

front = p // 2
n = (n // 2) * 2
back = max(0, (n - p + 1) // 2)
print(min(front, back))
-
Decoded C++ program:
#include <bits/stdc++.h>

using namespace std;

int main(){
    int n;
    cin >> n;
    int a[100];
    memset(ar, 0, sizeof(int)s);
    for(int i = 0; i < n; i++) {
        cin >> a[i];
        s[m[a][n]++a[i]-'a';
    }
    for (int i=0; i<n; i++) {
        if (a[i] > b[i]) {
            ans += 1;
        }
    }    
    cout << a << e dl << b ;  return 0;
}»
The Love-Letter Mystery
---
Input python program: 
def adjust(s):
    cost = 0
    for i in range(len(s) // 2):
        j = len(s) - 1 - i
        cost += abs(ord(s[i]) - ord(s[j]))
    return cost

rows = int(input())
for _ in range(rows):
    print(adjust(input()))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a;
        sum=sum+a;
    }
    cout<<sum;
    return 0;
}»
HackerRank in a String!
---
Input python program: 
#!/bin/python3

import sys

def has_hackerrank(string):
    x = 0
    for i in string:
        if x < len(HR) and i == HR[x]:
            x += 1
    if x == len(HR):
        print("YES")
    else:
        print("NO")

HR = "hackerrank"
q = int(input().strip())
for a0 in range(q):
    s = input().strip().lower()
    has_hackerrank(s)
-
Decoded C++ program:
#include <bits/stdc++.h>

using namespace std;

int main(){
    int n;
    cin >> n;
    int a[100];
    memset(ar, 0, sizeof(int)s);
    for(int i = 0; i < n; i++) {
        cin >> a[i];
        s[m[a][n]++a[i]-'a';
    }
    for (int i=0; i<n; i++) {
        if (a[i] > b[i]) {
            ans += 1;
        }
    }    
    cout << a << e dl << b ;  return 0;
}»
String Construction
---
Input python program: 
#!/bin/python3

import sys


n = int(input().strip())
for a0 in range(n):
    s = set(input())
    print(len(s))
    
-
Decoded C++ program:
#include <bits/stdc++.h>

using namespace std;

int main(){
    int n;
    cin >> n;
    int a[100];
    memset(ar, 0, sizeof(int)s);
    for(int i = 0; i < n; i++) {
        cin >> a[i];
        s[m[a][n]++a[i]-'a';
    }
    for (int i=0; i<n; i++) {
        if (a[i] > b[i]) {
            ans += 1;
        }
    }    
    cout << a << e dl << b ;  return 0;
}»
Library Fine
---
Input python program: 
import os
import sys
import re

def main():
 retDay, retMonth, retYear = map( (int), input().split() )
 day, month, year = map( (int), input().split() )

 #  Calculate fine
 if year < retYear:
  fine = 10000
 elif year > retYear:
  fine = 0
 elif month < retMonth:
  fine = 500 * ( retMonth - month )
 elif month > retMonth:
  fine = 0
 elif day < retDay:
  fine = 15 * ( retDay - day )
 else:
  fine = 0

 print( fine )

if __name__ == "__main__":
 main()
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a;
        sum=sum+a;
    }
    cout<<sum;
    return 0;
}»
Sum vs XOR
---
Input python program: 
n = bin(int(input()))[2:]
print(2**(n.count('0')) if n != '0' else 1)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a;
        sum=sum+a;
    }
    cout<<sum;
    return 0;
}»
Cut the sticks
---
Input python program: 
n = int(input())
line = input().split()
a =  [int(line[i]) for i in range(n)]
while len(a) > 0:
    print(len(a))
    m = min(a)
    b = []
    for i in range(len(a)):
        a[i] -= m
        if a[i] > 0:
            b.append(a[i])
    a = b 
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a;
        sum=sum+a;
    }
    cout<<sum;
    return 0;
}»
Grading Students
---
Input python program: 
#!/bin/python3

n = int(input())
for _ in range(n):
    grade = int(input())
    if grade >= 38 and grade % 5 >= 3:
        grade += 5 - (grade % 5)
    print(grade)
-
Decoded C++ program:
#include <bits/stdc++.h>

using namespace std;

int main(){
    int n;
    cin >> n;
    int a[100];
    memset(ar, 0, sizeof(int)s);
    for(int i = 0; i < n; i++) {
        cin >> a[i];
        s[m[a][n]++a[i]-'a';
    }
    for (int i=0; i<n; i++) {
        if (a[i] > b[i]) {
            ans += 1;
        }
    }    
    cout << a << e dl << b ;  return 0;
}»
Flipping bits
---
Input python program: 
n = int(input())
for i in range(n):
    a = int(input())
    print(a ^ 0b11111111111111111111111111111111)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a;
        sum=sum+a;
    }
    cout<<sum;
    return 0;
}»
Simple Array Sum
---
Input python program: 
n = int(input())
lst = input().split()

result = 0
for item in lst:
    result += int(item)

print(result)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a;
        sum=sum+a;
    }
    cout<<sum;
    return 0;
}»
Flipping bits
---
Input python program: 
for t in range(int(input())):
    n = int(input())
    x = 2**32-1
    print(n^x)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a;
        sum=sum+a;
    }
    cout<<sum;
    return 0;
}»
Time Conversion
---
Input python program: 
time_str = input()
am_pm, time_str = time_str[-2: ], time_str[ :-2]
hh, mm, ss = time_str.split(':')
hh, mm, ss = int(hh), int(mm), int(ss)

if am_pm == 'PM':
    if hh != 12:
        hh += 12
if am_pm == 'AM' and hh == 12:
    hh = 0

print('{:02d}'.format(hh) + ':' + '{:02d}'.format(mm) + ':' + '{:02d}'.format(ss))
 
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a;
        sum=sum+a;
    }
    cout<<sum;
    return 0;
}»
Save the Prisoner!
---
Input python program: 
t = int(input())
for _ in range(t):
    n, m, s = list(map(int, input().split()))
    print( (m+s-2) % n + 1)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a;
        sum=sum+a;
    }
    cout<<sum;
    return 0;
}»
Simple Array Sum
---
Input python program: 
import sys

# Read input
lines = sys.stdin.readlines()
N = int(lines[0])
Integers = [int(i) for i in lines[1].strip().split()]

sum_ints = 0
for item in Integers:
    sum_ints = sum_ints + item 

print(sum_ints)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a;
        sum=sum+a;
    }
    cout<<sum;
    return 0;
}»
A Very Big Sum
---
Input python program: 
T = int(input())
t_list = list(map(int,input().split()))
print(sum(t_list))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a;
        sum=sum+a;
    }
    cout<<sum;
    return 0;
}»
Lonely Integer
---
Input python program: 
#!/usr/bin/py
# Head ends here
def lonelyinteger(a):
    for n in a:
        if a.count(n) == 1:
            return n
    answer = a[0]
    return answer
# Tail starts here
if __name__ == '__main__':
    a = int(input())
    b = input().split(" ")
    print(lonelyinteger(b))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a;
        sum=sum+a;
    }
    cout<<sum;
    return 0;
}»
Non-Divisible Subset
---
Input python program: 
import math
n, k = [int(x) for x in input().strip().split(' ')]
arr = [int(x) % k for x in input().strip().split(' ')]
arrcount = [arr.count(x) for x in range(k)]
count = 0
count += min(1,arrcount[0])
if k % 2 == 0:
    count += min(1,arrcount[k//2])
for i in range(1,math.ceil(k/2)):
    count += max(arrcount[i],arrcount[k-i])
print(count)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a;
        sum=sum+a;
    }
    cout<<sum;
    return 0;
}»
A Very Big Sum
---
Input python program: 
N = int(input())
l = list(map(int, input().split()))

print(sum(l))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a;
        sum=sum+a;
    }
    cout<<sum;
    return 0;
}»
A Very Big Sum
---
Input python program: 
input()

print(sum(map(int, input().split())))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>a;
        sum=sum+a;
    }
    cout<<sum;
    return 0;
}»
Epoch: 2/10
Train on 36989 samples, validate on 1947 samples
Epoch 1/1

  256/36989 [..............................] - ETA: 5:45 - loss: 0.1461
  512/36989 [..............................] - ETA: 5:42 - loss: 0.1534
  768/36989 [..............................] - ETA: 5:38 - loss: 0.1517
 1024/36989 [..............................] - ETA: 5:36 - loss: 0.1519
 1280/36989 [>.............................] - ETA: 5:39 - loss: 0.1511
 1536/36989 [>.............................] - ETA: 5:40 - loss: 0.1525
 1792/36989 [>.............................] - ETA: 5:37 - loss: 0.1511
 2048/36989 [>.............................] - ETA: 5:33 - loss: 0.1509
 2304/36989 [>.............................] - ETA: 5:31 - loss: 0.1490
 2560/36989 [=>............................] - ETA: 5:27 - loss: 0.1484
 2816/36989 [=>............................] - ETA: 5:24 - loss: 0.1497
 3072/36989 [=>............................] - ETA: 5:23 - loss: 0.1493
 3328/36989 [=>............................] - ETA: 5:20 - loss: 0.1507
 3584/36989 [=>............................] - ETA: 5:17 - loss: 0.1503
 3840/36989 [==>...........................] - ETA: 5:14 - loss: 0.1504
 4096/36989 [==>...........................] - ETA: 5:11 - loss: 0.1501
 4352/36989 [==>...........................] - ETA: 5:08 - loss: 0.1492
 4608/36989 [==>...........................] - ETA: 5:06 - loss: 0.1490
 4864/36989 [==>...........................] - ETA: 5:04 - loss: 0.1485
 5120/36989 [===>..........................] - ETA: 5:01 - loss: 0.1482
 5376/36989 [===>..........................] - ETA: 4:59 - loss: 0.1486
 5632/36989 [===>..........................] - ETA: 4:56 - loss: 0.1482
 5888/36989 [===>..........................] - ETA: 4:53 - loss: 0.1486
 6144/36989 [===>..........................] - ETA: 4:50 - loss: 0.1485
 6400/36989 [====>.........................] - ETA: 4:48 - loss: 0.1481
 6656/36989 [====>.........................] - ETA: 4:46 - loss: 0.1484
 6912/36989 [====>.........................] - ETA: 4:44 - loss: 0.1487
 7168/36989 [====>.........................] - ETA: 4:42 - loss: 0.1484
 7424/36989 [=====>........................] - ETA: 4:39 - loss: 0.1482
 7680/36989 [=====>........................] - ETA: 4:37 - loss: 0.1484
 7936/36989 [=====>........................] - ETA: 4:34 - loss: 0.1483
 8192/36989 [=====>........................] - ETA: 4:32 - loss: 0.1484
 8448/36989 [=====>........................] - ETA: 4:31 - loss: 0.1484
 8704/36989 [======>.......................] - ETA: 4:29 - loss: 0.1485
 8960/36989 [======>.......................] - ETA: 4:26 - loss: 0.1487
 9216/36989 [======>.......................] - ETA: 4:24 - loss: 0.1486
 9472/36989 [======>.......................] - ETA: 4:22 - loss: 0.1486
 9728/36989 [======>.......................] - ETA: 4:19 - loss: 0.1485
 9984/36989 [=======>......................] - ETA: 4:17 - loss: 0.1483
10240/36989 [=======>......................] - ETA: 4:14 - loss: 0.1484
10496/36989 [=======>......................] - ETA: 4:12 - loss: 0.1484
10752/36989 [=======>......................] - ETA: 4:09 - loss: 0.1485
11008/36989 [=======>......................] - ETA: 4:07 - loss: 0.1483
11264/36989 [========>.....................] - ETA: 4:04 - loss: 0.1482
11520/36989 [========>.....................] - ETA: 4:02 - loss: 0.1480
11776/36989 [========>.....................] - ETA: 3:59 - loss: 0.1477
12032/36989 [========>.....................] - ETA: 3:57 - loss: 0.1479
12288/36989 [========>.....................] - ETA: 3:54 - loss: 0.1479
12544/36989 [=========>....................] - ETA: 3:52 - loss: 0.1478
12800/36989 [=========>....................] - ETA: 3:49 - loss: 0.1477
13056/36989 [=========>....................] - ETA: 3:47 - loss: 0.1473
13312/36989 [=========>....................] - ETA: 3:45 - loss: 0.1473
13568/36989 [==========>...................] - ETA: 3:42 - loss: 0.1473
13824/36989 [==========>...................] - ETA: 3:40 - loss: 0.1472
14080/36989 [==========>...................] - ETA: 3:37 - loss: 0.1473
14336/36989 [==========>...................] - ETA: 3:35 - loss: 0.1472
14592/36989 [==========>...................] - ETA: 3:32 - loss: 0.1473
14848/36989 [===========>..................] - ETA: 3:30 - loss: 0.1473
15104/36989 [===========>..................] - ETA: 3:27 - loss: 0.1474
15360/36989 [===========>..................] - ETA: 3:25 - loss: 0.1475
15616/36989 [===========>..................] - ETA: 3:23 - loss: 0.1473
15872/36989 [===========>..................] - ETA: 3:20 - loss: 0.1472
16128/36989 [============>.................] - ETA: 3:18 - loss: 0.1471
16384/36989 [============>.................] - ETA: 3:16 - loss: 0.1471
16640/36989 [============>.................] - ETA: 3:13 - loss: 0.1472
16896/36989 [============>.................] - ETA: 3:11 - loss: 0.1470
17152/36989 [============>.................] - ETA: 3:08 - loss: 0.1471
17408/36989 [=============>................] - ETA: 3:06 - loss: 0.1470
17664/36989 [=============>................] - ETA: 3:03 - loss: 0.1470
17920/36989 [=============>................] - ETA: 3:01 - loss: 0.1469
18176/36989 [=============>................] - ETA: 2:59 - loss: 0.1470
18432/36989 [=============>................] - ETA: 2:56 - loss: 0.1470
18688/36989 [==============>...............] - ETA: 2:54 - loss: 0.1468
18944/36989 [==============>...............] - ETA: 2:51 - loss: 0.1467
19200/36989 [==============>...............] - ETA: 2:49 - loss: 0.1467
19456/36989 [==============>...............] - ETA: 2:46 - loss: 0.1469
19712/36989 [==============>...............] - ETA: 2:44 - loss: 0.1467
19968/36989 [===============>..............] - ETA: 2:42 - loss: 0.1466
20224/36989 [===============>..............] - ETA: 2:39 - loss: 0.1465
20480/36989 [===============>..............] - ETA: 2:37 - loss: 0.1465
20736/36989 [===============>..............] - ETA: 2:34 - loss: 0.1464
20992/36989 [================>.............] - ETA: 2:32 - loss: 0.1465
21248/36989 [================>.............] - ETA: 2:29 - loss: 0.1466
21504/36989 [================>.............] - ETA: 2:27 - loss: 0.1464
21760/36989 [================>.............] - ETA: 2:24 - loss: 0.1463
22016/36989 [================>.............] - ETA: 2:22 - loss: 0.1463
22272/36989 [=================>............] - ETA: 2:19 - loss: 0.1462
22528/36989 [=================>............] - ETA: 2:17 - loss: 0.1462
22784/36989 [=================>............] - ETA: 2:14 - loss: 0.1462
23040/36989 [=================>............] - ETA: 2:12 - loss: 0.1462
23296/36989 [=================>............] - ETA: 2:10 - loss: 0.1462
23552/36989 [==================>...........] - ETA: 2:07 - loss: 0.1461
23808/36989 [==================>...........] - ETA: 2:05 - loss: 0.1462
24064/36989 [==================>...........] - ETA: 2:02 - loss: 0.1461
24320/36989 [==================>...........] - ETA: 2:00 - loss: 0.1461
24576/36989 [==================>...........] - ETA: 1:57 - loss: 0.1461
24832/36989 [===================>..........] - ETA: 1:55 - loss: 0.1461
25088/36989 [===================>..........] - ETA: 1:52 - loss: 0.1461
25344/36989 [===================>..........] - ETA: 1:50 - loss: 0.1463
25600/36989 [===================>..........] - ETA: 1:48 - loss: 0.1462
25856/36989 [===================>..........] - ETA: 1:45 - loss: 0.1462
26112/36989 [====================>.........] - ETA: 1:43 - loss: 0.1462
26368/36989 [====================>.........] - ETA: 1:40 - loss: 0.1461
26624/36989 [====================>.........] - ETA: 1:38 - loss: 0.1461
26880/36989 [====================>.........] - ETA: 1:35 - loss: 0.1460
27136/36989 [=====================>........] - ETA: 1:33 - loss: 0.1459
27392/36989 [=====================>........] - ETA: 1:30 - loss: 0.1459
27648/36989 [=====================>........] - ETA: 1:28 - loss: 0.1459
27904/36989 [=====================>........] - ETA: 1:26 - loss: 0.1458
28160/36989 [=====================>........] - ETA: 1:23 - loss: 0.1457
28416/36989 [======================>.......] - ETA: 1:21 - loss: 0.1457
28672/36989 [======================>.......] - ETA: 1:18 - loss: 0.1456
28928/36989 [======================>.......] - ETA: 1:16 - loss: 0.1456
29184/36989 [======================>.......] - ETA: 1:13 - loss: 0.1455
29440/36989 [======================>.......] - ETA: 1:11 - loss: 0.1456
29696/36989 [=======================>......] - ETA: 1:08 - loss: 0.1456
29952/36989 [=======================>......] - ETA: 1:06 - loss: 0.1455
30208/36989 [=======================>......] - ETA: 1:04 - loss: 0.1454
30464/36989 [=======================>......] - ETA: 1:01 - loss: 0.1454
30720/36989 [=======================>......] - ETA: 59s - loss: 0.1453 
30976/36989 [========================>.....] - ETA: 56s - loss: 0.1453
31232/36989 [========================>.....] - ETA: 54s - loss: 0.1452
31488/36989 [========================>.....] - ETA: 51s - loss: 0.1452
31744/36989 [========================>.....] - ETA: 49s - loss: 0.1451
32000/36989 [========================>.....] - ETA: 47s - loss: 0.1451
32256/36989 [=========================>....] - ETA: 44s - loss: 0.1451
32512/36989 [=========================>....] - ETA: 42s - loss: 0.1449
32768/36989 [=========================>....] - ETA: 39s - loss: 0.1449
33024/36989 [=========================>....] - ETA: 37s - loss: 0.1448
33280/36989 [=========================>....] - ETA: 34s - loss: 0.1449
33536/36989 [==========================>...] - ETA: 32s - loss: 0.1449
33792/36989 [==========================>...] - ETA: 30s - loss: 0.1448
34048/36989 [==========================>...] - ETA: 27s - loss: 0.1448
34304/36989 [==========================>...] - ETA: 25s - loss: 0.1448
34560/36989 [===========================>..] - ETA: 22s - loss: 0.1447
34816/36989 [===========================>..] - ETA: 20s - loss: 0.1446
35072/36989 [===========================>..] - ETA: 18s - loss: 0.1446
35328/36989 [===========================>..] - ETA: 15s - loss: 0.1446
35584/36989 [===========================>..] - ETA: 13s - loss: 0.1445
35840/36989 [============================>.] - ETA: 10s - loss: 0.1445
36096/36989 [============================>.] - ETA: 8s - loss: 0.1445 
36352/36989 [============================>.] - ETA: 5s - loss: 0.1444
36608/36989 [============================>.] - ETA: 3s - loss: 0.1443
36864/36989 [============================>.] - ETA: 1s - loss: 0.1443
36989/36989 [==============================] - 354s 10ms/step - loss: 0.1443 - val_loss: 0.3052
Bon Appétit
---
Input python program: 
def solve():
    n,k = map(int, input().split())
    a = list(map(int, input().split()))
    c = int(input())
    
    s = sum(a)-a[k]
    if s//2 >= c:
        return "Bon Appetit"
    return c-s//2

print(solve())
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>j;
        sum=sum+j;
        }
    cout<<sum;
    return 0;
}»
A Very Big Sum
---
Input python program: 
n=int(input())
l=[int(i) for i in input().split()]
print(sum(l))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>j;
        sum=sum+j;
        }
    cout<<sum;
    return 0;
}»
Making Anagrams
---
Input python program: 
from collections import Counter
A = Counter(input())
A.subtract(input())
print(sum(abs(x) for x in A.values()))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>j;
        sum=sum+j;
        }
    cout<<sum;
    return 0;
}»
Intro to Tutorial Challenges
---
Input python program: 
V = int(input())
n = int(input())
ar = [int(word) for word in input().split(' ')]
count = 0
for num in ar:
    if num == V:
        print(count)
    count += 1
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>j;
        sum=sum+j;
        }
    cout<<sum;
    return 0;
}»
A Very Big Sum
---
Input python program: 
n = input();

ns = input().split(' ');

print(sum(map(int, ns)));
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>j;
        sum=sum+j;
        }
    cout<<sum;
    return 0;
}»
The Love-Letter Mystery
---
Input python program: 
def func(str):
    return sum([abs(ord(str[i]) - ord(str[len(str) - i - 1])) for i in range(len(str) // 2)])
  
if __name__ == '__main__':
    for t in range(int(input())):
        print(func(input()))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>j;
        sum=sum+j;
        }
    cout<<sum;
    return 0;
}»
The Love-Letter Mystery
---
Input python program: 
import sys
import math

def main():
    n = int(sys.stdin.readline().strip())
    for line in sys.stdin:
        line = line.strip()
        sumation = 0
        for i in range(math.floor(len(line)/2)):
            sumation += abs(ord(line[len(line) - i - 1]) - ord(line[i]))
        print(sumation)
            
if __name__ == '__main__':
    main()
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>j;
        sum=sum+j;
        }
    cout<<sum;
    return 0;
}»
Plus Minus
---
Input python program: 
negs = 0
pos = 0
zero = 0

length = int(input())
array = input().strip().split()

for each in array:
    if int(each) < 0: negs += 1
    if int(each) == 0: zero += 1
    if int(each) > 0: pos += 1
        
print(pos/length)
print(negs/length)
print(zero/length)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>j;
        sum=sum+j;
        }
    cout<<sum;
    return 0;
}»
Counting Sort 1
---
Input python program: 
if __name__ == '__main__':
    n = int(input().strip())
    x = list(map(int,input().strip().split()))
    arr = [0 for i in range(100)]
    
    for i in x:
        arr[i] += 1
           
    for i in arr:
        print(i, end=" ")
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>j;
        sum=sum+j;
        }
    cout<<sum;
    return 0;
}»
String Construction
---
Input python program: 
#!/bin/python3

import sys


n = int(input().strip())
for a0 in range(n):
    s = input().strip()
    p=""
    count = 0
    for c in s:
        if c not in p:
            count += 1
        p += c
    print(count)
-
Decoded C++ program:
#include <bits/stdc++.h>

using namespace std;

int main(){
    int n;
    cin >> n;
    int min = 0;
    int max = 0;
    for (int i = 0; i < n; i++) {
        cin >> val;
        value[val ++5];
    }
    
    for (int i = 0; i < 100; ++i) {
        cout << ar[i] << " ";
    }
    return 0;
}»
Two Strings
---
Input python program: 
def check_common(a,b):
    f1=[0]*150
    f2=[0]*150
    for i in range(0,len(a)):
        f1[ord(a[i])]+=1
    for j in range(0,len(b)):
        f2[ord(b[j])]+=1
        
    for k in range(97,122):
        if  f1[k]>0 and f2[k]>0:
            return "YES"
    return "NO"
    
    
if __name__=="__main__":
    t=int(input())
    while t>0:
        a=input()
        b=input()
        print(check_common(a,b))
        t-=1
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>j;
        sum=sum+j;
        }
    cout<<sum;
    return 0;
}»
Find the Median
---
Input python program: 
N = int(input())

arr = [int(val) for val in input().strip().split(" ")]

arr.sort()

print(arr[int(len(arr)/2)])
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>j;
        sum=sum+j;
        }
    cout<<sum;
    return 0;
}»
Simple Array Sum
---
Input python program: 
n = int(input())
sum = 0
#for i in range(0,n):
nums = input().split()
for j in nums:
    sum +=int(j)
print (sum)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>j;
        sum=sum+j;
        }
    cout<<sum;
    return 0;
}»
Gemstones
---
Input python program: 
def num_gems (arr):
 count = [0] * 26
 for rock in arr:
  occurred = [False] * 26
  for c in rock:
   c = ord(c.lower()) - 97
   if c >= 0 and c < 26:
    if not occurred[c]:
     occurred[c] = True
     count[c] += 1

 num_gems = 0
 for i in count:
  if i == len(arr):
   num_gems += 1

 return num_gems




rocks = []
for i in range(0, int(input())):
 rocks.append(input())

print(num_gems(rocks))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>j;
        sum=sum+j;
        }
    cout<<sum;
    return 0;
}»
Simple Array Sum
---
Input python program: 
import sys

sys.stdin.readline()
print(sum([int(i) for i in sys.stdin.readline().split(' ')]))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>j;
        sum=sum+j;
        }
    cout<<sum;
    return 0;
}»
Counting Sort 1
---
Input python program: 
if __name__ == '__main__':
    n = int(input())
    ar = [int(i) for i in input().strip().split()]
    x = [0 for i in range(100)]
    for e in ar:
        x[e] += 1
    print(*x, sep= ' ')
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>j;
        sum=sum+j;
        }
    cout<<sum;
    return 0;
}»
Simple Array Sum
---
Input python program: 
if __name__ == '__main__':
    list_size = int(input())
    number_list = input().split(' ', list_size)

    print(sum(map(int, number_list)))
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>j;
        sum=sum+j;
        }
    cout<<sum;
    return 0;
}»
A Very Big Sum
---
Input python program: 
count = int(input())
arr = input().split()
addition = 0
for val in range(0,count):
    addition += int(arr[val])
print(addition)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>j;
        sum=sum+j;
        }
    cout<<sum;
    return 0;
}»
Plus Minus
---
Input python program: 
size = int(input())
arr = input().split()

pos = 0
neg = 0
zero = 0

for num in arr:
    if int(num) > 0:
        pos += 1
    elif int(num) == 0:
        zero += 1
    else:
        neg += 1

pos_frac = pos / size
zero_frac = zero / size
neg_frac = neg / size

print(pos_frac)
print(neg_frac)
print(zero_frac)
-
Decoded C++ program:
#include <cmath>
#include <cstdio>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;


int main() {
    /* Enter your code here. Read input from STDIN. Print output to STDOUT */   
    int n,i,j,sum=0;
    cin>>n;
    for(i=0;i<n;i++)
        {
        cin>>j;
        sum=sum+j;
        }
    cout<<sum;
    return 0;
}»
The Great XOR
---
Input python program: 
#!/bin/python3

import sys

def toBinary(number):
    result = []
    while(number >= 2):
        remainder = number%2
        number = int(number/2)
        result.append(remainder)
    result.append(1)
    return result

q = int(input().strip())
for a0 in range(q):
    x = int(input().strip())
    count = 0
    number = toBinary(x)
    for i in range(len(number)):
        if (number[i] == 0):
            count += 2**i
    print(count)
-
Decoded C++ program:
#include <bits/stdc++.h>

using namespace std;

int main(){
    int n;
    cin >> n;
    int min = 0;
    int max = 0;
    for (int i = 0; i < n; i++) {
        cin >> val;
        value[val ++5];
    }
    
    for (int i = 0; i < 100; ++i) {
        cout << ar[i] << " ";
    }
    return 0;
}»
Epoch: 3/10
Train on 36989 samples, validate on 1947 samples
Epoch 1/1

  256/36989 [..............................] - ETA: 5:59 - loss: 0.1352
  512/36989 [..............................] - ETA: 5:58 - loss: 0.1376
  768/36989 [..............................] - ETA: 5:49 - loss: 0.1392
 1024/36989 [..............................] - ETA: 5:43 - loss: 0.1376
 1280/36989 [>.............................] - ETA: 5:40 - loss: 0.1393
 1536/36989 [>.............................] - ETA: 5:37 - loss: 0.1393
 1792/36989 [>.............................] - ETA: 5:33 - loss: 0.1384
 2048/36989 [>.............................] - ETA: 5:29 - loss: 0.1389
 2304/36989 [>.............................] - ETA: 5:25 - loss: 0.1381
 2560/36989 [=>............................] - ETA: 5:21 - loss: 0.1380
 2816/36989 [=>............................] - ETA: 5:19 - loss: 0.1375
 3072/36989 [=>............................] - ETA: 5:16 - loss: 0.1377
 3328/36989 [=>............................] - ETA: 5:13 - loss: 0.1374
 3584/36989 [=>............................] - ETA: 5:10 - loss: 0.1378
 3840/36989 [==>...........................] - ETA: 5:08 - loss: 0.1378
 4096/36989 [==>...........................] - ETA: 5:05 - loss: 0.1378
 4352/36989 [==>...........................] - ETA: 5:02 - loss: 0.1379
 4608/36989 [==>...........................] - ETA: 5:00 - loss: 0.1376
 4864/36989 [==>...........................] - ETA: 4:57 - loss: 0.1376
 5120/36989 [===>..........................] - ETA: 4:55 - loss: 0.1378
 5376/36989 [===>..........................] - ETA: 4:53 - loss: 0.1377
 5632/36989 [===>..........................] - ETA: 4:51 - loss: 0.1377
 5888/36989 [===>..........................] - ETA: 4:48 - loss: 0.1380
 6144/36989 [===>..........................] - ETA: 4:46 - loss: 0.1380
 6400/36989 [====>.........................] - ETA: 4:43 - loss: 0.1379
 6656/36989 [====>.........................] - ETA: 4:41 - loss: 0.1378
 6912/36989 [====>.........................] - ETA: 4:39 - loss: 0.1374
 7168/36989 [====>.........................] - ETA: 4:36 - loss: 0.1370
 7424/36989 [=====>........................] - ETA: 4:34 - loss: 0.1366
 7680/36989 [=====>........................] - ETA: 4:32 - loss: 0.1365
 7936/36989 [=====>........................] - ETA: 4:30 - loss: 0.1366
 8192/36989 [=====>........................] - ETA: 4:28 - loss: 0.1364
 8448/36989 [=====>........................] - ETA: 4:25 - loss: 0.1365
 8704/36989 [======>.......................] - ETA: 4:23 - loss: 0.1365
 8960/36989 [======>.......................] - ETA: 4:20 - loss: 0.1365
 9216/36989 [======>.......................] - ETA: 4:18 - loss: 0.1365
 9472/36989 [======>.......................] - ETA: 4:15 - loss: 0.1364
 9728/36989 [======>.......................] - ETA: 4:13 - loss: 0.1361
 9984/36989 [=======>......................] - ETA: 4:10 - loss: 0.1360
10240/36989 [=======>......................] - ETA: 4:08 - loss: 0.1361
10496/36989 [=======>......................] - ETA: 4:05 - loss: 0.1357
10752/36989 [=======>......................] - ETA: 4:03 - loss: 0.1356
11008/36989 [=======>......................] - ETA: 4:00 - loss: 0.1354
11264/36989 [========>.....................] - ETA: 3:58 - loss: 0.1354
11520/36989 [========>.....................] - ETA: 3:55 - loss: 0.1353
11776/36989 [========>.....................] - ETA: 3:53 - loss: 0.1354
12032/36989 [========>.....................] - ETA: 3:50 - loss: 0.1354
12288/36989 [========>.....................] - ETA: 3:48 - loss: 0.1352
12544/36989 [=========>....................] - ETA: 3:45 - loss: 0.1350
12800/36989 [=========>....................] - ETA: 3:43 - loss: 0.1350
13056/36989 [=========>....................] - ETA: 3:40 - loss: 0.1350
13312/36989 [=========>....................] - ETA: 3:38 - loss: 0.1349
13568/36989 [==========>...................] - ETA: 3:36 - loss: 0.1349
13824/36989 [==========>...................] - ETA: 3:33 - loss: 0.1350
14080/36989 [==========>...................] - ETA: 3:31 - loss: 0.1350
14336/36989 [==========>...................] - ETA: 3:29 - loss: 0.1349
14592/36989 [==========>...................] - ETA: 3:26 - loss: 0.1350
14848/36989 [===========>..................] - ETA: 3:24 - loss: 0.1347
15104/36989 [===========>..................] - ETA: 3:22 - loss: 0.1348
15360/36989 [===========>..................] - ETA: 3:19 - loss: 0.1348
15616/36989 [===========>..................] - ETA: 3:17 - loss: 0.1346
15872/36989 [===========>..................] - ETA: 3:14 - loss: 0.1347
16128/36989 [============>.................] - ETA: 3:12 - loss: 0.1347
16384/36989 [============>.................] - ETA: 3:10 - loss: 0.1347
16640/36989 [============>.................] - ETA: 3:07 - loss: 0.1346
16896/36989 [============>.................] - ETA: 3:05 - loss: 0.1345
17152/36989 [============>.................] - ETA: 3:03 - loss: 0.1346
17408/36989 [=============>................] - ETA: 3:00 - loss: 0.1346
17664/36989 [=============>................] - ETA: 2:58 - loss: 0.1346
17920/36989 [=============>................] - ETA: 2:56 - loss: 0.1345